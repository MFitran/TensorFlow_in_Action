{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Teaching machines to see better: Improving CNNs and making them confess\n",
    "\n",
    "This notebook reproduces the code and summarizes the theoretical concepts from Chapter 7 of *'TensorFlow in Action'* by Thushan Ganegedara.\n",
    "\n",
    "In Chapter 6, we built an Inception v1 model that suffered from severe **overfitting** (high training accuracy, low validation accuracy). This chapter focuses on practical techniques to solve that problem and significantly improve our model's performance. \n",
    "\n",
    "We will cover:\n",
    "1.  **Regularization Techniques**: Using Image Data Augmentation, Dropout, and Early Stopping to combat overfitting.\n",
    "2.  **A Better Architecture (Minception)**: Implementing a more modern architecture inspired by Inception-ResNet, which uses Batch Normalization and Residual Connections.\n",
    "3.  **Transfer Learning**: Using a large, pretrained model (Inception-ResNet v2) to get state-of-the-art results.\n",
    "4.  **Model Explainability (Grad-CAM)**: Visualizing *why* our CNN makes certain decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Data Pipeline from Chapter 6\n",
    "\n",
    "Before we can improve the model, we need the same data pipeline from Chapter 6. We'll use the **tiny-imagenet-200** dataset and the `ImageDataGenerator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow.keras.backend as K\n",
    "from functools import partial\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "random_seed = 4321\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# Define file paths (assuming data is in 'data/tiny-imagenet-200')\n",
    "data_dir = os.path.join('data', 'tiny-imagenet-200')\n",
    "train_image_dir = os.path.join(data_dir, 'train')\n",
    "val_dir = os.path.join(data_dir, 'val')\n",
    "val_ann_path = os.path.join(val_dir, 'val_annotations.txt')\n",
    "\n",
    "# Helper function to read the test (validation) annotations\n",
    "def get_test_labels_df(test_labels_path):\n",
    "    test_df = pd.read_csv(test_labels_path, sep='\\t', index_col=None, header=None)\n",
    "    test_df = test_df.iloc[:, [0, 1]].rename({0: \"filename\", 1: \"class\"}, axis=1)\n",
    "    return test_df\n",
    "\n",
    "# Helper function to create the auxiliary data generator\n",
    "# Our Inception model has 3 outputs, so the generator must yield (x, (y, y, y))\n",
    "def data_gen_aux(gen):\n",
    "    for x, y in gen:\n",
    "        yield x, (y, y, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Techniques for Reducing Overfitting\n",
    "\n",
    "**Overfitting** is when a model learns the training data *too* well, including its noise and random fluctuations. It memorizes the training examples instead of learning the general patterns. This results in high training accuracy but poor performance on new, unseen data (like the validation or test set).\n",
    "\n",
    "We will apply three techniques to fight this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1 Image Data Augmentation with Keras\n",
    "\n",
    "Data augmentation artificially creates more training data by applying random transformations to the existing images (e.g., rotating, shifting, zooming, and flipping). This teaches the model that these transformed images all belong to the same class, making it more robust and less likely to overfit on specific orientations.\n",
    "\n",
    "We only apply augmentation to the **training set**. The validation and test sets must remain unchanged to serve as a consistent benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "# We'll use 56x56 as the target size for our custom Minception model later\n",
    "target_size = (56, 56)\n",
    "\n",
    "# 1. Define the generator for Training and Validation WITH AUGMENTATION\n",
    "image_gen_aug = ImageDataGenerator(\n",
    "    samplewise_center=True,      # Normalize each image\n",
    "    rotation_range=30,         # Randomly rotate up to 30 degrees\n",
    "    width_shift_range=0.2,     # Randomly shift width by 20%\n",
    "    height_shift_range=0.2,    # Randomly shift height by 20%\n",
    "    brightness_range=(0.5, 1.5), # Randomly change brightness\n",
    "    shear_range=5,             # Apply shear transformation\n",
    "    zoom_range=0.2,            # Randomly zoom in by 20%\n",
    "    horizontal_flip=True,      # Randomly flip horizontally\n",
    "    fill_mode='reflect',         # How to fill pixels after a shift/zoom\n",
    "    validation_split=0.1     # Split 10% of data for validation\n",
    ")\n",
    "\n",
    "# 2. Define the generator for Test data (NO AUGMENTATION, only normalization)\n",
    "image_gen_test = ImageDataGenerator(samplewise_center=True)\n",
    "\n",
    "# 3. Create the Training and Validation Generators\n",
    "partial_flow_func = partial(\n",
    "    image_gen_aug.flow_from_directory,\n",
    "    directory=train_image_dir,\n",
    "    target_size=target_size,\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=random_seed\n",
    ")\n",
    "train_gen = partial_flow_func(subset='training')\n",
    "valid_gen = partial_flow_func(subset='validation')\n",
    "\n",
    "# 4. Create the Test Generator\n",
    "test_df = get_test_labels_df(val_ann_path)\n",
    "test_gen = image_gen_test.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    directory=os.path.join(val_dir, 'images'),\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=target_size,\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"Data generators with augmentation are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.2 Dropout\n",
    "\n",
    "**Dropout** is a regularization technique where, during each training step, a random fraction of neurons (e.g., 40%) are \"dropped out\" or temporarily switched off. \n",
    "\n",
    "This prevents the network from becoming too reliant on any single neuron or feature. It forces the network to learn redundant representations, which makes it more generalizable.\n",
    "\n",
    "We will re-define the `aux_out` function and the main model's output to include `Dropout` layers, as specified in the Inception v1 paper (which we omitted in Ch. 6 for simplicity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-defining the 'aux_out' function from Chapter 6, but with Dropout\n",
    "def aux_out_with_dropout(inp, name=None):\n",
    "    avgpool1 = layers.AvgPool2D((5,5), strides=(3,3), padding='valid')(inp) \n",
    "    conv1 = layers.Conv2D(128, (1,1), activation='relu', padding='same')(avgpool1) \n",
    "    flat = layers.Flatten()(conv1) \n",
    "    dense1 = layers.Dense(1024, activation='relu')(flat) \n",
    "    # Add Dropout(0.7) as specified in the original Inception paper\n",
    "    dropout1 = layers.Dropout(0.7)(dense1)\n",
    "    aux_out = layers.Dense(200, activation='softmax', name=name)(dropout1) # 200 classes\n",
    "    return aux_out\n",
    "\n",
    "# We would then build the Inception v1 model, but add a Dropout(0.4) layer\n",
    "# before the final prediction layer.\n",
    "\n",
    "# (Conceptual model snippet showing where Dropout is added)\n",
    "# ... (Inception blocks) ...\n",
    "# avgpool_final = layers.AvgPool2D((7,7), strides=(1,1), padding='valid')(inc_5b)\n",
    "# flat_out = layers.Flatten()(avgpool_final)\n",
    "# -- ADD DROPOUT HERE --\n",
    "# dropout_final = layers.Dropout(0.4)(flat_out)\n",
    "# main_output = layers.Dense(200, activation='softmax', name='final')(dropout_final)\n",
    "# ... (rest of the model) ...\n",
    "print(\"Dropout concept added to model definition.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.3 Early Stopping\n",
    "\n",
    "**Early Stopping** is a technique to stop the training process automatically when the model's performance on the *validation set* stops improving. \n",
    "\n",
    "We monitor a specific metric (e.g., `val_loss`). If that metric doesn't improve for a set number of epochs (called `patience`), we halt training. This prevents the model from continuing to train into an overfitted state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the EarlyStopping callback\n",
    "es_callback = EarlyStopping(\n",
    "    monitor='val_loss', # Monitor the validation loss\n",
    "    patience=5          # Stop if it doesn't improve for 5 epochs\n",
    ")\n",
    "\n",
    "print(\"EarlyStopping callback defined.\")\n",
    "\n",
    "# When fitting the model, we would pass this in the 'callbacks' list:\n",
    "# model.fit(\n",
    "#     train_gen_aux, \n",
    "#     validation_data=valid_gen_aux, \n",
    "#     epochs=50, \n",
    "#     callbacks=[es_callback, csv_logger]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Toward minimalism: Minception instead of Inception\n",
    "\n",
    "The Inception v1 architecture is effective but somewhat outdated. The book proposes building a *new* model, **\"Minception\"**, inspired by the more modern Inception-ResNet v2. This model introduces two powerful concepts: Batch Normalization and Residual Connections.\n",
    "\n",
    "### Batch Normalization (BN)\n",
    "BN normalizes the output of a layer by re-centering and re-scaling the activations. This solves the \"internal covariate shift\" problem, where the distribution of each layer's inputs changes during training. \n",
    "\n",
    "**Benefits:**\n",
    "* Allows for much faster training (higher learning rates).\n",
    "* Stabilizes the training process.\n",
    "* Acts as a regularizer, sometimes replacing the need for Dropout.\n",
    "\n",
    "It's typically applied **after** the convolution/dense layer and **before** the activation function.\n",
    "\n",
    "### Residual Connections (Skip Connections)\n",
    "A residual connection allows the input of a layer (or block) to be added directly to its output. \n",
    "\n",
    "`output = layers.Add()([layer_output, layer_input])`\n",
    "\n",
    "This creates a \"shortcut\" for the gradient, allowing it to flow directly back through the network. This makes it possible to train much deeper networks (e.g., 100+ layers) without suffering from the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1-7.2.5 Implementing the Minception Model\n",
    "\n",
    "We will now build the Minception model piece by piece using the Functional API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, MaxPool2D, AvgPool2D, Dense, \n",
    "    Concatenate, Flatten, BatchNormalization, Activation, Add\n",
    ")\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomCrop, RandomContrast\n",
    "\n",
    "# We'll use a standard initializer\n",
    "init = 'glorot_uniform'\n",
    "\n",
    "def bn_relu(inp):\n",
    "    \"\"\"Helper function for Batch Norm -> ReLU.\"\"\"\n",
    "    bn = BatchNormalization()(inp)\n",
    "    return Activation('relu')(bn)\n",
    "\n",
    "# 1. The Stem (based on Listing 7.6, simplified for clarity)\n",
    "def stem(inp, activation='relu', bn=True):\n",
    "    conv1_1 = Conv2D(32, (3,3), strides=(2,2), activation=None, kernel_initializer=init, padding='same')(inp)\n",
    "    conv1_1 = bn_relu(conv1_1)\n",
    "    conv1_2 = Conv2D(32, (3,3), strides=(1,1), activation=None, kernel_initializer=init, padding='same')(conv1_1)\n",
    "    conv1_2 = bn_relu(conv1_2)\n",
    "    conv1_3 = Conv2D(64, (3,3), strides=(1,1), activation=None, kernel_initializer=init, padding='same')(conv1_2)\n",
    "    conv1_3 = bn_relu(conv1_3)\n",
    "    \n",
    "    split_1_pool = MaxPool2D((3,3), strides=(2,2), padding='same')(conv1_3)\n",
    "    split_1_conv = Conv2D(96, (3,3), strides=(2,2), activation=None, kernel_initializer=init, padding='same')(conv1_3)\n",
    "    split_1_conv = bn_relu(split_1_conv)\n",
    "    \n",
    "    out_split_1 = Concatenate(axis=-1)([split_1_pool, split_1_conv])\n",
    "    # ... (omitting the rest of the complex stem for this summary)\n",
    "    # The book's stem is quite complex. We will use a simplified stem \n",
    "    # for this notebook to focus on the Inception-ResNet blocks.\n",
    "    return out_split_1\n",
    "\n",
    "# 2. Inception-ResNet Block A (based on Listing 7.7)\n",
    "def inception_resnet_a(inp, n_filters, activation='relu', bn=True, res_w=0.1):\n",
    "    # Branch 1 (1x1)\n",
    "    out1_1 = Conv2D(n_filters[0][0], (1,1), strides=(1,1), activation=None, kernel_initializer=init, padding='same')(inp)\n",
    "    out1_1 = bn_relu(out1_1)\n",
    "    \n",
    "    # Branch 2 (1x1 -> 3x3)\n",
    "    out2_1 = Conv2D(n_filters[1][0], (1,1), strides=(1,1), activation=None, kernel_initializer=init, padding='same')(inp)\n",
    "    out2_1 = bn_relu(out2_1)\n",
    "    out2_2 = Conv2D(n_filters[1][1], (3,3), strides=(1,1), activation=None, kernel_initializer=init, padding='same')(out2_1)\n",
    "    out2_2 = bn_relu(out2_2)\n",
    "\n",
    "    # Branch 3 (1x1 -> 3x3 -> 3x3)\n",
    "    out3_1 = Conv2D(n_filters[2][0], (1,1), strides=(1,1), activation=None, kernel_initializer=init, padding='same')(inp)\n",
    "    out3_1 = bn_relu(out3_1)\n",
    "    out3_2 = Conv2D(n_filters[2][1], (3,3), strides=(1,1), activation=None, kernel_initializer=init, padding='same')(out3_1)\n",
    "    out3_2 = bn_relu(out3_2)\n",
    "    out3_3 = Conv2D(n_filters[2][2], (3,3), strides=(1,1), activation=None, kernel_initializer=init, padding='same')(out3_2)\n",
    "    out3_3 = bn_relu(out3_3)\n",
    "    \n",
    "    # Concatenate all branches\n",
    "    out_concat = Concatenate(axis=-1)([out1_1, out2_2, out3_3])\n",
    "    \n",
    "    # Final 1x1 convolution (Linear activation)\n",
    "    out_final_conv = Conv2D(n_filters[3][0], (1,1), strides=(1,1), activation=None, kernel_initializer=init, padding='same')(out_concat)\n",
    "    \n",
    "    # --- Residual Connection ---\n",
    "    # Add the input (shortcut) to the output of the conv block\n",
    "    out_final = Add()([out_final_conv, inp])\n",
    "    out_final = Activation(activation)(out_final) # Apply activation *after* adding\n",
    "    return out_final\n",
    "\n",
    "# Note: Inception-ResNet-B and Reduction blocks are similar in principle\n",
    "# We will use just Block A for this simplified example.\n",
    "\n",
    "# 3. Build the full Minception model (Simplified from Listing 7.10)\n",
    "def build_minception(input_shape=(64, 64, 3), num_classes=200):\n",
    "    K.clear_session()\n",
    "    \n",
    "    inp = Input(shape=input_shape)\n",
    "    \n",
    "    # Preprocessing layers\n",
    "    crop_inp = RandomCrop(56, 56, seed=random_seed)(inp)\n",
    "    contrast_inp = RandomContrast(0.3, seed=random_seed)(crop_inp)\n",
    "    \n",
    "    # Stem\n",
    "    stem_out = stem(contrast_inp)\n",
    "    \n",
    "    # Body (A few Inception-ResNet blocks)\n",
    "    # Filter numbers are simplified from the book's version\n",
    "    inc_a_1 = inception_resnet_a(stem_out, [(32,),(32,32), (32, 48, 64),(288)], initializer=init)\n",
    "    inc_a_2 = inception_resnet_a(inc_a_1, [(32,),(32,32), (32, 48, 64),(288)], initializer=init)\n",
    "    \n",
    "    # Classification Head\n",
    "    avgpool1 = layers.GlobalAveragePooling2D()(inc_a_2)\n",
    "    dropout1 = layers.Dropout(0.5)(avgpool1)\n",
    "    out_main = Dense(num_classes, activation='softmax', kernel_initializer=init, name='final')(dropout1)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=out_main)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "minception_model = build_minception()\n",
    "minception_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.6 Training Minception\n",
    "\n",
    "When training this model, we introduce another callback: `ReduceLROnPlateau`. This will automatically reduce the learning rate (e.g., by a factor of 10) if the `val_loss` stops improving. This helps the model settle into a good minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks for Minception training\n",
    "es_callback_min = EarlyStopping(monitor='val_loss', patience=10)\n",
    "csv_logger_min = CSVLogger(os.path.join('eval', '3_eval_minception.log'))\n",
    "lr_callback_min = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.1, \n",
    "    patience=5, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Note: Minception has only 1 output, so we use the original (non-aux) generators\n",
    "train_gen_single = train_gen # From 7.1.1\n",
    "valid_gen_single = valid_gen # From 7.1.1\n",
    "\n",
    "print(\"Training Minception model...\")\n",
    "history_minception = minception_model.fit(\n",
    "    train_gen_single,\n",
    "    validation_data=valid_gen_single,\n",
    "    steps_per_epoch=get_steps_per_epoch(len(train_gen_single.filenames), batch_size),\n",
    "    validation_steps=get_steps_per_epoch(len(valid_gen_single.filenames), batch_size),\n",
    "    epochs=5, # Book uses 50, we use 5 for speed\n",
    "    callbacks=[es_callback_min, csv_logger_min, lr_callback_min]\n",
    ")\n",
    "print(\"Minception training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Transfer Learning: Using Pretrained Networks\n",
    "\n",
    "**Transfer Learning** is one of the most powerful techniques in deep learning. Instead of training a model from scratch, we use a model that has already been trained on a massive dataset (like ImageNet, with over 1 million images).\n",
    "\n",
    "The "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idea is that this model has already learned rich, general-purpose features (edges, textures, shapes). We can then "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use this model as a **feature extractor** and simply add a new, small classification head on top, which we train on our specific (and smaller) dataset.\n",
    "\n",
    "We will use the full `InceptionResNetV2` model, pretrained on ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# 1. Define the input shape required by InceptionResNetV2 (e.g., 224x224)\n",
    "INPUT_SHAPE = (224, 224, 3)\n",
    "\n",
    "# 2. Load the base model (pretrained on ImageNet)\n",
    "base_model = InceptionResNetV2(\n",
    "    include_top=False,     # <-- DO NOT include the final 1000-class ImageNet classifier\n",
    "    weights='imagenet',    # <-- Load pretrained weights\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    pooling='avg'        # <-- Apply Global Average Pooling to the output\n",
    ")\n",
    "\n",
    "# 3. Freeze the base model (optional, but good for initial training)\n",
    "# This prevents its weights from being updated.\n",
    "# base_model.trainable = False\n",
    "\n",
    "# 4. Create our new model\n",
    "model_tl = Sequential([\n",
    "    layers.Input(shape=INPUT_SHAPE), \n",
    "    base_model, # The pretrained base\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(200, activation='softmax') # Our new 200-class head\n",
    "])\n",
    "\n",
    "# 5. Compile with a low learning rate for fine-tuning\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model_tl.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model_tl.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create new data generators that resize our images to `(224, 224)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generators for 224x224 images (based on Listing 7.13)\n",
    "tl_target_size = (224, 224)\n",
    "tl_batch_size = 32 # Use a smaller batch size for this large model\n",
    "\n",
    "# We can re-use the image_gen_aug from section 7.1.1\n",
    "partial_flow_func_tl = partial(\n",
    "    image_gen_aug.flow_from_directory,\n",
    "    directory=train_image_dir,\n",
    "    target_size=tl_target_size, # New target size\n",
    "    class_mode='categorical',\n",
    "    batch_size=tl_batch_size,   # New batch size\n",
    "    shuffle=True,\n",
    "    seed=random_seed,\n",
    "    interpolation='bilinear'   # Specify interpolation for resizing\n",
    ")\n",
    "\n",
    "train_gen_tl = partial_flow_func_tl(subset='training')\n",
    "valid_gen_tl = partial_flow_func_tl(subset='validation')\n",
    "\n",
    "# We also need to re-create the auxiliary generators (with only 1 output)\n",
    "def data_gen_single(gen):\n",
    "    for x, y in gen:\n",
    "        yield x, y\n",
    "\n",
    "train_gen_tl_single = data_gen_single(train_gen_tl)\n",
    "valid_gen_tl_single = data_gen_single(valid_gen_tl)\n",
    "\n",
    "print(\"Training Transfer Learning model...\")\n",
    "history_tl = model_tl.fit(\n",
    "    train_gen_tl_single,\n",
    "    validation_data=valid_gen_tl_single,\n",
    "    steps_per_epoch=get_steps_per_epoch(len(train_gen_tl.filenames), tl_batch_size),\n",
    "    validation_steps=get_steps_per_epoch(len(valid_gen_tl.filenames), tl_batch_size),\n",
    "    epochs=3 # This will take a long time to train. We'll keep it short.\n",
    ")\n",
    "print(\"Transfer Learning training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Grad-CAM: Making CNNs Confess\n",
    "\n",
    "**Grad-CAM (Gradient-weighted Class Activation Mapping)** is a technique to visualize where a CNN is \"looking\" when it makes a prediction. It produces a heatmap that highlights the most important regions in the input image for a given class.\n",
    "\n",
    "**How it works (simplified):**\n",
    "1.  Get the model's prediction for an image.\n",
    "2.  Get the output feature map of the **last convolutional layer** (just before pooling and flatten).\n",
    "3.  Calculate the **gradient** of the predicted class's score with respect to the feature map from step 2.\n",
    "4.  Average these gradients for each feature map (channel) to get \"weights\" (this is `alpha_k` in the paper).\n",
    "5.  Compute a weighted sum of all the feature maps using these weights.\n",
    "6.  Apply a ReLU to the result (we only care about features that have a *positive* influence).\n",
    "7.  The result is a coarse heatmap, which we can resize and overlay on the original image.\n",
    "\n",
    "*(Note: The full code (based on Appendix B) is complex. We will implement the core logic here.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a trained model. Let's assume we saved our 'model_tl'.\n",
    "# model_tl.save(os.path.join('models', 'inception_resnet_v2_tl.h5'))\n",
    "# model = load_model(os.path.join('models', 'inception_resnet_v2_tl.h5'))\n",
    "\n",
    "# For this example, we'll just use the model_tl we just defined.\n",
    "model = model_tl\n",
    "\n",
    "# 1. Find the name of the last convolutional layer in the base model\n",
    "base_model = model.get_layer('inception_resnet_v2')\n",
    "last_conv_layer_name = \"conv_7b_ac\" # Found by inspecting base_model.summary()\n",
    "\n",
    "# 2. Create a new model that outputs the last conv layer's features and the final prediction\n",
    "grad_model = Model(\n",
    "    inputs=[base_model.input],\n",
    "    outputs=[base_model.get_layer(last_conv_layer_name).output, model.output]\n",
    ")\n",
    "\n",
    "# 3. Get a sample image\n",
    "x_sample_test, y_sample_test = next(iter(valid_gen_tl_single))\n",
    "sample_image = x_sample_test[0:1] # Get first image, keep batch dim\n",
    "sample_label_idx = np.argmax(y_sample_test[0])\n",
    "\n",
    "# 4. Use tf.GradientTape to get gradients\n",
    "with tf.GradientTape() as tape:\n",
    "    # Get the two outputs we defined\n",
    "    conv_outputs, predictions = grad_model(sample_image)\n",
    "    # Get the score for the predicted class\n",
    "    loss = predictions[:, sample_label_idx]\n",
    "\n",
    "# 5. Get the gradients of the score w.r.t the feature map\n",
    "grads = tape.gradient(loss, conv_outputs)\n",
    "\n",
    "# 6. Calculate channel weights (Global Average Pooling of gradients)\n",
    "weights = tf.reduce_mean(grads, axis=(1, 2), keepdims=True)\n",
    "\n",
    "# 7. Create the heatmap (weighted sum of feature maps)\n",
    "heatmap = conv_outputs * weights\n",
    "heatmap = tf.reduce_sum(heatmap, axis=-1) # Sum across channels\n",
    "\n",
    "# 8. Apply ReLU (we only want positive contributions)\n",
    "heatmap = tf.nn.relu(heatmap)\n",
    "\n",
    "# 9. Normalize\n",
    "heatmap /= tf.reduce_max(heatmap)\n",
    "heatmap = tf.squeeze(heatmap) # Remove batch dim\n",
    "\n",
    "print(\"Grad-CAM Heatmap generated.\")\n",
    "\n",
    "# 10. Visualize the heatmap and overlay it\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow((sample_image[0] + 1) / 2) # Un-normalize from [-1, 1] to [0, 1]\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow((sample_image[0] + 1) / 2)\n",
    "# Resize heatmap to match image and overlay it\n",
    "plt.imshow(tf.image.resize(heatmap[..., tf.newaxis], (224, 224)), cmap='jet', alpha=0.5)\n",
    "plt.title(\"Grad-CAM Overlay\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
