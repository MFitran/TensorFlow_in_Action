{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: Natural language processing with TensorFlow: Language modeling\n",
    "\n",
    "This notebook reproduces the code and summarizes the theoretical concepts from Chapter 10 of *'TensorFlow in Action'* by Thushan Ganegedara.\n",
    "\n",
    "This chapter focuses on **language modeling**â€”the task of predicting the next token (a word or character) in a sequence. This is a fundamental task in NLP that enables models to generate text.\n",
    "\n",
    "We will cover:\n",
    "1.  **Data Processing**: How to process a raw text corpus, use n-grams to manage vocabulary size, and build an efficient `tf.data` pipeline.\n",
    "2.  **Model Implementation**: Building a language model using a **Gated Recurrent Unit (GRU)**, which is similar to an LSTM.\n",
    "3.  **Model Evaluation**: Creating a custom **Perplexity** metric to evaluate the quality of the language model.\n",
    "4.  **Text Generation**: Using the trained model for inference, including **Greedy Decoding** and the more advanced **Beam Search**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Processing the Data\n",
    "\n",
    "Language modeling is an unsupervised task. The labels are generated from the data itself: the input is a sequence of tokens, and the target is the same sequence, shifted one step to the right. \n",
    "\n",
    "**Input**: `[ \"The\", \"cat\", \"sat\" ]`\n",
    "**Target**: `[ \"cat\", \"sat\", \"on\" ]`\n",
    "\n",
    "### 10.1.3 N-grams\n",
    "\n",
    "A major challenge in language modeling is large vocabulary size. A model that predicts the next *word* might have to choose from 50,000+ possibilities. The book uses **n-grams** (sequences of *n* characters) to solve this. \n",
    "\n",
    "Using 2-grams (bigrams), for example, dramatically reduces the vocabulary. The word \"hello\" becomes `[\"he\", \"ll\", \"o\"]` (with padding). This allows the model to handle a much smaller vocabulary and even create words it has never seen before by combining known n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import pickle\n",
    "\n",
    "# --- 1. Download and Read Data (Simulated from book) ---\n",
    "# The book uses the bAbI dataset. We'll simulate downloading and reading it.\n",
    "data_dir = os.path.join('data', 'lm', 'CBTest', 'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "train_path = os.path.join(data_dir, 'cbt_train.txt')\n",
    "valid_path = os.path.join(data_dir, 'cbt_valid.txt')\n",
    "test_path = os.path.join(data_dir, 'cbt_test.txt')\n",
    "\n",
    "# Create dummy data files for demonstration\n",
    "if not os.path.exists(train_path):\n",
    "    with open(train_path, 'w') as f:\n",
    "        f.write(\"_BOOK_TITLE_ A Simple Story\\n\")\n",
    "        f.write(\"Once upon a time, there was a fox.\\n\")\n",
    "        f.write(\"The fox was quick and brown.\\n\")\n",
    "        f.write(\"_BOOK_TITLE_ Another Story\\n\")\n",
    "        f.write(\"A dog and a cat were friends.\\n\")\n",
    "        f.write(\"They played in the yard.\\n\")\n",
    "\n",
    "    with open(valid_path, 'w') as f:\n",
    "        f.write(\"_BOOK_TITLE_ A Validation Story\\n\")\n",
    "        f.write(\"The sun was bright.\\n\")\n",
    "\n",
    "    with open(test_path, 'w') as f:\n",
    "        f.write(\"_BOOK_TITLE_ A Test Story\\n\")\n",
    "        f.write(\"The moon was full.\\n\")\n",
    "\n",
    "def read_data(path):\n",
    "    stories = []\n",
    "    with open(path, 'r') as f:\n",
    "        s = []\n",
    "        for row in f:\n",
    "            if row.startswith(\"_BOOK_TITLE_\"):\n",
    "                if len(s) > 0:\n",
    "                    stories.append(' '.join(s).lower())\n",
    "                s = []\n",
    "            s.append(row.strip()) # Add strip() to remove newlines\n",
    "        if len(s) > 0:\n",
    "            stories.append(' '.join(s).lower())\n",
    "    return stories\n",
    "\n",
    "stories = read_data(train_path)\n",
    "val_stories = read_data(valid_path)\n",
    "test_stories = read_data(test_path)\n",
    "\n",
    "print(f\"Loaded {len(stories)} training stories.\")\n",
    "print(stories[0][:100]) # Print first 100 chars of first story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. N-gram and Tokenizer Processing ---\n",
    "\n",
    "# Function to get n-grams\n",
    "def get_ngrams(text, n):\n",
    "    return [text[i:i+n] for i in range(0, len(text), n)]\n",
    "\n",
    "ngrams = 2 # We'll use 2-grams (bigrams)\n",
    "train_ngram_stories = [get_ngrams(s, ngrams) for s in stories]\n",
    "\n",
    "# Calculate vocabulary size (e.g., all n-grams appearing >= 10 times)\n",
    "# In our small demo, we'll use a threshold of 1\n",
    "text_corpus = chain(*train_ngram_stories)\n",
    "cnt = Counter(text_corpus)\n",
    "freq_df = pd.Series(list(cnt.values()), index=list(cnt.keys())).sort_values(ascending=False)\n",
    "\n",
    "n_vocab = (freq_df >= 1).sum() # For demo, use 1. Book uses 10.\n",
    "print(f\"\\nN-gram vocabulary size: {n_vocab}\")\n",
    "print(\"Most common n-grams:\")\n",
    "print(freq_df.head())\n",
    "\n",
    "# --- 3. Tokenize Data ---\n",
    "tokenizer = Tokenizer(num_words=n_vocab, oov_token='unk', lower=False)\n",
    "\n",
    "# Fit tokenizer on training n-grams\n",
    "tokenizer.fit_on_texts(train_ngram_stories)\n",
    "\n",
    "# Convert all datasets to sequences of integer IDs\n",
    "train_data_seq = tokenizer.texts_to_sequences(train_ngram_stories)\n",
    "\n",
    "val_ngram_stories = [get_ngrams(s, ngrams) for s in val_stories]\n",
    "val_data_seq = tokenizer.texts_to_sequences(val_ngram_stories)\n",
    "\n",
    "test_ngram_stories = [get_ngrams(s, ngrams) for s in test_stories]\n",
    "test_data_seq = tokenizer.texts_to_sequences(test_ngram_stories)\n",
    "\n",
    "print(\"\\nOriginal text:\")\n",
    "print(train_ngram_stories[0][:15])\n",
    "print(\"\\nTokenized sequence:\")\n",
    "print(train_data_seq[0][:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.5 Defining a `tf.data` pipeline\n",
    "\n",
    "We now create a pipeline that takes our long list of token sequences and turns it into `(input, target)` batches for training. \n",
    "\n",
    "1.  `from_tensor_slices`: Creates a dataset from our list of stories.\n",
    "2.  `flat_map` + `window`: This is the key part. It slides a `window` (of size `n_seq + 1`) across each story, creating many overlapping subsequences.\n",
    "3.  `shuffle`: Shuffles these windows.\n",
    "4.  `batch`: Groups the windows into batches.\n",
    "5.  `map`: Splits each window `[t_0, t_1, ..., t_n]` into an input `x = [t_0, ..., t_{n-1}]` and a target `y = [t_1, ..., t_n]`.\n",
    "6.  `prefetch`: Optimizes performance by pre-loading the next batch while the current one is processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Listing 10.3\n",
    "def get_tf_pipeline(data_seq, n_seq, batch_size=64, shift=1, shuffle=True):\n",
    "    \"\"\"Converts sequences of text IDs into (input, target) batches.\"\"\"\n",
    "    \n",
    "    # Use RaggedTensor to handle stories of different lengths\n",
    "    text_ds = tf.data.Dataset.from_tensor_slices(tf.ragged.constant(data_seq))\n",
    "    \n",
    "    if shuffle:\n",
    "        text_ds = text_ds.shuffle(buffer_size=len(data_seq) // 2)\n",
    "    \n",
    "    # Use flat_map to apply windowing to each story individually\n",
    "    text_ds = text_ds.flat_map(\n",
    "        lambda x: tf.data.Dataset.from_tensor_slices(x).window(\n",
    "            n_seq + 1, shift=shift, drop_remainder=True\n",
    "        ).flat_map(\n",
    "            lambda window: window.batch(n_seq + 1, drop_remainder=True)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    if shuffle:\n",
    "        text_ds = text_ds.shuffle(buffer_size=10 * batch_size)\n",
    "    \n",
    "    text_ds = text_ds.batch(batch_size)\n",
    "    \n",
    "    # Split into (x, y) pairs where y is x shifted by one\n",
    "    text_ds = text_ds.map(lambda x: (x[:, :-1], x[:, 1:]))\n",
    "    \n",
    "    text_ds = text_ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return text_ds\n",
    "\n",
    "# Set hyperparameters\n",
    "n_seq = 100 # Sequence length for the model\n",
    "batch_size = 128\n",
    "\n",
    "train_ds = get_tf_pipeline(train_data_seq, n_seq, batch_size=batch_size, shuffle=True)\n",
    "valid_ds = get_tf_pipeline(val_data_seq, n_seq, batch_size=batch_size)\n",
    "test_ds = get_tf_pipeline(test_data_seq, n_seq, batch_size=batch_size)\n",
    "\n",
    "# Inspect a batch\n",
    "for x_batch, y_batch in train_ds.take(1):\n",
    "    print(f\"X batch shape: {x_batch.shape}\")\n",
    "    print(f\"Y batch shape: {y_batch.shape}\")\n",
    "    print(f\"\\nExample X: {x_batch[0, :10]}\")\n",
    "    print(f\"Example Y: {y_batch[0, :10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 GRUs in Wonderland: Generating text with deep learning\n",
    "\n",
    "A **Gated Recurrent Unit (GRU)** is a type of recurrent neural network (RNN), similar to an LSTM. It's designed to learn from sequences and remember information over long periods. It's slightly simpler than an LSTM, using two gates (an *update gate* and a *reset gate*) instead of three, and one hidden state instead of two. This often makes it faster to train with comparable performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Listing 10.4\n",
    "K_.clear_session()\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=n_vocab + 1, # +1 for the padding token (ID 0)\n",
    "        output_dim=512,\n",
    "        input_shape=(None,) # (None,) means it can accept sequences of any length\n",
    "    ),\n",
    "    \n",
    "    # return_sequences=True is critical.\n",
    "    # It makes the GRU output a prediction for *every* token in the sequence, \n",
    "    # not just the very last one.\n",
    "    layers.GRU(1024, return_state=False, return_sequences=True),\n",
    "    \n",
    "    layers.Dense(512, activation='relu'),\n",
    "    \n",
    "    # The final layer predicts the next token ID from the entire vocabulary\n",
    "    layers.Dense(n_vocab, name='final_out'),\n",
    "    layers.Activation('softmax') # Use softmax to get probabilities\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Measuring the quality of the generated text\n",
    "\n",
    "Simple accuracy is a poor metric for language models. If the correct next word is \"dog\" and the model predicts \"cat,\" the accuracy is 0, but the prediction is semantically reasonable.\n",
    "\n",
    "A better metric is **Perplexity (PPL)**, which measures how \"surprised\" or \"confused\" the model is by the true target sequence. It's derived from the cross-entropy (CE) loss:\n",
    "\n",
    "$$PPL = e^{\\text{CE_Loss}}$$ \n",
    "\n",
    "A lower perplexity is better. A PPL of 100 means the model is, on average, as confused as if it were randomly guessing between 100 different words at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Listing 10.5: Custom Perplexity Metric\n",
    "class PerplexityMetric(tf.keras.metrics.Mean):\n",
    "    def __init__(self, name='perplexity', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        # We use sparse categorical crossentropy because our y_true (targets) \n",
    "        # are integers, not one-hot vectors.\n",
    "        self.cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=False, reduction='none'\n",
    "        )\n",
    "\n",
    "    def _calculate_perplexity(self, real, pred):\n",
    "        # Calculate the cross-entropy loss for each token\n",
    "        loss_ = self.cross_entropy(real, pred)\n",
    "        \n",
    "        # Get the mean loss across the sequence\n",
    "        mean_loss = K.mean(loss_, axis=-1)\n",
    "        \n",
    "        # Perplexity is the exponential of the mean loss\n",
    "        perplexity = K.exp(mean_loss)\n",
    "        return perplexity\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        perplexity = self._calculate_perplexity(y_true, y_pred)\n",
    "        super().update_state(perplexity, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Training and evaluating the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy', PerplexityMetric()]\n",
    ")\n",
    "\n",
    "# Define callbacks\n",
    "os.makedirs('eval', exist_ok=True)\n",
    "csv_logger = CSVLogger(os.path.join('eval', '1_language_modelling.log'))\n",
    "es_callback = EarlyStopping(monitor='val_perplexity', patience=5, mode='min')\n",
    "lr_callback = ReduceLROnPlateau(monitor='val_perplexity', factor=0.1, patience=2, mode='min')\n",
    "\n",
    "# Train the model (only 3 epochs for this demo, book uses 50)\n",
    "print(\"Starting model training...\")\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=3, \n",
    "    validation_data=valid_ds,\n",
    "    callbacks=[es_callback, lr_callback, csv_logger]\n",
    ")\n",
    "\n",
    "# Evaluate on the test set\n",
    "print(\"\\nEvaluating model on test set...\")\n",
    "model.evaluate(test_ds)\n",
    "\n",
    "# Save the model and tokenizer\n",
    "os.makedirs('models', exist_ok=True)\n",
    "model.save(os.path.join('models', '2_gram_lm.h5'))\n",
    "\n",
    "with open(os.path.join('models', 'text_hyperparams.pkl'), 'wb') as f:\n",
    "    pickle.dump({'n_vocab': n_vocab, 'ngrams': ngrams, 'n_seq': n_seq}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 Generating new text from the language model: Greedy decoding\n",
    "\n",
    "For **inference** (text generation), we can't use `model.fit()` or `model.predict()` on a whole sequence. We need to generate one token at a time, feed that token back into the model, and get the next one.\n",
    "\n",
    "This requires a new model that:\n",
    "1.  Takes the previous token(s) **and** the GRU's previous hidden state as input.\n",
    "2.  Outputs the prediction (logits) **and** the new hidden state.\n",
    "\n",
    "**Greedy Decoding** is the simplest method: at each step, we just pick the single token with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Re-build the model for inference using the Functional API\n",
    "K_.clear_session()\n",
    "\n",
    "trained_model = load_model(os.path.join('models', '2_gram_lm.h5'), \n",
    "                           custom_objects={'PerplexityMetric': PerplexityMetric})\n",
    "\n",
    "# Define inputs\n",
    "inp = tf.keras.layers.Input(shape=(None,))\n",
    "inp_state = tf.keras.layers.Input(shape=(1024,)) # 1024 is the GRU units\n",
    "\n",
    "# Get layers from the trained model\n",
    "emb_layer = trained_model.get_layer('embedding')\n",
    "gru_layer = trained_model.get_layer('gru')\n",
    "dense_layer_1 = trained_model.get_layer('dense')\n",
    "final_layer = trained_model.get_layer('final_out')\n",
    "softmax_layer = trained_model.get_layer('activation')\n",
    "\n",
    "# Set return_state=True for the GRU layer\n",
    "gru_layer.return_state = True\n",
    "\n",
    "# Build the graph\n",
    "emb_out = emb_layer(inp)\n",
    "gru_out, gru_state = gru_layer(emb_out, initial_state=inp_state)\n",
    "dense_out = dense_layer_1(gru_out)\n",
    "final_out = final_layer(dense_out)\n",
    "softmax_out = softmax_layer(final_out)\n",
    "\n",
    "infer_model = tf.keras.models.Model(\n",
    "    inputs=[inp, inp_state],\n",
    "    outputs=[softmax_out, gru_state]\n",
    ")\n",
    "\n",
    "infer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Write the Greedy Decoding loop (based on Listing 10.7)\n",
    "\n",
    "def generate_text_greedy(seed_text, n_to_generate=50):\n",
    "    print(f\"Seed text: '{seed_text}'\\n\")\n",
    "    text = get_ngrams(seed_text.lower(), ngrams)\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    \n",
    "    # Initialize the state\n",
    "    state = np.zeros(shape=(1, 1024))\n",
    "    \n",
    "    # Feed the seed text to the model to \"warm up\" the state\n",
    "    for i in range(len(seq[0]) - 1):\n",
    "        x_in = np.array([[seq[0][i]]])\n",
    "        out, state = infer_model.predict([x_in, state])\n",
    "    \n",
    "    # Start generating from the last token of the seed text\n",
    "    x = np.array([[seq[0][-1]]])\n",
    "    generated_text = list(text)\n",
    "\n",
    "    for _ in range(n_to_generate):\n",
    "        out, state = infer_model.predict([x, state])\n",
    "        \n",
    "        # Greedy step: get the ID of the most probable next token\n",
    "        wid = int(np.argmax(out[0], axis=-1).ravel())\n",
    "        \n",
    "        # Stop if we predict 'unk' or 0 (padding)\n",
    "        if wid == 0 or wid == tokenizer.word_index['unk']:\n",
    "            break\n",
    "            \n",
    "        word = tokenizer.index_word[wid]\n",
    "        generated_text.append(word)\n",
    "        \n",
    "        # The new input is the word we just predicted\n",
    "        x = np.array([[wid]])\n",
    "        \n",
    "    print(\"Generated text:\")\n",
    "    print(''.join(generated_text))\n",
    "\n",
    "generate_text_greedy(\"the dog was\", n_to_generate=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6 Beam search: Enhancing the predictive power of sequential models\n",
    "\n",
    "Greedy decoding has a major flaw: it's not optimal. It might pick a word that seems good *now* but leads to a dead end later (e.g., \"the dog was **ru**\" -> \"the dog was **running**\" vs. \"the dog was **ru**\" -> \"the dog was **ru**g\").\n",
    "\n",
    "**Beam Search** improves this by keeping track of the *k* (e.g., *k*=3) most probable sequences at each step. \n",
    "\n",
    "1.  **Step 1**: Get the top 3 most likely next words (e.g., \"running\", \"barking\", \"sleeping\").\n",
    "2.  **Step 2**: For *each* of those 3 sequences, predict the *next* top 3 words. This gives $3 \\times 3 = 9$ candidate sequences.\n",
    "3.  **Step 3**: Rank all 9 sequences by their combined probability and keep only the new top 3.\n",
    "4.  Repeat.\n",
    "\n",
    "This is a more complex, recursive function (based on Listing 10.8) that explores more of the search space and usually produces more coherent text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Listing 10.8 (Conceptual implementation)\n",
    "def beam_search(\n",
    "    model, input_token_id, initial_state, beam_depth=5, beam_width=3\n",
    "):\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    # This recursive function explores the search tree\n",
    "    def recursive_fn(input_token_id, state, sequence, log_prob, i):\n",
    "        if i == beam_depth: # Base case: reached end of search\n",
    "            results.append((list(sequence), state, np.exp(log_prob)))\n",
    "            return\n",
    "        \n",
    "        # Get predictions from the model\n",
    "        output, new_state = model.predict([np.array([[input_token_id]]), state])\n",
    "        \n",
    "        # Get the top 'k' (beam_width) most probable next tokens\n",
    "        top_probs, top_ids = tf.nn.top_k(output[0], k=beam_width)\n",
    "        top_probs = top_probs.numpy().ravel()\n",
    "        top_ids = top_ids.numpy().ravel()\n",
    "        \n",
    "        # For each of the top 'k' tokens, continue the search\n",
    "        for p, wid in zip(top_probs, top_ids):\n",
    "            if p < 1e-6: continue # Avoid log(0)\n",
    "            \n",
    "            new_log_prob = log_prob + np.log(p)\n",
    "            sequence.append(wid)\n",
    "            recursive_fn(wid, new_state, sequence, new_log_prob, i + 1)\n",
    "            sequence.pop() # Backtrack\n",
    "\n",
    "    # Start the recursive search\n",
    "    recursive_fn(input_token_id, initial_state, sequence=[], log_prob=0.0, i=0)\n",
    "    \n",
    "    # Sort all completed beams by their probability (highest first)\n",
    "    results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "    return results\n",
    "\n",
    "print(\"Beam Search function defined.\")\n",
    "\n",
    "# --- Generate text using Beam Search (Listing 10.9) ---\n",
    "seed_text = \"the dog was\"\n",
    "print(f\"Generating with Beam Search from seed: '{seed_text}'\\n\")\n",
    "\n",
    "text = get_ngrams(seed_text.lower(), ngrams)\n",
    "seq = tokenizer.texts_to_sequences([text])\n",
    "state = np.zeros(shape=(1, 1024))\n",
    "\n",
    "generated_text = list(text)\n",
    "\n",
    "# Warm up the state\n",
    "for i in range(len(seq[0])):\n",
    "    x_in = np.array([[seq[0][i]]])\n",
    "    out, state = infer_model.predict([x_in, state])\n",
    "\n",
    "x = np.array([[seq[0][-1]]]) # Start generating from the last *real* token\n",
    "\n",
    "for i in range(10): # Generate 10 *sequences* (e.g., 10 * 5 tokens)\n",
    "    # Get the top beams (e.g., 3 beams, each 5 tokens long)\n",
    "    result = beam_search(infer_model, x, state, beam_depth=5, beam_width=3)\n",
    "    \n",
    "    # Choose the best beam\n",
    "    best_beam_ids, state, prob = result[0]\n",
    "    \n",
    "    # Use the last token of the best beam as the new input\n",
    "    x = np.array([[best_beam_ids[-1]]])\n",
    "    \n",
    "    # Append the predicted n-grams to our text\n",
    "    generated_text.extend([tokenizer.index_word[w] for w in best_beam_ids])\n",
    "    \n",
    "print(\"Generated text (Beam Search):\")\n",
    "print(''.join(generated_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
