{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yje9qTxlNN_Q"
      },
      "source": [
        "# Chapter 10: Natural language processing with TensorFlow: Language modeling\n",
        "\n",
        "This notebook reproduces the code and summarizes the theoretical concepts from Chapter 10 of *'TensorFlow in Action'* by Thushan Ganegedara.\n",
        "\n",
        "This chapter focuses on **language modeling**—the task of predicting the next token (a word or character) in a sequence. This is a fundamental task in NLP that enables models to generate text.\n",
        "\n",
        "We will cover:\n",
        "1.  **Data Processing**: How to process a raw text corpus, use n-grams to manage vocabulary size, and build an efficient `tf.data` pipeline.\n",
        "2.  **Model Implementation**: Building a language model using a **Gated Recurrent Unit (GRU)**, which is similar to an LSTM.\n",
        "3.  **Model Evaluation**: Creating a custom **Perplexity** metric to evaluate the quality of the language model.\n",
        "4.  **Text Generation**: Using the trained model for inference, including **Greedy Decoding** and the more advanced **Beam Search**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn4SiNjnNN_V"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfxYjC5ANN_W"
      },
      "source": [
        "## 10.1 Processing the Data\n",
        "\n",
        "Language modeling is an unsupervised task. The labels are generated from the data itself: the input is a sequence of tokens, and the target is the same sequence, shifted one step to the right.\n",
        "\n",
        "**Input**: `[ \"The\", \"cat\", \"sat\" ]`\n",
        "**Target**: `[ \"cat\", \"sat\", \"on\" ]`\n",
        "\n",
        "### 10.1.3 N-grams\n",
        "\n",
        "A major challenge in language modeling is large vocabulary size. A model that predicts the next *word* might have to choose from 50,000+ possibilities. The book uses **n-grams** (sequences of *n* characters) to solve this.\n",
        "\n",
        "Using 2-grams (bigrams), for example, dramatically reduces the vocabulary. The word \"hello\" becomes `[\"he\", \"ll\", \"o\"]` (with padding). This allows the model to handle a much smaller vocabulary and even create words it has never seen before by combining known n-grams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTbQBaDRNN_W",
        "outputId": "5f33da1b-2465-46b2-a396-a478b539bd33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 2 training stories.\n",
            "_book_title_ a simple story once upon a time, there was a fox. the fox was quick and brown.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "import os\n",
        "import requests\n",
        "import tarfile\n",
        "from collections import Counter\n",
        "from itertools import chain\n",
        "import pickle\n",
        "\n",
        "# --- 1. Download and Read Data (Simulated from book) ---\n",
        "# The book uses the bAbI dataset. We'll simulate downloading and reading it.\n",
        "data_dir = os.path.join('data', 'lm', 'CBTest', 'data')\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "train_path = os.path.join(data_dir, 'cbt_train.txt')\n",
        "valid_path = os.path.join(data_dir, 'cbt_valid.txt')\n",
        "test_path = os.path.join(data_dir, 'cbt_test.txt')\n",
        "\n",
        "# Create dummy data files for demonstration\n",
        "if not os.path.exists(train_path):\n",
        "    with open(train_path, 'w') as f:\n",
        "        f.write(\"_BOOK_TITLE_ A Simple Story\\n\")\n",
        "        f.write(\"Once upon a time, there was a fox.\\n\")\n",
        "        f.write(\"The fox was quick and brown.\\n\")\n",
        "        f.write(\"_BOOK_TITLE_ Another Story\\n\")\n",
        "        f.write(\"A dog and a cat were friends.\\n\")\n",
        "        f.write(\"They played in the yard.\\n\")\n",
        "\n",
        "    with open(valid_path, 'w') as f:\n",
        "        f.write(\"_BOOK_TITLE_ A Validation Story\\n\")\n",
        "        f.write(\"The sun was bright.\\n\")\n",
        "\n",
        "    with open(test_path, 'w') as f:\n",
        "        f.write(\"_BOOK_TITLE_ A Test Story\\n\")\n",
        "        f.write(\"The moon was full.\\n\")\n",
        "\n",
        "def read_data(path):\n",
        "    stories = []\n",
        "    with open(path, 'r') as f:\n",
        "        s = []\n",
        "        for row in f:\n",
        "            if row.startswith(\"_BOOK_TITLE_\"):\n",
        "                if len(s) > 0:\n",
        "                    stories.append(' '.join(s).lower())\n",
        "                s = []\n",
        "            s.append(row.strip()) # Add strip() to remove newlines\n",
        "        if len(s) > 0:\n",
        "            stories.append(' '.join(s).lower())\n",
        "    return stories\n",
        "\n",
        "stories = read_data(train_path)\n",
        "val_stories = read_data(valid_path)\n",
        "test_stories = read_data(test_path)\n",
        "\n",
        "print(f\"Loaded {len(stories)} training stories.\")\n",
        "print(stories[0][:100]) # Print first 100 chars of first story"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTvPmslRNN_Y",
        "outputId": "91b5ba24-146d-49c4-dd16-ecaede1db77f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "N-gram vocabulary size: 57\n",
            "Most common n-grams:\n",
            " a    6\n",
            "e     4\n",
            "ti    3\n",
            "er    3\n",
            "th    3\n",
            "dtype: int64\n",
            "\n",
            "Original text:\n",
            "['_b', 'oo', 'k_', 'ti', 'tl', 'e_', ' a', ' s', 'im', 'pl', 'e ', 'st', 'or', 'y ', 'on']\n",
            "\n",
            "Tokenized sequence:\n",
            "[8, 9, 10, 4, 11, 12, 2, 13, 22, 14, 3, 23, 24, 15, 25]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# --- 2. N-gram and Tokenizer Processing ---\n",
        "\n",
        "# Function to get n-grams\n",
        "def get_ngrams(text, n):\n",
        "    return [text[i:i+n] for i in range(0, len(text), n)]\n",
        "\n",
        "ngrams = 2 # We'll use 2-grams (bigrams)\n",
        "train_ngram_stories = [get_ngrams(s, ngrams) for s in stories]\n",
        "\n",
        "# Calculate vocabulary size (e.g., all n-grams appearing >= 10 times)\n",
        "# In our small demo, we'll use a threshold of 1\n",
        "text_corpus = chain(*train_ngram_stories)\n",
        "cnt = Counter(text_corpus)\n",
        "freq_df = pd.Series(list(cnt.values()), index=list(cnt.keys())).sort_values(ascending=False)\n",
        "\n",
        "n_vocab = (freq_df >= 1).sum() # For demo, use 1. Book uses 10.\n",
        "print(f\"\\nN-gram vocabulary size: {n_vocab}\")\n",
        "print(\"Most common n-grams:\")\n",
        "print(freq_df.head())\n",
        "\n",
        "# --- 3. Tokenize Data ---\n",
        "tokenizer = Tokenizer(num_words=n_vocab, oov_token='unk', lower=False)\n",
        "\n",
        "# Fit tokenizer on training n-grams\n",
        "tokenizer.fit_on_texts(train_ngram_stories)\n",
        "\n",
        "# Convert all datasets to sequences of integer IDs\n",
        "train_data_seq = tokenizer.texts_to_sequences(train_ngram_stories)\n",
        "\n",
        "val_ngram_stories = [get_ngrams(s, ngrams) for s in val_stories]\n",
        "val_data_seq = tokenizer.texts_to_sequences(val_ngram_stories)\n",
        "\n",
        "test_ngram_stories = [get_ngrams(s, ngrams) for s in test_stories]\n",
        "test_data_seq = tokenizer.texts_to_sequences(test_ngram_stories)\n",
        "\n",
        "print(\"\\nOriginal text:\")\n",
        "print(train_ngram_stories[0][:15])\n",
        "print(\"\\nTokenized sequence:\")\n",
        "print(train_data_seq[0][:15])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_t3BQ7jNN_Z"
      },
      "source": [
        "### 10.1.5 Defining a `tf.data` pipeline\n",
        "\n",
        "We now create a pipeline that takes our long list of token sequences and turns it into `(input, target)` batches for training.\n",
        "\n",
        "1.  `from_tensor_slices`: Creates a dataset from our list of stories.\n",
        "2.  `flat_map` + `window`: This is the key part. It slides a `window` (of size `n_seq + 1`) across each story, creating many overlapping subsequences.\n",
        "3.  `shuffle`: Shuffles these windows.\n",
        "4.  `batch`: Groups the windows into batches.\n",
        "5.  `map`: Splits each window `[t_0, t_1, ..., t_n]` into an input `x = [t_0, ..., t_{n-1}]` and a target `y = [t_1, ..., t_n]`.\n",
        "6.  `prefetch`: Optimizes performance by pre-loading the next batch while the current one is processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxKUJ57HNN_Z",
        "outputId": "f4894a57-dc28-4625-b803-33c4336c8549"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X batch shape: (67, 10)\n",
            "Y batch shape: (67, 10)\n",
            "\n",
            "Example X: [44  5  6 13 45 46  2 47 48  2]\n",
            "Example Y: [ 5  6 13 45 46  2 47 48  2  7]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "import os\n",
        "import requests\n",
        "import tarfile\n",
        "from collections import Counter\n",
        "from itertools import chain\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "# --- 1. Download and Read Data (Simulated from book) ---\n",
        "# The book uses the bAbI dataset. We'll simulate downloading and reading it.\n",
        "data_dir = os.path.join('data', 'lm', 'CBTest', 'data')\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "train_path = os.path.join(data_dir, 'cbt_train.txt')\n",
        "valid_path = os.path.join(data_dir, 'cbt_valid.txt')\n",
        "test_path = os.path.join(data_dir, 'cbt_test.txt')\n",
        "\n",
        "# Create dummy data files for demonstration\n",
        "if not os.path.exists(train_path):\n",
        "    with open(train_path, 'w') as f:\n",
        "        f.write(\"_BOOK_TITLE_ A Simple Story\\n\")\n",
        "        f.write(\"Once upon a time, there was a fox.\\n\")\n",
        "        f.write(\"The fox was quick and brown.\\n\")\n",
        "        f.write(\"_BOOK_TITLE_ Another Story\\n\")\n",
        "        f.write(\"A dog and a cat were friends.\\n\")\n",
        "        f.write(\"They played in the yard.\\n\")\n",
        "\n",
        "    with open(valid_path, 'w') as f:\n",
        "        f.write(\"_BOOK_TITLE_ A Validation Story\\n\")\n",
        "        f.write(\"The sun was bright.\\n\")\n",
        "\n",
        "    with open(test_path, 'w') as f:\n",
        "        f.write(\"_BOOK_TITLE_ A Test Story\\n\")\n",
        "        f.write(\"The moon was full.\\n\")\n",
        "\n",
        "def read_data(path):\n",
        "    stories = []\n",
        "    with open(path, 'r') as f:\n",
        "        s = []\n",
        "        for row in f:\n",
        "            if row.startswith(\"_BOOK_TITLE_\"):\n",
        "                if len(s) > 0:\n",
        "                    stories.append(' '.join(s).lower())\n",
        "                s = []\n",
        "            s.append(row.strip()) # Add strip() to remove newlines\n",
        "        if len(s) > 0:\n",
        "            stories.append(' '.join(s).lower())\n",
        "    return stories\n",
        "\n",
        "stories = read_data(train_path)\n",
        "val_stories = read_data(valid_path)\n",
        "test_stories = read_data(test_path)\n",
        "\n",
        "# --- 2. N-gram and Tokenizer Processing ---\n",
        "\n",
        "# Function to get n-grams\n",
        "def get_ngrams(text, n):\n",
        "    return [text[i:i+n] for i in range(0, len(text), n)]\n",
        "\n",
        "ngrams = 2 # We'll use 2-grams (bigrams)\n",
        "train_ngram_stories = [get_ngrams(s, ngrams) for s in stories]\n",
        "\n",
        "# Calculate vocabulary size (e.g., all n-grams appearing >= 10 times)\n",
        "# In our small demo, we'll use a threshold of 1\n",
        "text_corpus = chain(*train_ngram_stories)\n",
        "cnt = Counter(text_corpus)\n",
        "freq_df = pd.Series(list(cnt.values()), index=list(cnt.keys())).sort_values(ascending=False)\n",
        "\n",
        "n_vocab = (freq_df >= 1).sum() # For demo, use 1. Book uses 10.\n",
        "\n",
        "# --- 3. Tokenize Data ---\n",
        "tokenizer = Tokenizer(num_words=n_vocab, oov_token='unk', lower=False)\n",
        "\n",
        "# Fit tokenizer on training n-grams\n",
        "tokenizer.fit_on_texts(train_ngram_stories)\n",
        "\n",
        "# Convert all datasets to sequences of integer IDs\n",
        "train_data_seq = tokenizer.texts_to_sequences(train_ngram_stories)\n",
        "\n",
        "val_ngram_stories = [get_ngrams(s, ngrams) for s in val_stories]\n",
        "val_data_seq = tokenizer.texts_to_sequences(val_ngram_stories)\n",
        "\n",
        "test_ngram_stories = [get_ngrams(s, ngrams) for s in test_stories]\n",
        "test_data_seq = tokenizer.texts_to_sequences(test_ngram_stories)\n",
        "\n",
        "# Based on Listing 10.3\n",
        "def get_tf_pipeline(data_seq, n_seq, batch_size=64, shift=1, shuffle=True):\n",
        "    \"\"\"Converts sequences of text IDs into (input, target) batches.\"\"\"\n",
        "\n",
        "    # Use RaggedTensor to handle stories of different lengths\n",
        "    text_ds = tf.data.Dataset.from_tensor_slices(tf.ragged.constant(data_seq))\n",
        "\n",
        "    if shuffle:\n",
        "        # Ensure buffer_size is at least 1, even for very small datasets\n",
        "        buffer_size_stories = max(1, len(data_seq) // 2)\n",
        "        text_ds = text_ds.shuffle(buffer_size=buffer_size_stories)\n",
        "\n",
        "    # Use flat_map to apply windowing to each story individually\n",
        "    text_ds = text_ds.flat_map(\n",
        "        lambda x: tf.data.Dataset.from_tensor_slices(x).window(\n",
        "            n_seq + 1, shift=shift, drop_remainder=True\n",
        "        ).flat_map(\n",
        "            lambda window: window.batch(n_seq + 1, drop_remainder=True)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if shuffle:\n",
        "        # Ensure buffer_size is at least 1\n",
        "        buffer_size_batches = max(1, 10 * batch_size)\n",
        "        text_ds = text_ds.shuffle(buffer_size=buffer_size_batches)\n",
        "\n",
        "    text_ds = text_ds.batch(batch_size)\n",
        "\n",
        "    # Split into (x, y) pairs where y is x shifted by one\n",
        "    text_ds = text_ds.map(lambda x: (x[:, :-1], x[:, 1:]))\n",
        "\n",
        "    # Add .repeat() for training datasets to ensure multiple epochs\n",
        "    if shuffle: # Only repeat for training dataset, not validation/test\n",
        "        text_ds = text_ds.repeat()\n",
        "\n",
        "    text_ds = text_ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "    return text_ds\n",
        "\n",
        "# Set hyperparameters\n",
        "n_seq = 10 # Sequence length for the model (changed from 100 to 10)\n",
        "batch_size = 128\n",
        "\n",
        "train_ds = get_tf_pipeline(train_data_seq, n_seq, batch_size=batch_size, shuffle=True)\n",
        "valid_ds = get_tf_pipeline(val_data_seq, n_seq, batch_size=batch_size)\n",
        "test_ds = get_tf_pipeline(test_data_seq, n_seq, batch_size=batch_size)\n",
        "\n",
        "# Inspect a batch\n",
        "for x_batch, y_batch in train_ds.take(1):\n",
        "    print(f\"X batch shape: {x_batch.shape}\")\n",
        "    print(f\"Y batch shape: {y_batch.shape}\")\n",
        "    print(f\"\\nExample X: {x_batch[0, :10]}\")\n",
        "    print(f\"Example Y: {y_batch[0, :10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THfigj2ONN_a"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71kUJi6ANN_a"
      },
      "source": [
        "## 10.2 GRUs in Wonderland: Generating text with deep learning\n",
        "\n",
        "A **Gated Recurrent Unit (GRU)** is a type of recurrent neural network (RNN), similar to an LSTM. It's designed to learn from sequences and remember information over long periods. It's slightly simpler than an LSTM, using two gates (an *update gate* and a *reset gate*) instead of three, and one hidden state instead of two. This often makes it faster to train with comparable performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "ioAm9_OINN_b",
        "outputId": "737bb6cd-5b6a-4d45-ba49-8119a3905852"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:100: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │        \u001b[38;5;34m29,696\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │     \u001b[38;5;34m4,724,736\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │       \u001b[38;5;34m524,800\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ final_out (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m57\u001b[0m)       │        \u001b[38;5;34m29,241\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m57\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">29,696</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,724,736</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ final_out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">57</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">29,241</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">57</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,308,473\u001b[0m (20.25 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,308,473</span> (20.25 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,308,473\u001b[0m (20.25 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,308,473</span> (20.25 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Based on Listing 10.4\n",
        "K.clear_session()\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Embedding(\n",
        "        input_dim=n_vocab + 1, # +1 for the padding token (ID 0)\n",
        "        output_dim=512,\n",
        "        input_shape=(None,) # (None,) means it can accept sequences of any length\n",
        "    ),\n",
        "\n",
        "    # return_sequences=True is critical.\n",
        "    # It makes the GRU output a prediction for *every* token in the sequence,\n",
        "    # not just the very last one.\n",
        "    layers.GRU(1024, return_state=False, return_sequences=True),\n",
        "\n",
        "    layers.Dense(512, activation='relu'),\n",
        "\n",
        "    # The final layer predicts the next token ID from the entire vocabulary\n",
        "    layers.Dense(n_vocab, name='final_out'),\n",
        "    layers.Activation('softmax') # Use softmax to get probabilities\n",
        "])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKDXufe7NN_b"
      },
      "source": [
        "## 10.3 Measuring the quality of the generated text\n",
        "\n",
        "Simple accuracy is a poor metric for language models. If the correct next word is \"dog\" and the model predicts \"cat,\" the accuracy is 0, but the prediction is semantically reasonable.\n",
        "\n",
        "A better metric is **Perplexity (PPL)**, which measures how \"surprised\" or \"confused\" the model is by the true target sequence. It's derived from the cross-entropy (CE) loss:\n",
        "\n",
        "$$PPL = e^{\\text{CE_Loss}}$$\n",
        "\n",
        "A lower perplexity is better. A PPL of 100 means the model is, on average, as confused as if it were randomly guessing between 100 different words at each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BAhIaBrUNN_c"
      },
      "outputs": [],
      "source": [
        "# Based on Listing 10.5: Custom Perplexity Metric\n",
        "class PerplexityMetric(tf.keras.metrics.Mean):\n",
        "    def __init__(self, name='perplexity', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        # We use sparse categorical crossentropy because our y_true (targets)\n",
        "        # are integers, not one-hot vectors.\n",
        "        self.cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "            from_logits=False, reduction='none'\n",
        "        )\n",
        "\n",
        "    def _calculate_perplexity(self, real, pred):\n",
        "        # Calculate the cross-entropy loss for each token\n",
        "        loss_ = self.cross_entropy(real, pred)\n",
        "\n",
        "        # Get the mean loss across the sequence\n",
        "        mean_loss = K.mean(loss_, axis=-1)\n",
        "\n",
        "        # Perplexity is the exponential of the mean loss\n",
        "        perplexity = K.exp(mean_loss)\n",
        "        return perplexity\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        perplexity = self._calculate_perplexity(y_true, y_pred)\n",
        "        super().update_state(perplexity, sample_weight=sample_weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qat9_VrNN_c"
      },
      "source": [
        "## 10.4 Training and evaluating the language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "FkdLyKVXNN_d",
        "outputId": "ca381b93-d98f-435a-b116-081816353460"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting model training...\n",
            "  19634/Unknown \u001b[1m19994s\u001b[0m 1s/step - accuracy: 0.9655 - loss: 0.0546 - perplexity: 179.1632"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3199592375.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Train the model (only 3 epochs for this demo, book uses 50)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting model training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy', PerplexityMetric()]\n",
        ")\n",
        "\n",
        "# Define callbacks\n",
        "os.makedirs('eval', exist_ok=True)\n",
        "csv_logger = CSVLogger(os.path.join('eval', '1_language_modelling.log'))\n",
        "es_callback = EarlyStopping(monitor='val_perplexity', patience=5, mode='min')\n",
        "lr_callback = ReduceLROnPlateau(monitor='val_perplexity', factor=0.1, patience=2, mode='min')\n",
        "\n",
        "# Train the model (only 3 epochs for this demo, book uses 50)\n",
        "print(\"Starting model training...\")\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=1,\n",
        "    validation_data=valid_ds,\n",
        "    callbacks=[es_callback, lr_callback, csv_logger]\n",
        ")\n",
        "\n",
        "# Evaluate on the test set\n",
        "print(\"\\nEvaluating model on test set...\")\n",
        "model.evaluate(test_ds)\n",
        "\n",
        "# Save the model and tokenizer\n",
        "os.makedirs('models', exist_ok=True)\n",
        "model.save(os.path.join('models', '2_gram_lm.h5'))\n",
        "\n",
        "with open(os.path.join('models', 'text_hyperparams.pkl'), 'wb') as f:\n",
        "    pickle.dump({'n_vocab': n_vocab, 'ngrams': ngrams, 'n_seq': n_seq}, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiy5pH0CNN_e"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ofo_6xk9NN_e"
      },
      "source": [
        "## 10.5 Generating new text from the language model: Greedy decoding\n",
        "\n",
        "For **inference** (text generation), we can't use `model.fit()` or `model.predict()` on a whole sequence. We need to generate one token at a time, feed that token back into the model, and get the next one.\n",
        "\n",
        "This requires a new model that:\n",
        "1.  Takes the previous token(s) **and** the GRU's previous hidden state as input.\n",
        "2.  Outputs the prediction (logits) **and** the new hidden state.\n",
        "\n",
        "**Greedy Decoding** is the simplest method: at each step, we just pick the single token with the highest probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "166VVbSFNN_f",
        "outputId": "bffee019-c2b8-405e-dd02-d913dc1656ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_token         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_infer     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m29,696\u001b[0m │ input_token[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_state         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ gru_infer (\u001b[38;5;33mGRU\u001b[0m)     │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m),    │  \u001b[38;5;34m4,724,736\u001b[0m │ embedding_infer[\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)]     │            │ input_state[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_infer (\u001b[38;5;33mDense\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │    \u001b[38;5;34m524,800\u001b[0m │ gru_infer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ final_out_infer     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m57\u001b[0m)        │     \u001b[38;5;34m29,241\u001b[0m │ dense_infer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_infer    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m57\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ final_out_infer[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_token         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_infer     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">29,696</span> │ input_token[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_state         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ gru_infer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)     │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>),    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,724,736</span> │ embedding_infer[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)]     │            │ input_state[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_infer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │ gru_infer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ final_out_infer     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">57</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">29,241</span> │ dense_infer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_infer    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">57</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ final_out_infer[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,308,473\u001b[0m (20.25 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,308,473</span> (20.25 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,308,473\u001b[0m (20.25 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,308,473</span> (20.25 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# 1. Re-build the model for inference using the Functional API\n",
        "K.clear_session()\n",
        "\n",
        "from tensorflow.keras.models import load_model # Import load_model\n",
        "\n",
        "# Load the entire trained model\n",
        "trained_model = load_model(os.path.join('models', '2_gram_lm.h5'),\n",
        "                           custom_objects={'PerplexityMetric': PerplexityMetric})\n",
        "\n",
        "# Define inputs for the inference model\n",
        "inp_token = tf.keras.layers.Input(shape=(1,), dtype=tf.int32, name='input_token') # Changed shape from (None,) to (1,)\n",
        "inp_state = tf.keras.layers.Input(shape=(1024,), name='input_state') # 1024 is the GRU units\n",
        "\n",
        "# Get weights from the trained model's layers\n",
        "embedding_weights = trained_model.get_layer('embedding').get_weights()\n",
        "gru_weights = trained_model.get_layer('gru').get_weights()\n",
        "dense_1_weights = trained_model.get_layer('dense').get_weights()\n",
        "final_out_weights = trained_model.get_layer('final_out').get_weights()\n",
        "\n",
        "# Create new layer instances for the inference model, explicitly configuring GRU\n",
        "# for single step prediction and state output. Weights will be set later.\n",
        "embedding_layer_infer = layers.Embedding(\n",
        "    input_dim=trained_model.get_layer('embedding').input_dim,\n",
        "    output_dim=trained_model.get_layer('embedding').output_dim,\n",
        "    name='embedding_infer'\n",
        ")\n",
        "\n",
        "gru_layer_infer = layers.GRU(\n",
        "    trained_model.get_layer('gru').units,\n",
        "    return_sequences=False, # Process single input token, get single output\n",
        "    return_state=True,      # Return the hidden state\n",
        "    name='gru_infer'\n",
        ")\n",
        "\n",
        "dense_layer_1_infer = layers.Dense(\n",
        "    trained_model.get_layer('dense').units,\n",
        "    activation=trained_model.get_layer('dense').activation,\n",
        "    name='dense_infer'\n",
        ")\n",
        "\n",
        "final_layer_infer = layers.Dense(\n",
        "    trained_model.get_layer('final_out').units,\n",
        "    name='final_out_infer'\n",
        ")\n",
        "\n",
        "softmax_layer_infer = layers.Activation('softmax', name='activation_infer')\n",
        "\n",
        "# Build the functional graph for inference\n",
        "emb_out = embedding_layer_infer(inp_token)\n",
        "gru_output, gru_state_out = gru_layer_infer(emb_out, initial_state=inp_state)\n",
        "dense_out = dense_layer_1_infer(gru_output)\n",
        "final_out = final_layer_infer(dense_out)\n",
        "softmax_out = softmax_layer_infer(final_out)\n",
        "\n",
        "infer_model = tf.keras.models.Model(\n",
        "    inputs=[inp_token, inp_state],\n",
        "    outputs=[softmax_out, gru_state_out]\n",
        ")\n",
        "\n",
        "# Set the weights for the new layers\n",
        "embedding_layer_infer.set_weights(embedding_weights)\n",
        "gru_layer_infer.set_weights(gru_weights)\n",
        "dense_layer_1_infer.set_weights(dense_1_weights)\n",
        "final_layer_infer.set_weights(final_out_weights)\n",
        "\n",
        "infer_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_h6qsIKNN_g",
        "outputId": "a64e3946-8ab0-4472-fdae-54c0d9a962f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed text: 'the dog was'\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2606742312.py:24: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  wid = int(np.argmax(out[0], axis=-1).ravel())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
            "Generated text:\n",
            "the dog wasa fo thehe f w w quick and and a c w were friend thehey  w w\n"
          ]
        }
      ],
      "source": [
        "# 2. Write the Greedy Decoding loop (based on Listing 10.7)\n",
        "\n",
        "def generate_text_greedy(seed_text, n_to_generate=50):\n",
        "    print(f\"Seed text: '{seed_text}'\\n\")\n",
        "    text = get_ngrams(seed_text.lower(), ngrams)\n",
        "    seq = tokenizer.texts_to_sequences([text])\n",
        "\n",
        "    # Initialize the state\n",
        "    state = np.zeros(shape=(1, 1024))\n",
        "\n",
        "    # Feed the seed text to the model to \"warm up\" the state\n",
        "    for i in range(len(seq[0]) - 1):\n",
        "        x_in = np.array([[seq[0][i]]])\n",
        "        out, state = infer_model.predict([x_in, state])\n",
        "\n",
        "    # Start generating from the last token of the seed text\n",
        "    x = np.array([[seq[0][-1]]])\n",
        "    generated_text = list(text)\n",
        "\n",
        "    for _ in range(n_to_generate):\n",
        "        out, state = infer_model.predict([x, state])\n",
        "\n",
        "        # Greedy step: get the ID of the most probable next token\n",
        "        wid = int(np.argmax(out[0], axis=-1).ravel())\n",
        "\n",
        "        # Stop if we predict 'unk' or 0 (padding)\n",
        "        if wid == 0 or wid == tokenizer.word_index['unk']:\n",
        "            break\n",
        "\n",
        "        word = tokenizer.index_word[wid]\n",
        "        generated_text.append(word)\n",
        "\n",
        "        # The new input is the word we just predicted\n",
        "        x = np.array([[wid]])\n",
        "\n",
        "    print(\"Generated text:\")\n",
        "    print(''.join(generated_text))\n",
        "\n",
        "generate_text_greedy(\"the dog was\", n_to_generate=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20Si3RebNN_h"
      },
      "source": [
        "## 10.6 Beam search: Enhancing the predictive power of sequential models\n",
        "\n",
        "Greedy decoding has a major flaw: it's not optimal. It might pick a word that seems good *now* but leads to a dead end later (e.g., \"the dog was **ru**\" -> \"the dog was **running**\" vs. \"the dog was **ru**\" -> \"the dog was **ru**g\").\n",
        "\n",
        "**Beam Search** improves this by keeping track of the *k* (e.g., *k*=3) most probable sequences at each step.\n",
        "\n",
        "1.  **Step 1**: Get the top 3 most likely next words (e.g., \"running\", \"barking\", \"sleeping\").\n",
        "2.  **Step 2**: For *each* of those 3 sequences, predict the *next* top 3 words. This gives $3 \\times 3 = 9$ candidate sequences.\n",
        "3.  **Step 3**: Rank all 9 sequences by their combined probability and keep only the new top 3.\n",
        "4.  Repeat.\n",
        "\n",
        "This is a more complex, recursive function (based on Listing 10.8) that explores more of the search space and usually produces more coherent text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0707a5f"
      },
      "source": [
        "# Task\n",
        "To address the training speed issue, I will modify the model architecture and training parameters as per the plan:\n",
        "\n",
        "First, I'll modify the model architecture to reduce the number of GRU units and the embedding output dimension. Then, I'll update the `batch_size` for data pipelines. Finally, I'll recompile and retrain the model and evaluate its performance.\n",
        "\n",
        "Here's how I'll update the parameters in cell `ioAm9_OINN_b`:\n",
        "1. **Reduce Model Parameters**:\n",
        "    - Change `output_dim` of `layers.Embedding` from 512 to 128.\n",
        "    - Change `layers.GRU` units from 1024 to 256.\n",
        "\n",
        "And in cell `mxKUJ57HNN_Z`:\n",
        "1. **Increase Batch Size**:\n",
        "    - Change `batch_size` from 128 to 256.\n",
        "\n",
        "After these modifications, I'll rerun cell `mxKUJ57HNN_Z` to update the data pipelines, then rerun cell `ioAm9_OINN_b` to apply the model architecture changes. Finally, I'll calculate `steps_per_epoch` and `validation_steps` and retrain the model in cell `FkdLyKVXNN_d`.\n",
        "\n",
        "I will start by executing the cell `mxKUJ57HNN_Z` to increase the batch size and update the data pipelines.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "import os\n",
        "import requests\n",
        "import tarfile\n",
        "from collections import Counter\n",
        "from itertools import chain\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "# --- 1. Download and Read Data (Simulated from book) ---\n",
        "# The book uses the bAbI dataset. We'll simulate downloading and reading it.\n",
        "data_dir = os.path.join('data', 'lm', 'CBTest', 'data')\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "train_path = os.path.join(data_dir, 'cbt_train.txt')\n",
        "valid_path = os.path.join(data_dir, 'cbt_valid.txt')\n",
        "test_path = os.path.join(data_dir, 'cbt_test.txt')\n",
        "\n",
        "# Create dummy data files for demonstration\n",
        "if not os.path.exists(train_path):\n",
        "    with open(train_path, 'w') as f:\n",
        "        f.write(\"_BOOK_TITLE_ A Simple Story\\n\")\n",
        "        f.write(\"Once upon a time, there was a fox.\\n\")\n",
        "        f.write(\"The fox was quick and brown.\\n\")\n",
        "        f.write(\"_BOOK_TITLE_ Another Story\\n\")\n",
        "        f.write(\"A dog and a cat were friends.\\n\")\n",
        "        f.write(\"They played in the yard.\\n\")\n",
        "\n",
        "    with open(valid_path, 'w') as f:\n",
        "        f.write(\"_BOOK_TITLE_ A Validation Story\\n\")\n",
        "        f.write(\"The sun was bright.\\n\")\n",
        "\n",
        "    with open(test_path, 'w') as f:\n",
        "        f.write(\"_BOOK_TITLE_ A Test Story\\n\")\n",
        "        f.write(\"The moon was full.\\n\")\n",
        "\n",
        "def read_data(path):\n",
        "    stories = []\n",
        "    with open(path, 'r') as f:\n",
        "        s = []\n",
        "        for row in f:\n",
        "            if row.startswith(\"_BOOK_TITLE_\"):\n",
        "                if len(s) > 0:\n",
        "                    stories.append(' '.join(s).lower())\n",
        "                s = []\n",
        "            s.append(row.strip()) # Add strip() to remove newlines\n",
        "        if len(s) > 0:\n",
        "            stories.append(' '.join(s).lower())\n",
        "    return stories\n",
        "\n",
        "stories = read_data(train_path)\n",
        "val_stories = read_data(valid_path)\n",
        "test_stories = read_data(test_path)\n",
        "\n",
        "# --- 2. N-gram and Tokenizer Processing ---\n",
        "\n",
        "# Function to get n-grams\n",
        "def get_ngrams(text, n):\n",
        "    return [text[i:i+n] for i in range(0, len(text), n)]\n",
        "\n",
        "ngrams = 2 # We'll use 2-grams (bigrams)\n",
        "train_ngram_stories = [get_ngrams(s, ngrams) for s in stories]\n",
        "\n",
        "# Calculate vocabulary size (e.g., all n-grams appearing >= 10 times)\n",
        "# In our small demo, we'll use a threshold of 1\n",
        "text_corpus = chain(*train_ngram_stories)\n",
        "cnt = Counter(text_corpus)\n",
        "freq_df = pd.Series(list(cnt.values()), index=list(cnt.keys())).sort_values(ascending=False)\n",
        "\n",
        "n_vocab = (freq_df >= 1).sum() # For demo, use 1. Book uses 10.\n",
        "\n",
        "# --- 3. Tokenize Data ---\n",
        "tokenizer = Tokenizer(num_words=n_vocab, oov_token='unk', lower=False)\n",
        "\n",
        "# Fit tokenizer on training n-grams\n",
        "tokenizer.fit_on_texts(train_ngram_stories)\n",
        "\n",
        "# Convert all datasets to sequences of integer IDs\n",
        "train_data_seq = tokenizer.texts_to_sequences(train_ngram_stories)\n",
        "\n",
        "val_ngram_stories = [get_ngrams(s, ngrams) for s in val_stories]\n",
        "val_data_seq = tokenizer.texts_to_sequences(val_ngram_stories)\n",
        "\n",
        "test_ngram_stories = [get_ngrams(s, ngrams) for s in test_stories]\n",
        "test_data_seq = tokenizer.texts_to_sequences(test_ngram_stories)\n",
        "\n",
        "# Based on Listing 10.3\n",
        "def get_tf_pipeline(data_seq, n_seq, batch_size=64, shift=1, shuffle=True):\n",
        "    \"\"\"Converts sequences of text IDs into (input, target) batches.\"\"\"\n",
        "\n",
        "    # Use RaggedTensor to handle stories of different lengths\n",
        "    text_ds = tf.data.Dataset.from_tensor_slices(tf.ragged.constant(data_seq))\n",
        "\n",
        "    if shuffle:\n",
        "        # Ensure buffer_size is at least 1, even for very small datasets\n",
        "        buffer_size_stories = max(1, len(data_seq) // 2)\n",
        "        text_ds = text_ds.shuffle(buffer_size=buffer_size_stories)\n",
        "\n",
        "    # Use flat_map to apply windowing to each story individually\n",
        "    text_ds = text_ds.flat_map(\n",
        "        lambda x: tf.data.Dataset.from_tensor_slices(x).window(\n",
        "            n_seq + 1, shift=shift, drop_remainder=True\n",
        "        ).flat_map(\n",
        "            lambda window: window.batch(n_seq + 1, drop_remainder=True)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if shuffle:\n",
        "        # Ensure buffer_size is at least 1\n",
        "        buffer_size_batches = max(1, 10 * batch_size)\n",
        "        text_ds = text_ds.shuffle(buffer_size=buffer_size_batches)\n",
        "\n",
        "    text_ds = text_ds.batch(batch_size)\n",
        "\n",
        "    # Split into (x, y) pairs where y is x shifted by one\n",
        "    text_ds = text_ds.map(lambda x: (x[:, :-1], x[:, 1:]))\n",
        "\n",
        "    # Add .repeat() for training datasets to ensure multiple epochs\n",
        "    if shuffle: # Only repeat for training dataset, not validation/test\n",
        "        text_ds = text_ds.repeat()\n",
        "\n",
        "    text_ds = text_ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "    return text_ds\n",
        "\n",
        "# Set hyperparameters\n",
        "n_seq = 10 # Sequence length for the model (changed from 100 to 10)\n",
        "batch_size = 256 # Increased batch size from 128 to 256\n",
        "\n",
        "train_ds = get_tf_pipeline(train_data_seq, n_seq, batch_size=batch_size, shuffle=True)\n",
        "valid_ds = get_tf_pipeline(val_data_seq, n_seq, batch_size=batch_size)\n",
        "test_ds = get_tf_pipeline(test_data_seq, n_seq, batch_size=batch_size)\n",
        "\n",
        "# Inspect a batch\n",
        "for x_batch, y_batch in train_ds.take(1):\n",
        "    print(f\"X batch shape: {x_batch.shape}\")\n",
        "    print(f\"Y batch shape: {y_batch.shape}\")\n",
        "    print(f\"\\nExample X: {x_batch[0, :10]}\")\n",
        "    print(f\"Example Y: {y_batch[0, :10]}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3873819b",
        "outputId": "1519f0e4-f60e-4938-f56e-91f2f91f2ddf"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "import os\n",
        "import requests\n",
        "import tarfile\n",
        "from collections import Counter\n",
        "from itertools import chain\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "# --- 1. Download and Read Data (Simulated from book) ---\n",
        "# The book uses the bAbI dataset. We'll simulate downloading and reading it.\n",
        "data_dir = os.path.join('data', 'lm', 'CBTest', 'data')\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "train_path = os.path.join(data_dir, 'cbt_train.txt')\n",
        "valid_path = os.path.join(data_dir, 'cbt_valid.txt')\n",
        "test_path = os.path.join(data_dir, 'cbt_test.txt')\n",
        "\n",
        "# Create dummy data files for demonstration\n",
        "if not os.path.exists(train_path):\n",
        "    with open(train_path, 'w') as f:\n",
        "        f.write(\"_BOOK_TITLE_ A Simple Story\\n\")\n",
        "        f.write(\"Once upon a time, there was a fox.\\n\")\n",
        "        f.write(\"The fox was quick and brown.\\n\")\n",
        "        f.write(\"_BOOK_TITLE_ Another Story\\n\")\n",
        "        f.write(\"A dog and a cat were friends.\\n\")\n",
        "        f.write(\"They played in the yard.\\n\")\n",
        "\n",
        "    with open(valid_path, 'w') as f:\n",
        "        f.write(\"_BOOK_TITLE_ A Validation Story\\n\")\n",
        "        f.write(\"The sun was bright.\\n\")\n",
        "\n",
        "    with open(test_path, 'w') as f:\n",
        "        f.write(\"_BOOK_TITLE_ A Test Story\\n\")\n",
        "        f.write(\"The moon was full.\\n\")\n",
        "\n",
        "def read_data(path):\n",
        "    stories = []\n",
        "    with open(path, 'r') as f:\n",
        "        s = []\n",
        "        for row in f:\n",
        "            if row.startswith(\"_BOOK_TITLE_\"):\n",
        "                if len(s) > 0:\n",
        "                    stories.append(' '.join(s).lower())\n",
        "                s = []\n",
        "            s.append(row.strip()) # Add strip() to remove newlines\n",
        "        if len(s) > 0:\n",
        "            stories.append(' '.join(s).lower())\n",
        "    return stories\n",
        "\n",
        "stories = read_data(train_path)\n",
        "val_stories = read_data(valid_path)\n",
        "test_stories = read_data(test_path)\n",
        "\n",
        "# --- 2. N-gram and Tokenizer Processing ---\n",
        "\n",
        "# Function to get n-grams\n",
        "def get_ngrams(text, n):\n",
        "    return [text[i:i+n] for i in range(0, len(text), n)]\n",
        "\n",
        "ngrams = 2 # We'll use 2-grams (bigrams)\n",
        "train_ngram_stories = [get_ngrams(s, ngrams) for s in stories]\n",
        "\n",
        "# Calculate vocabulary size (e.g., all n-grams appearing >= 10 times)\n",
        "# In our small demo, we'll use a threshold of 1\n",
        "text_corpus = chain(*train_ngram_stories)\n",
        "cnt = Counter(text_corpus)\n",
        "freq_df = pd.Series(list(cnt.values()), index=list(cnt.keys())).sort_values(ascending=False)\n",
        "\n",
        "n_vocab = (freq_df >= 1).sum() # For demo, use 1. Book uses 10.\n",
        "\n",
        "# --- 3. Tokenize Data ---\n",
        "tokenizer = Tokenizer(num_words=n_vocab, oov_token='unk', lower=False)\n",
        "\n",
        "# Fit tokenizer on training n-grams\n",
        "tokenizer.fit_on_texts(train_ngram_stories)\n",
        "\n",
        "# Convert all datasets to sequences of integer IDs\n",
        "train_data_seq = tokenizer.texts_to_sequences(train_ngram_stories)\n",
        "\n",
        "val_ngram_stories = [get_ngrams(s, ngrams) for s in val_stories]\n",
        "val_data_seq = tokenizer.texts_to_sequences(val_ngram_stories)\n",
        "\n",
        "test_ngram_stories = [get_ngrams(s, ngrams) for s in test_stories]\n",
        "test_data_seq = tokenizer.texts_to_sequences(test_ngram_stories)\n",
        "\n",
        "# Based on Listing 10.3\n",
        "def get_tf_pipeline(data_seq, n_seq, batch_size=64, shift=1, shuffle=True):\n",
        "    \"\"\"Converts sequences of text IDs into (input, target) batches.\"\"\"\n",
        "\n",
        "    # Use RaggedTensor to handle stories of different lengths\n",
        "    text_ds = tf.data.Dataset.from_tensor_slices(tf.ragged.constant(data_seq))\n",
        "\n",
        "    if shuffle:\n",
        "        # Ensure buffer_size is at least 1, even for very small datasets\n",
        "        buffer_size_stories = max(1, len(data_seq) // 2)\n",
        "        text_ds = text_ds.shuffle(buffer_size=buffer_size_stories)\n",
        "\n",
        "    # Use flat_map to apply windowing to each story individually\n",
        "    text_ds = text_ds.flat_map(\n",
        "        lambda x: tf.data.Dataset.from_tensor_slices(x).window(\n",
        "            n_seq + 1, shift=shift, drop_remainder=True\n",
        "        ).flat_map(\n",
        "            lambda window: window.batch(n_seq + 1, drop_remainder=True)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if shuffle:\n",
        "        # Ensure buffer_size is at least 1\n",
        "        buffer_size_batches = max(1, 10 * batch_size)\n",
        "        text_ds = text_ds.shuffle(buffer_size=buffer_size_batches)\n",
        "\n",
        "    text_ds = text_ds.batch(batch_size)\n",
        "\n",
        "    # Split into (x, y) pairs where y is x shifted by one\n",
        "    text_ds = text_ds.map(lambda x: (x[:, :-1], x[:, 1:]))\n",
        "\n",
        "    # Add .repeat() for training datasets to ensure multiple epochs\n",
        "    if shuffle: # Only repeat for training dataset, not validation/test\n",
        "        text_ds = text_ds.repeat()\n",
        "\n",
        "    text_ds = text_ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "    return text_ds\n",
        "\n",
        "# Set hyperparameters\n",
        "n_seq = 10 # Sequence length for the model (changed from 100 to 10)\n",
        "batch_size = 256\n",
        "\n",
        "train_ds = get_tf_pipeline(train_data_seq, n_seq, batch_size=batch_size, shuffle=True)\n",
        "valid_ds = get_tf_pipeline(val_data_seq, n_seq, batch_size=batch_size)\n",
        "test_ds = get_tf_pipeline(test_data_seq, n_seq, batch_size=batch_size)\n",
        "\n",
        "# Inspect a batch\n",
        "for x_batch, y_batch in train_ds.take(1):\n",
        "    print(f\"X batch shape: {x_batch.shape}\")\n",
        "    print(f\"Y batch shape: {y_batch.shape}\")\n",
        "    print(f\"\\nExample X: {x_batch[0, :10]}\")\n",
        "    print(f\"Example Y: {y_batch[0, :10]}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X batch shape: (67, 10)\n",
            "Y batch shape: (67, 10)\n",
            "\n",
            "Example X: [45 46  2 47 48  2  7  2 49 50]\n",
            "Example Y: [46  2 47 48  2  7  2 49 50 20]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "4b4623fd",
        "outputId": "6cb04b03-eb12-4011-947b-ba3c5f9c9c8e"
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Embedding(\n",
        "        input_dim=n_vocab + 1, # +1 for the padding token (ID 0)\n",
        "        output_dim=128, # Changed from 512 to 128\n",
        "        input_shape=(None,)\n",
        "    ),\n",
        "\n",
        "    layers.GRU(256, return_state=False, return_sequences=True), # Changed from 1024 to 256\n",
        "\n",
        "    layers.Dense(512, activation='relu'),\n",
        "\n",
        "    layers.Dense(n_vocab, name='final_out'),\n",
        "    layers.Activation('softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:100: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │         \u001b[38;5;34m7,424\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m296,448\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │       \u001b[38;5;34m131,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ final_out (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m57\u001b[0m)       │        \u001b[38;5;34m29,241\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m57\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,424</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">296,448</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ final_out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">57</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">29,241</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">57</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m464,697\u001b[0m (1.77 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">464,697</span> (1.77 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m464,697\u001b[0m (1.77 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">464,697</span> (1.77 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "229825c9",
        "outputId": "5cc1565e-a07d-4f6c-baf2-0842df322559"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "import os\n",
        "import requests\n",
        "import tarfile\n",
        "from collections import Counter\n",
        "from itertools import chain\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "# --- 1. Download and Read Data (Simulated from book) ---\n",
        "# The book uses the bAbI dataset. We'll simulate downloading and reading it.\n",
        "data_dir = os.path.join('data', 'lm', 'CBTest', 'data')\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "train_path = os.path.join(data_dir, 'cbt_train.txt')\n",
        "valid_path = os.path.join(data_dir, 'cbt_valid.txt')\n",
        "test_path = os.path.join(data_dir, 'cbt_test.txt')\n",
        "\n",
        "# Create dummy data files for demonstration\n",
        "if not os.path.exists(train_path):\n",
        "    with open(train_path, 'w') as f:\n",
        "        f.write(\"_BOOK_TITLE_ A Simple Story\\n\")\n",
        "        f.write(\"Once upon a time, there was a fox.\\n\")\n",
        "        f.write(\"The fox was quick and brown.\\n\")\n",
        "        f.write(\"_BOOK_TITLE_ Another Story\\n\")\n",
        "        f.write(\"A dog and a cat were friends.\\n\")\n",
        "        f.write(\"They played in the yard.\\n\")\n",
        "\n",
        "    with open(valid_path, 'w') as f:\n",
        "        f.write(\"_BOOK_TITLE_ A Validation Story\\n\")\n",
        "        f.write(\"The sun was bright.\\n\")\n",
        "\n",
        "    with open(test_path, 'w') as f:\n",
        "        f.write(\"_BOOK_TITLE_ A Test Story\\n\")\n",
        "        f.write(\"The moon was full.\\n\")\n",
        "\n",
        "def read_data(path):\n",
        "    stories = []\n",
        "    with open(path, 'r') as f:\n",
        "        s = []\n",
        "        for row in f:\n",
        "            if row.startswith(\"_BOOK_TITLE_\"):\n",
        "                if len(s) > 0:\n",
        "                    stories.append(' '.join(s).lower())\n",
        "                s = []\n",
        "            s.append(row.strip()) # Add strip() to remove newlines\n",
        "        if len(s) > 0:\n",
        "            stories.append(' '.join(s).lower())\n",
        "    return stories\n",
        "\n",
        "stories = read_data(train_path)\n",
        "val_stories = read_data(valid_path)\n",
        "test_stories = read_data(test_path)\n",
        "\n",
        "# --- 2. N-gram and Tokenizer Processing ---\n",
        "\n",
        "# Function to get n-grams\n",
        "def get_ngrams(text, n):\n",
        "    return [text[i:i+n] for i in range(0, len(text), n)]\n",
        "\n",
        "ngrams = 2 # We'll use 2-grams (bigrams)\n",
        "train_ngram_stories = [get_ngrams(s, ngrams) for s in stories]\n",
        "\n",
        "# Calculate vocabulary size (e.g., all n-grams appearing >= 10 times)\n",
        "# In our small demo, we'll use a threshold of 1\n",
        "text_corpus = chain(*train_ngram_stories)\n",
        "cnt = Counter(text_corpus)\n",
        "freq_df = pd.Series(list(cnt.values()), index=list(cnt.keys())).sort_values(ascending=False)\n",
        "\n",
        "n_vocab = (freq_df >= 1).sum() # For demo, use 1. Book uses 10.\n",
        "\n",
        "# --- 3. Tokenize Data ---\n",
        "tokenizer = Tokenizer(num_words=n_vocab, oov_token='unk', lower=False)\n",
        "\n",
        "# Fit tokenizer on training n-grams\n",
        "tokenizer.fit_on_texts(train_ngram_stories)\n",
        "\n",
        "# Convert all datasets to sequences of integer IDs\n",
        "train_data_seq = tokenizer.texts_to_sequences(train_ngram_stories)\n",
        "\n",
        "val_ngram_stories = [get_ngrams(s, ngrams) for s in val_stories]\n",
        "val_data_seq = tokenizer.texts_to_sequences(val_ngram_stories)\n",
        "\n",
        "test_ngram_stories = [get_ngrams(s, ngrams) for s in test_stories]\n",
        "test_data_seq = tokenizer.texts_to_sequences(test_ngram_stories)\n",
        "\n",
        "# Based on Listing 10.3\n",
        "def get_tf_pipeline(data_seq, n_seq, batch_size=64, shift=1, shuffle=True):\n",
        "    \"\"\"Converts sequences of text IDs into (input, target) batches.\"\"\"\n",
        "\n",
        "    # Use RaggedTensor to handle stories of different lengths\n",
        "    text_ds = tf.data.Dataset.from_tensor_slices(tf.ragged.constant(data_seq))\n",
        "\n",
        "    if shuffle:\n",
        "        # Ensure buffer_size is at least 1, even for very small datasets\n",
        "        buffer_size_stories = max(1, len(data_seq) // 2)\n",
        "        text_ds = text_ds.shuffle(buffer_size=buffer_size_stories)\n",
        "\n",
        "    # Use flat_map to apply windowing to each story individually\n",
        "    text_ds = text_ds.flat_map(\n",
        "        lambda x: tf.data.Dataset.from_tensor_slices(x).window(\n",
        "            n_seq + 1, shift=shift, drop_remainder=True\n",
        "        ).flat_map(\n",
        "            lambda window: window.batch(n_seq + 1, drop_remainder=True)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if shuffle:\n",
        "        # Ensure buffer_size is at least 1\n",
        "        buffer_size_batches = max(1, 10 * batch_size)\n",
        "        text_ds = text_ds.shuffle(buffer_size=buffer_size_batches)\n",
        "\n",
        "    text_ds = text_ds.batch(batch_size)\n",
        "\n",
        "    # Split into (x, y) pairs where y is x shifted by one\n",
        "    text_ds = text_ds.map(lambda x: (x[:, :-1], x[:, 1:]))\n",
        "\n",
        "    # Removed: `text_ds = text_ds.repeat()` for ALL datasets.\n",
        "    # All datasets will now be finite, and `model.fit(epochs=N)` will handle repetitions if needed.\n",
        "\n",
        "    text_ds = text_ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "    return text_ds\n",
        "\n",
        "# Set hyperparameters\n",
        "n_seq = 10 # Sequence length for the model (changed from 100 to 10)\n",
        "batch_size = 256 # Increased batch size from 128 to 256\n",
        "\n",
        "train_ds = get_tf_pipeline(train_data_seq, n_seq, batch_size=batch_size, shuffle=True)\n",
        "valid_ds = get_tf_pipeline(val_data_seq, n_seq, batch_size=batch_size, shuffle=False)\n",
        "test_ds = get_tf_pipeline(test_data_seq, n_seq, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Inspect a batch\n",
        "for x_batch, y_batch in train_ds.take(1):\n",
        "    print(f\"X batch shape: {x_batch.shape}\")\n",
        "    print(f\"Y batch shape: {y_batch.shape}\")\n",
        "    print(f\"\\nExample X: {x_batch[0, :10]}\")\n",
        "    print(f\"Example Y: {y_batch[0, :10]}\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X batch shape: (67, 10)\n",
            "Y batch shape: (67, 10)\n",
            "\n",
            "Example X: [18 19 15 14 54 55 56 16  5  3]\n",
            "Example Y: [19 15 14 54 55 56 16  5  3  1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e690c0d",
        "outputId": "e5effb83-0e41-435a-8b77-5e8a82d06bc3"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy', PerplexityMetric()]\n",
        ")\n",
        "\n",
        "# Define callbacks\n",
        "os.makedirs('eval', exist_ok=True)\n",
        "csv_logger = CSVLogger(os.path.join('eval', '1_language_modelling.log'))\n",
        "es_callback = EarlyStopping(monitor='val_perplexity', patience=5, mode='min')\n",
        "lr_callback = ReduceLROnPlateau(monitor='val_perplexity', factor=0.1, patience=2, mode='min')\n",
        "\n",
        "# Calculate num_train_windows and num_val_windows for context (steps_per_epoch will be handled automatically by Keras)\n",
        "num_train_windows = sum(max(0, len(s) - n_seq) for s in train_data_seq)\n",
        "num_val_windows = sum(max(0, len(s) - n_seq) for s in val_data_seq)\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting model training...\")\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=10,\n",
        "    validation_data=valid_ds,\n",
        "    callbacks=[es_callback, lr_callback, csv_logger]\n",
        ")\n",
        "\n",
        "# Evaluate on the test set\n",
        "print(\"\\nEvaluating model on test set...\")\n",
        "model.evaluate(test_ds)\n",
        "\n",
        "# Save the model and tokenizer\n",
        "os.makedirs('models', exist_ok=True)\n",
        "model.save(os.path.join('models', '2_gram_lm.h5'))\n",
        "\n",
        "with open(os.path.join('models', 'text_hyperparams.pkl'), 'wb') as f:\n",
        "    pickle.dump({'n_vocab': n_vocab, 'ngrams': ngrams, 'n_seq': n_seq}, f)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting model training...\n",
            "Epoch 1/10\n",
            "      1/Unknown \u001b[1m6s\u001b[0m 6s/step - accuracy: 0.9537 - loss: 0.2214 - perplexity: 1.2597"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - accuracy: 0.9537 - loss: 0.2214 - perplexity: 1.2597 - val_accuracy: 0.2000 - val_loss: 6.8920 - val_perplexity: 1576.9830 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step - accuracy: 0.9045 - loss: 0.3140 - perplexity: 1.3913 - val_accuracy: 0.1875 - val_loss: 7.1631 - val_perplexity: 3245.8750 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.9552 - loss: 0.1992 - perplexity: 1.2320 - val_accuracy: 0.1875 - val_loss: 7.4845 - val_perplexity: 6902.5352 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy: 0.9522 - loss: 0.1904 - perplexity: 1.2188 - val_accuracy: 0.1875 - val_loss: 7.5192 - val_perplexity: 7358.2900 - learning_rate: 1.0000e-04\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.9522 - loss: 0.1907 - perplexity: 1.2192 - val_accuracy: 0.1875 - val_loss: 7.5509 - val_perplexity: 7761.7236 - learning_rate: 1.0000e-04\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step - accuracy: 0.9522 - loss: 0.1903 - perplexity: 1.2187 - val_accuracy: 0.1875 - val_loss: 7.5537 - val_perplexity: 7795.9199 - learning_rate: 1.0000e-05\n",
            "\n",
            "Evaluating model on test set...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471ms/step - accuracy: 0.3167 - loss: 7.8618 - perplexity: 8104.5688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}