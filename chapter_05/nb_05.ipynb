{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: State-of-the-art in deep learning: Transformers\n",
    "\n",
    "This notebook reproduces the code and summarizes the theoretical concepts from Chapter 5 of *'TensorFlow in Action'* by Thushan Ganegedara.\n",
    "\n",
    "This chapter introduces the **Transformer model**, the architecture that has become the foundation for modern state-of-the-art Natural Language Processing (NLP). We will cover:\n",
    "1.  How text is represented numerically for a model.\n",
    "2.  The core components of the Transformer: the **encoder-decoder** architecture.\n",
    "3.  The **self-attention** mechanism (the \"Query, Key, and Value\" concept).\n",
    "4.  **Masked self-attention** for the decoder.\n",
    "5.  **Multi-head attention**.\n",
    "6.  Building a complete, simplified Transformer model from scratch using Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Representing Text as Numbers\n",
    "\n",
    "Machine learning models cannot understand raw text. We must first convert text into numbers. This is a multi-step process:\n",
    "\n",
    "1.  **Tokenization**: Split a sentence into individual pieces, or \"tokens.\" This can be done at the word level (e.g., `\"I went\"` -> `[\"I\", \"went\"]`).\n",
    "2.  **Build a Vocabulary**: Assign a unique integer ID to each unique token (e.g., `{\"<PAD>\": 0, \"I\": 1, \"went\": 2, ...}`). We reserve ID 0 for a special `<PAD>` (padding) token.\n",
    "3.  **Integer Encoding**: Convert each token in the sentence to its corresponding ID.\n",
    "4.  **Padding & Truncating**: Ensure all sequences in a batch have the same length. \n",
    "    * Sentences shorter than the fixed length are **padded** with the `<PAD>` (0) token.\n",
    "    * Sentences longer than the fixed length are **truncated**.\n",
    "    * Example (length 5): `\"It was cold\"` -> `[6, 7, 8]` -> `[6, 7, 8, 0, 0]`\n",
    "5.  **Vector Representation (e.g., One-Hot Encoding)**: Convert each integer ID into a vector. In one-hot encoding, the ID becomes the index for a '1' in a vector of zeros. This is necessary to prevent the model from assuming a false numerical relationship between words (e.g., that word 3 is \"more\" than word 2).\n",
    "\n",
    "The final result is a 3D tensor with the shape `(batch_size, sequence_length, vocabulary_size)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Understanding the Transformer Model\n",
    "\n",
    "The Transformer is based on an **encoder-decoder architecture**, which is common for sequence-to-sequence tasks like machine translation.\n",
    "\n",
    "* **Encoder**: Takes the input sequence (e.g., an English sentence) and maps it to a rich, latent representation (a set of vectors).\n",
    "* **Decoder**: Takes the encoder's output and generates the target sequence (e.g., the French translation) one token at a time.\n",
    "\n",
    "Unlike RNNs/LSTMs which process text one word at a time, the Transformer processes all tokens at once using a mechanism called **self-attention**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1-5.2.2 Diving Deeper: Encoder and Decoder Layers\n",
    "\n",
    "Both the encoder and decoder are stacks of identical layers.\n",
    "\n",
    "A single **Encoder Layer** has two sub-layers:\n",
    "1.  A **self-attention layer**: Allows every token in the input sequence to look at and weigh the importance of all other tokens in the same sequence. For example, in \"I kicked the ball and **it** disappeared,\" self-attention helps the model learn that \"it\" refers to \"ball.\"\n",
    "2.  A **fully connected layer**: A simple feed-forward network applied to each token's representation.\n",
    "\n",
    "A single **Decoder Layer** has three sub-layers:\n",
    "1.  A **masked self-attention layer**: This is the same as self-attention, but it's \"masked\" to prevent a token from \"seeing\" future tokens. When predicting the 3rd word, the model can only attend to words 1 and 2.\n",
    "2.  An **encoder-decoder attention layer**: This is the key link. The decoder's output from the layer above is used to \"query\" the encoder's output, deciding which parts of the *input* sentence are most relevant to generating the *current* output token.\n",
    "3.  A **fully connected layer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 The Self-Attention Layer\n",
    "\n",
    "Self-attention is the core idea of the Transformer. It computes a token's representation by taking a weighted sum of all other tokens in the sequence. The weights are calculated using three vectors for each token:\n",
    "\n",
    "* **Query (Q)**: A representation of the current token, used to \"ask\" other tokens for their relevance.\n",
    "* **Key (K)**: A representation of another token, used to be \"asked\" by the query. The Query-Key interaction determines the attention score (the weight).\n",
    "* **Value (V)**: The actual content or representation of the token. Once scores are calculated, we sum up the **Values** weighted by their scores.\n",
    "\n",
    "The formula is: **$h = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$**\n",
    "\n",
    "We can implement this as a custom Keras layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Implementation of the Self-Attention Layer (based on Listing 5.1)\n",
    "class SelfAttentionLayer(layers.Layer):\n",
    "    def __init__(self, d):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "        self.d = d # Output dimensionality\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # Define the weight matrices for Q, K, V\n",
    "        self.Wq = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32', name='Wq'\n",
    "        )\n",
    "        self.Wk = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32', name='Wk'\n",
    "        )\n",
    "        self.Wv = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32', name='Wv'\n",
    "        )\n",
    "    \n",
    "    def call(self, q_x, k_x, v_x):\n",
    "        # Calculate Q, K, V matrices\n",
    "        q = tf.matmul(q_x, self.Wq)\n",
    "        k = tf.matmul(k_x, self.Wk)\n",
    "        v = tf.matmul(v_x, self.Wv)\n",
    "        \n",
    "        # Calculate attention scores (P)\n",
    "        # (Q * K_transpose) / sqrt(d)\n",
    "        p = tf.matmul(q, k, transpose_b=True) / math.sqrt(float(self.d))\n",
    "        p = tf.nn.softmax(p)\n",
    "        \n",
    "        # Calculate the final attended output (h = P * V)\n",
    "        h = tf.matmul(p, v)\n",
    "        return h, p\n",
    "\n",
    "# Test the layer\n",
    "K_ = tf.keras.backend\n",
    "K_.clear_session()\n",
    "\n",
    "d_model = 512\n",
    "n_seq = 7\n",
    "x = tf.constant(np.random.normal(size=(1, n_seq, d_model)), dtype=tf.float32)\n",
    "\n",
    "layer = SelfAttentionLayer(d_model)\n",
    "h, p = layer(x, x, x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output (h) shape: {h.shape}\")\n",
    "print(f\"Attention (p) shape: {p.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.6 Masked Self-Attention Layers\n",
    "\n",
    "The decoder uses **masked** self-attention to prevent it from \"cheating\" by looking at future tokens. When predicting the word at position `t`, it should only have access to tokens from `0` to `t`.\n",
    "\n",
    "This is achieved by adding a \"mask\" (a very large negative number, e.g., `-1e9`) to the attention scores right before the softmax. This turns all future positions' scores into zeros after the softmax, effectively ignoring them.\n",
    "\n",
    "Here is the updated `SelfAttentionLayer` with masking capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated SelfAttentionLayer with masking (based on Listing 5.2)\n",
    "class SelfAttentionLayer(layers.Layer):\n",
    "    def __init__(self, d, **kwargs):\n",
    "        super(SelfAttentionLayer, self).__init__(**kwargs)\n",
    "        self.d = d\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.Wq = self.add_weight(shape=(input_shape[-1], self.d), initializer='glorot_uniform', trainable=True, name='Wq')\n",
    "        self.Wk = self.add_weight(shape=(input_shape[-1], self.d), initializer='glorot_uniform', trainable=True, name='Wk')\n",
    "        self.Wv = self.add_weight(shape=(input_shape[-1], self.d), initializer='glorot_uniform', trainable=True, name='Wv')\n",
    "    \n",
    "    def call(self, q_x, k_x, v_x, mask=None):\n",
    "        q = tf.matmul(q_x, self.Wq)\n",
    "        k = tf.matmul(k_x, self.Wk)\n",
    "        v = tf.matmul(v_x, self.Wv)\n",
    "        \n",
    "        p = tf.matmul(q, k, transpose_b=True) / math.sqrt(float(self.d))\n",
    "        \n",
    "        # Apply the mask (if one is provided)\n",
    "        if mask is not None:\n",
    "            p += (mask * -1e9) # Add a large negative number where mask is 1\n",
    "            \n",
    "        p = tf.nn.softmax(p)\n",
    "        h = tf.matmul(p, v)\n",
    "        return h, p\n",
    "\n",
    "# Create the triangular mask for a sequence of length 7\n",
    "# This is a look-ahead mask for the decoder\n",
    "mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "print(\"Decoder Look-Ahead Mask:\")\n",
    "print(mask.numpy())\n",
    "\n",
    "# Test the masked layer\n",
    "K_.clear_session()\n",
    "masked_layer = SelfAttentionLayer(d_model)\n",
    "h_masked, p_masked = masked_layer(x, x, x, mask=mask)\n",
    "\n",
    "print(\"\\nAttention scores (p) with mask (should be lower-triangular):\")\n",
    "print(p_masked.numpy()[0].round(2)) # Rounding for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.7 Multi-Head Attention\n",
    "\n",
    "Instead of one large self-attention calculation, **Multi-Head Attention** runs several smaller self-attention mechanisms in parallel (e.g., 8 \"heads\"). Each head can theoretically learn a different type of relationship between tokens.\n",
    "\n",
    "The outputs of all heads are concatenated and passed through a final linear layer to produce the sub-layer's final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code snippet for multi-head attention\n",
    "n_heads = 8\n",
    "d_head = d_model // n_heads # 512 / 8 = 64\n",
    "\n",
    "multi_attn_heads = [SelfAttentionLayer(d_head) for i in range(n_heads)]\n",
    "\n",
    "# In a full implementation, you'd pass x to each head\n",
    "outputs = [head(x, x, x)[0] for head in multi_attn_heads]\n",
    "\n",
    "# Concatenate all head outputs\n",
    "outputs_concat = tf.concat(outputs, axis=-1)\n",
    "\n",
    "print(f\"Shape of one head: {outputs[0].shape}\")\n",
    "print(f\"Shape after concatenation: {outputs_concat.shape}\")\n",
    "\n",
    "# This concatenated output would then be passed through a final Dense layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.8 Fully Connected Layer\n",
    "\n",
    "The second sub-layer in both the encoder and decoder is a simple, position-wise fully connected network. It consists of two `Dense` layers with a ReLU activation in between. This is applied independently to each token's representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation using Keras Dense layers (based on Listing 5.4)\n",
    "class FCLayer(layers.Layer):\n",
    "    def __init__(self, d1, d2, **kwargs):\n",
    "        super(FCLayer, self).__init__(**kwargs)\n",
    "        self.dense_layer_1 = layers.Dense(d1, activation='relu')\n",
    "        self.dense_layer_2 = layers.Dense(d2)\n",
    "    \n",
    "    def call(self, x):\n",
    "        ff1 = self.dense_layer_1(x)\n",
    "        ff2 = self.dense_layer_2(ff1)\n",
    "        return ff2\n",
    "\n",
    "# Test the FC layer\n",
    "fc = FCLayer(d1=2048, d2=d_model)\n",
    "fc_out = fc(h_masked) # Using output from previous layer\n",
    "print(f\"Input shape to FC: {h_masked.shape}\")\n",
    "print(f\"Output shape from FC: {fc_out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.9 Putting Everything Together: A Full Transformer Model\n",
    "\n",
    "Now we combine these components (`SelfAttentionLayer` and `FCLayer`) into an `EncoderLayer` and `DecoderLayer`. (Note: For simplicity, this code omits the residual connections and layer normalization mentioned in Chapter 13)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Listing 5.5 - Encoder Layer\n",
    "# (Note: This is a simplified version without multi-head or residuals for clarity)\n",
    "class EncoderLayer(layers.Layer):\n",
    "    def __init__(self, d, **kwargs):\n",
    "        super(EncoderLayer, self).__init__(**kwargs)\n",
    "        self.attn_head = SelfAttentionLayer(d)\n",
    "        self.fc_layer = FCLayer(2048, d)\n",
    "    \n",
    "    def call(self, x):\n",
    "        h, _ = self.attn_head(x, x, x)\n",
    "        y = self.fc_layer(h)\n",
    "        return y\n",
    "\n",
    "# Based on Listing 5.6 - Decoder Layer\n",
    "class DecoderLayer(layers.Layer):\n",
    "    def __init__(self, d, **kwargs):\n",
    "        super(DecoderLayer, self).__init__(**kwargs)\n",
    "        self.masked_attn_head = SelfAttentionLayer(d, name=\"masked_attn\")\n",
    "        self.attn_head = SelfAttentionLayer(d, name=\"enc_dec_attn\")\n",
    "        self.fc_layer = FCLayer(2048, d)\n",
    "    \n",
    "    def call(self, de_x, en_x, mask=None):\n",
    "        # 1. Masked self-attention (on decoder input)\n",
    "        h1, _ = self.masked_attn_head(de_x, de_x, de_x, mask)\n",
    "        \n",
    "        # 2. Encoder-decoder attention (Query=decoder, Key/Value=encoder)\n",
    "        h2, _ = self.attn_head(q_x=h1, k_x=en_x, v_x=en_x)\n",
    "        \n",
    "        # 3. Fully connected layer\n",
    "        y = self.fc_layer(h2)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Listing 5.7 - Building the final \"MinTransformer\"\n",
    "K_.clear_session()\n",
    "\n",
    "# Hyperparameters\n",
    "n_steps = 25\n",
    "n_en_vocab = 300\n",
    "n_de_vocab = 400\n",
    "d = 512\n",
    "\n",
    "# Define the decoder look-ahead mask\n",
    "mask = 1 - tf.linalg.band_part(tf.ones((n_steps, n_steps)), -1, 0)\n",
    "\n",
    "# --- Define Model Inputs ---\n",
    "en_inp = layers.Input(shape=(n_steps,), name=\"encoder_input_ids\")\n",
    "de_inp = layers.Input(shape=(n_steps,), name=\"decoder_input_ids\")\n",
    "\n",
    "# --- Define Embedding Layers ---\n",
    "en_emb_layer = layers.Embedding(n_en_vocab, d, input_length=n_steps)\n",
    "de_emb_layer = layers.Embedding(n_de_vocab, d, input_length=n_steps)\n",
    "\n",
    "en_emb = en_emb_layer(en_inp)\n",
    "de_emb = de_emb_layer(de_inp)\n",
    "\n",
    "# --- Build Encoder & Decoder ---\n",
    "# (We'll use two layers for each)\n",
    "en_out1 = EncoderLayer(d, name=\"enc_1\")(en_emb)\n",
    "en_out2 = EncoderLayer(d, name=\"enc_2\")(en_out1)\n",
    "\n",
    "de_out1 = DecoderLayer(d, name=\"dec_1\")(de_emb, en_out2, mask)\n",
    "de_out2 = DecoderLayer(d, name=\"dec_2\")(de_out1, en_out2, mask)\n",
    "\n",
    "# --- Final Prediction Layer ---\n",
    "de_pred = layers.Dense(n_de_vocab, activation='softmax', name='final_output')(de_out2)\n",
    "\n",
    "# --- Create and Compile the Model ---\n",
    "transformer = models.Model(\n",
    "    inputs=[en_inp, de_inp],\n",
    "    outputs=de_pred,\n",
    "    name='MinTransformer'\n",
    ")\n",
    "\n",
    "transformer.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['acc']\n",
    ")\n",
    "\n",
    "transformer.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
