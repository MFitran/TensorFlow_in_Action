{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODdML-EKNG7O"
      },
      "source": [
        "# Chapter 5: State-of-the-art in deep learning: Transformers\n",
        "\n",
        "This notebook reproduces the code and summarizes the theoretical concepts from Chapter 5 of *'TensorFlow in Action'* by Thushan Ganegedara.\n",
        "\n",
        "This chapter introduces the **Transformer model**, the architecture that has become the foundation for modern state-of-the-art Natural Language Processing (NLP). We will cover:\n",
        "1.  How text is represented numerically for a model.\n",
        "2.  The core components of the Transformer: the **encoder-decoder** architecture.\n",
        "3.  The **self-attention** mechanism (the \"Query, Key, and Value\" concept).\n",
        "4.  **Masked self-attention** for the decoder.\n",
        "5.  **Multi-head attention**.\n",
        "6.  Building a complete, simplified Transformer model from scratch using Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEF2GH9BNG7X"
      },
      "source": [
        "## 5.1 Representing Text as Numbers\n",
        "\n",
        "Machine learning models cannot understand raw text. We must first convert text into numbers. This is a multi-step process:\n",
        "\n",
        "1.  **Tokenization**: Split a sentence into individual pieces, or \"tokens.\" This can be done at the word level (e.g., `\"I went\"` -> `[\"I\", \"went\"]`).\n",
        "2.  **Build a Vocabulary**: Assign a unique integer ID to each unique token (e.g., `{\"<PAD>\": 0, \"I\": 1, \"went\": 2, ...}`). We reserve ID 0 for a special `<PAD>` (padding) token.\n",
        "3.  **Integer Encoding**: Convert each token in the sentence to its corresponding ID.\n",
        "4.  **Padding & Truncating**: Ensure all sequences in a batch have the same length.\n",
        "    * Sentences shorter than the fixed length are **padded** with the `<PAD>` (0) token.\n",
        "    * Sentences longer than the fixed length are **truncated**.\n",
        "    * Example (length 5): `\"It was cold\"` -> `[6, 7, 8]` -> `[6, 7, 8, 0, 0]`\n",
        "5.  **Vector Representation (e.g., One-Hot Encoding)**: Convert each integer ID into a vector. In one-hot encoding, the ID becomes the index for a '1' in a vector of zeros. This is necessary to prevent the model from assuming a false numerical relationship between words (e.g., that word 3 is \"more\" than word 2).\n",
        "\n",
        "The final result is a 3D tensor with the shape `(batch_size, sequence_length, vocabulary_size)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMuvp-m4NG7a"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiP2jSfRNG7b"
      },
      "source": [
        "## 5.2 Understanding the Transformer Model\n",
        "\n",
        "The Transformer is based on an **encoder-decoder architecture**, which is common for sequence-to-sequence tasks like machine translation.\n",
        "\n",
        "* **Encoder**: Takes the input sequence (e.g., an English sentence) and maps it to a rich, latent representation (a set of vectors).\n",
        "* **Decoder**: Takes the encoder's output and generates the target sequence (e.g., the French translation) one token at a time.\n",
        "\n",
        "Unlike RNNs/LSTMs which process text one word at a time, the Transformer processes all tokens at once using a mechanism called **self-attention**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkABCRsRNG7c"
      },
      "source": [
        "### 5.2.1-5.2.2 Diving Deeper: Encoder and Decoder Layers\n",
        "\n",
        "Both the encoder and decoder are stacks of identical layers.\n",
        "\n",
        "A single **Encoder Layer** has two sub-layers:\n",
        "1.  A **self-attention layer**: Allows every token in the input sequence to look at and weigh the importance of all other tokens in the same sequence. For example, in \"I kicked the ball and **it** disappeared,\" self-attention helps the model learn that \"it\" refers to \"ball.\"\n",
        "2.  A **fully connected layer**: A simple feed-forward network applied to each token's representation.\n",
        "\n",
        "A single **Decoder Layer** has three sub-layers:\n",
        "1.  A **masked self-attention layer**: This is the same as self-attention, but it's \"masked\" to prevent a token from \"seeing\" future tokens. When predicting the 3rd word, the model can only attend to words 1 and 2.\n",
        "2.  An **encoder-decoder attention layer**: This is the key link. The decoder's output from the layer above is used to \"query\" the encoder's output, deciding which parts of the *input* sentence are most relevant to generating the *current* output token.\n",
        "3.  A **fully connected layer**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhw1VbOVNG7e"
      },
      "source": [
        "### 5.2.3 The Self-Attention Layer\n",
        "\n",
        "Self-attention is the core idea of the Transformer. It computes a token's representation by taking a weighted sum of all other tokens in the sequence. The weights are calculated using three vectors for each token:\n",
        "\n",
        "* **Query (Q)**: A representation of the current token, used to \"ask\" other tokens for their relevance.\n",
        "* **Key (K)**: A representation of another token, used to be \"asked\" by the query. The Query-Key interaction determines the attention score (the weight).\n",
        "* **Value (V)**: The actual content or representation of the token. Once scores are calculated, we sum up the **Values** weighted by their scores.\n",
        "\n",
        "The formula is: **$h = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$**\n",
        "\n",
        "We can implement this as a custom Keras layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T34vQfXmNG7f",
        "outputId": "6015d622-cfa4-40a0-8225-2453b693ec98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (1, 7, 512)\n",
            "Output (h) shape: (1, 7, 512)\n",
            "Attention (p) shape: (1, 7, 7)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Implementation of the Self-Attention Layer (based on Listing 5.1)\n",
        "class SelfAttentionLayer(layers.Layer):\n",
        "    def __init__(self, d):\n",
        "        super(SelfAttentionLayer, self).__init__()\n",
        "        self.d = d # Output dimensionality\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Define the weight matrices for Q, K, V\n",
        "        self.Wq = self.add_weight(\n",
        "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
        "            trainable=True, dtype='float32', name='Wq'\n",
        "        )\n",
        "        self.Wk = self.add_weight(\n",
        "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
        "            trainable=True, dtype='float32', name='Wk'\n",
        "        )\n",
        "        self.Wv = self.add_weight(\n",
        "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
        "            trainable=True, dtype='float32', name='Wv'\n",
        "        )\n",
        "\n",
        "    def call(self, q_x, k_x, v_x):\n",
        "        # Calculate Q, K, V matrices\n",
        "        q = tf.matmul(q_x, self.Wq)\n",
        "        k = tf.matmul(k_x, self.Wk)\n",
        "        v = tf.matmul(v_x, self.Wv)\n",
        "\n",
        "        # Calculate attention scores (P)\n",
        "        # (Q * K_transpose) / sqrt(d)\n",
        "        p = tf.matmul(q, k, transpose_b=True) / math.sqrt(float(self.d))\n",
        "        p = tf.nn.softmax(p)\n",
        "\n",
        "        # Calculate the final attended output (h = P * V)\n",
        "        h = tf.matmul(p, v)\n",
        "        return h, p\n",
        "\n",
        "# Test the layer\n",
        "K_ = tf.keras.backend\n",
        "K_.clear_session()\n",
        "\n",
        "d_model = 512\n",
        "n_seq = 7\n",
        "x = tf.constant(np.random.normal(size=(1, n_seq, d_model)), dtype=tf.float32)\n",
        "\n",
        "layer = SelfAttentionLayer(d_model)\n",
        "h, p = layer(x, x, x)\n",
        "\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output (h) shape: {h.shape}\")\n",
        "print(f\"Attention (p) shape: {p.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-hh-jAmNG7j"
      },
      "source": [
        "### 5.2.6 Masked Self-Attention Layers\n",
        "\n",
        "The decoder uses **masked** self-attention to prevent it from \"cheating\" by looking at future tokens. When predicting the word at position `t`, it should only have access to tokens from `0` to `t`.\n",
        "\n",
        "This is achieved by adding a \"mask\" (a very large negative number, e.g., `-1e9`) to the attention scores right before the softmax. This turns all future positions' scores into zeros after the softmax, effectively ignoring them.\n",
        "\n",
        "Here is the updated `SelfAttentionLayer` with masking capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCKgW_syNG7k",
        "outputId": "e3fac9d3-a4c7-48fa-9cdb-1bfeb95d7c53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder Look-Ahead Mask:\n",
            "[[0. 1. 1. 1. 1. 1. 1.]\n",
            " [0. 0. 1. 1. 1. 1. 1.]\n",
            " [0. 0. 0. 1. 1. 1. 1.]\n",
            " [0. 0. 0. 0. 1. 1. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            "Attention scores (p) with mask (should be lower-triangular):\n",
            "[[1.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.17 0.83 0.   0.   0.   0.   0.  ]\n",
            " [0.4  0.02 0.58 0.   0.   0.   0.  ]\n",
            " [0.36 0.14 0.28 0.22 0.   0.   0.  ]\n",
            " [0.24 0.43 0.13 0.08 0.12 0.   0.  ]\n",
            " [0.05 0.03 0.24 0.1  0.5  0.07 0.  ]\n",
            " [0.14 0.4  0.08 0.1  0.12 0.09 0.09]]\n"
          ]
        }
      ],
      "source": [
        "# Updated SelfAttentionLayer with masking (based on Listing 5.2)\n",
        "class SelfAttentionLayer(layers.Layer):\n",
        "    def __init__(self, d, **kwargs):\n",
        "        super(SelfAttentionLayer, self).__init__(**kwargs)\n",
        "        self.d = d\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.Wq = self.add_weight(shape=(input_shape[-1], self.d), initializer='glorot_uniform', trainable=True, name='Wq')\n",
        "        self.Wk = self.add_weight(shape=(input_shape[-1], self.d), initializer='glorot_uniform', trainable=True, name='Wk')\n",
        "        self.Wv = self.add_weight(shape=(input_shape[-1], self.d), initializer='glorot_uniform', trainable=True, name='Wv')\n",
        "\n",
        "    def call(self, q_x, k_x, v_x, mask=None):\n",
        "        q = tf.matmul(q_x, self.Wq)\n",
        "        k = tf.matmul(k_x, self.Wk)\n",
        "        v = tf.matmul(v_x, self.Wv)\n",
        "\n",
        "        p = tf.matmul(q, k, transpose_b=True) / math.sqrt(float(self.d))\n",
        "\n",
        "        # Apply the mask (if one is provided)\n",
        "        if mask is not None:\n",
        "            p += (mask * -1e9) # Add a large negative number where mask is 1\n",
        "\n",
        "        p = tf.nn.softmax(p)\n",
        "        h = tf.matmul(p, v)\n",
        "        return h, p\n",
        "\n",
        "# Create the triangular mask for a sequence of length 7\n",
        "# This is a look-ahead mask for the decoder\n",
        "mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
        "print(\"Decoder Look-Ahead Mask:\")\n",
        "print(mask.numpy())\n",
        "\n",
        "# Test the masked layer\n",
        "K_.clear_session()\n",
        "masked_layer = SelfAttentionLayer(d_model)\n",
        "h_masked, p_masked = masked_layer(x, x, x, mask=mask)\n",
        "\n",
        "print(\"\\nAttention scores (p) with mask (should be lower-triangular):\")\n",
        "print(p_masked.numpy()[0].round(2)) # Rounding for readability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_zTacqMNG7l"
      },
      "source": [
        "### 5.2.7 Multi-Head Attention\n",
        "\n",
        "Instead of one large self-attention calculation, **Multi-Head Attention** runs several smaller self-attention mechanisms in parallel (e.g., 8 \"heads\"). Each head can theoretically learn a different type of relationship between tokens.\n",
        "\n",
        "The outputs of all heads are concatenated and passed through a final linear layer to produce the sub-layer's final output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_wo23kHNG7l",
        "outputId": "b8bd6977-64e1-441f-8798-4973d9710862"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of one head: (1, 7, 64)\n",
            "Shape after concatenation: (1, 7, 512)\n"
          ]
        }
      ],
      "source": [
        "# Code snippet for multi-head attention\n",
        "n_heads = 8\n",
        "d_head = d_model // n_heads # 512 / 8 = 64\n",
        "\n",
        "multi_attn_heads = [SelfAttentionLayer(d_head) for i in range(n_heads)]\n",
        "\n",
        "# In a full implementation, you'd pass x to each head\n",
        "outputs = [head(x, x, x)[0] for head in multi_attn_heads]\n",
        "\n",
        "# Concatenate all head outputs\n",
        "outputs_concat = tf.concat(outputs, axis=-1)\n",
        "\n",
        "print(f\"Shape of one head: {outputs[0].shape}\")\n",
        "print(f\"Shape after concatenation: {outputs_concat.shape}\")\n",
        "\n",
        "# This concatenated output would then be passed through a final Dense layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knr6JeOMNG7m"
      },
      "source": [
        "### 5.2.8 Fully Connected Layer\n",
        "\n",
        "The second sub-layer in both the encoder and decoder is a simple, position-wise fully connected network. It consists of two `Dense` layers with a ReLU activation in between. This is applied independently to each token's representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSQFZ9dcNG7n",
        "outputId": "efce2f46-c826-446d-b68e-cf3764be8407"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape to FC: (1, 7, 512)\n",
            "Output shape from FC: (1, 7, 512)\n"
          ]
        }
      ],
      "source": [
        "# Implementation using Keras Dense layers (based on Listing 5.4)\n",
        "class FCLayer(layers.Layer):\n",
        "    def __init__(self, d1, d2, **kwargs):\n",
        "        super(FCLayer, self).__init__(**kwargs)\n",
        "        self.dense_layer_1 = layers.Dense(d1, activation='relu')\n",
        "        self.dense_layer_2 = layers.Dense(d2)\n",
        "\n",
        "    def call(self, x):\n",
        "        ff1 = self.dense_layer_1(x)\n",
        "        ff2 = self.dense_layer_2(ff1)\n",
        "        return ff2\n",
        "\n",
        "# Test the FC layer\n",
        "fc = FCLayer(d1=2048, d2=d_model)\n",
        "fc_out = fc(h_masked) # Using output from previous layer\n",
        "print(f\"Input shape to FC: {h_masked.shape}\")\n",
        "print(f\"Output shape from FC: {fc_out.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmJ8ri3nNG7n"
      },
      "source": [
        "### 5.2.9 Putting Everything Together: A Full Transformer Model\n",
        "\n",
        "Now we combine these components (`SelfAttentionLayer` and `FCLayer`) into an `EncoderLayer` and `DecoderLayer`. (Note: For simplicity, this code omits the residual connections and layer normalization mentioned in Chapter 13)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lEuFsOiFNG7o"
      },
      "outputs": [],
      "source": [
        "# Based on Listing 5.5 - Encoder Layer\n",
        "# (Note: This is a simplified version without multi-head or residuals for clarity)\n",
        "class EncoderLayer(layers.Layer):\n",
        "    def __init__(self, d, **kwargs):\n",
        "        super(EncoderLayer, self).__init__(**kwargs)\n",
        "        self.attn_head = SelfAttentionLayer(d)\n",
        "        self.fc_layer = FCLayer(2048, d)\n",
        "\n",
        "    def call(self, x):\n",
        "        h, _ = self.attn_head(x, x, x)\n",
        "        y = self.fc_layer(h)\n",
        "        return y\n",
        "\n",
        "# Based on Listing 5.6 - Decoder Layer\n",
        "class DecoderLayer(layers.Layer):\n",
        "    def __init__(self, d, **kwargs):\n",
        "        super(DecoderLayer, self).__init__(**kwargs)\n",
        "        self.masked_attn_head = SelfAttentionLayer(d, name=\"masked_attn\")\n",
        "        self.attn_head = SelfAttentionLayer(d, name=\"enc_dec_attn\")\n",
        "        self.fc_layer = FCLayer(2048, d)\n",
        "\n",
        "    def call(self, de_x, en_x, mask=None):\n",
        "        # 1. Masked self-attention (on decoder input)\n",
        "        h1, _ = self.masked_attn_head(de_x, de_x, de_x, mask)\n",
        "\n",
        "        # 2. Encoder-decoder attention (Query=decoder, Key/Value=encoder)\n",
        "        h2, _ = self.attn_head(q_x=h1, k_x=en_x, v_x=en_x)\n",
        "\n",
        "        # 3. Fully connected layer\n",
        "        y = self.fc_layer(h2)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "XFqWaU1MNG7o",
        "outputId": "f5e94eb5-b83c-43bf-98ef-1e3047268e88"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"MinTransformer\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"MinTransformer\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_input_ids   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │    \u001b[38;5;34m153,600\u001b[0m │ encoder_input_id… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_input_ids   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ enc_1               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │  \u001b[38;5;34m2,886,144\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mEncoderLayer\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │    \u001b[38;5;34m204,800\u001b[0m │ decoder_input_id… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ enc_2               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │  \u001b[38;5;34m2,886,144\u001b[0m │ enc_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mEncoderLayer\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dec_1               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │  \u001b[38;5;34m3,672,576\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mDecoderLayer\u001b[0m)      │                   │            │ enc_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dec_2               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │  \u001b[38;5;34m3,672,576\u001b[0m │ dec_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│ (\u001b[38;5;33mDecoderLayer\u001b[0m)      │                   │            │ enc_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ final_output        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m400\u001b[0m)   │    \u001b[38;5;34m205,200\u001b[0m │ dec_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_input_ids   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">153,600</span> │ encoder_input_id… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_input_ids   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ enc_1               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,886,144</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EncoderLayer</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">204,800</span> │ decoder_input_id… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ enc_2               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,886,144</span> │ enc_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EncoderLayer</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dec_1               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,672,576</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderLayer</span>)      │                   │            │ enc_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dec_2               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,672,576</span> │ dec_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderLayer</span>)      │                   │            │ enc_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ final_output        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">205,200</span> │ dec_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m13,681,040\u001b[0m (52.19 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,681,040</span> (52.19 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m13,681,040\u001b[0m (52.19 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,681,040</span> (52.19 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "K_.clear_session()\n",
        "\n",
        "# Hyperparameters\n",
        "n_steps = 25\n",
        "n_en_vocab = 300\n",
        "n_de_vocab = 400\n",
        "d = 512\n",
        "\n",
        "# Define the decoder look-ahead mask\n",
        "mask = 1 - tf.linalg.band_part(tf.ones((n_steps, n_steps)), -1, 0)\n",
        "\n",
        "# --- Define Model Inputs ---\n",
        "en_inp = layers.Input(shape=(n_steps,), name=\"encoder_input_ids\")\n",
        "de_inp = layers.Input(shape=(n_steps,), name=\"decoder_input_ids\")\n",
        "\n",
        "# --- Define Embedding Layers ---\n",
        "en_emb_layer = layers.Embedding(n_en_vocab, d)\n",
        "de_emb_layer = layers.Embedding(n_de_vocab, d)\n",
        "\n",
        "en_emb = en_emb_layer(en_inp)\n",
        "de_emb = de_emb_layer(de_inp)\n",
        "\n",
        "# --- Build Encoder & Decoder ---\n",
        "# (We'll use two layers for each)\n",
        "en_out1 = EncoderLayer(d, name=\"enc_1\")(en_emb)\n",
        "en_out2 = EncoderLayer(d, name=\"enc_2\")(en_out1)\n",
        "\n",
        "de_out1 = DecoderLayer(d, name=\"dec_1\")(de_emb, en_out2, mask)\n",
        "de_out2 = DecoderLayer(d, name=\"dec_2\")(de_out1, en_out2, mask)\n",
        "\n",
        "# --- Final Prediction Layer ---\n",
        "de_pred = layers.Dense(n_de_vocab, activation='softmax', name='final_output')(de_out2)\n",
        "\n",
        "# --- Create and Compile the Model ---\n",
        "transformer = models.Model(\n",
        "    inputs=[en_inp, de_inp],\n",
        "    outputs=de_pred,\n",
        "    name='MinTransformer'\n",
        ")\n",
        "\n",
        "transformer.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['acc']\n",
        ")\n",
        "\n",
        "transformer.summary()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}