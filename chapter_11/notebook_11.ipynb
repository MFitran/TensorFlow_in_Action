{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX_bpFtPunaS"
      },
      "source": [
        "# Chapter 11: Sequence-to-sequence learning: Part 1\n",
        "\n",
        "This notebook reproduces the code and summarizes the theoretical concepts from Chapter 11 of *'TensorFlow in Action'* by Thushan Ganegedara.\n",
        "\n",
        "This chapter introduces **sequence-to-sequence (seq2seq)** models, a powerful architecture for tasks that map an input sequence of one length to an output sequence of another length (e.g., machine translation).\n",
        "\n",
        "We will cover:\n",
        "1.  **Data Preparation**: Loading and processing a parallel English-to-German text corpus.\n",
        "2.  **The `TextVectorization` Layer**: Using this Keras layer to build an end-to-end model that accepts raw strings.\n",
        "3.  **Seq2seq Model Architecture**: Building an encoder-decoder model using GRUs (Gated Recurrent Units).\n",
        "4.  **Training (Teacher Forcing)**: How to train a seq2seq model using the \"teacher forcing\" technique.\n",
        "5.  **Inference Model**: Building a separate model for generating new translations recursively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0s78deJzunae"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7m_vkB6unaf"
      },
      "source": [
        "## 11.1 Understanding the machine translation data\n",
        "\n",
        "We will use an English-to-German parallel corpus from `manythings.org`. The data is a text file where each line contains an English sentence, a tab, and its German translation.\n",
        "\n",
        "**Note**: The book requires you to manually download the file `deu-eng.zip` from `http://www.manythings.org/anki/deu-eng.zip` and place it in a `data` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5_O7pJeunah",
        "outputId": "d6fca0f4-ea76-47e2-cac7-6898a4b36153"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data already extracted.\n",
            "Loaded and processed 50000 samples.\n",
            "\n",
            "Sample data:\n",
            "                                                   EN  \\\n",
            "158450                 Do I look like I'm having fun?   \n",
            "283485  We took the elevator down to the third floor.   \n",
            "37230                             Mary lost her baby.   \n",
            "75516                         I prefer mineral water.   \n",
            "8316                                   Tom is wasted.   \n",
            "\n",
            "                                                       DE  \n",
            "158450  sos Sehe ich so aus, als würde ich mich amüsie...  \n",
            "283485  sos Wir haben den Fahrstuhl hinunter in den zw...  \n",
            "37230                sos Maria hat ihr Kind verloren. eos  \n",
            "75516                     sos Ich mag lieber Selters. eos  \n",
            "8316                            sos Tom ist erledigt. eos  \n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.models import load_model # Added for inference model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import zipfile\n",
        "import json\n",
        "from collections import Counter # Import Counter\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "random_seed = 4321\n",
        "np.random.seed(random_seed)\n",
        "tf.random.set_seed(random_seed)\n",
        "\n",
        "# --- 1. Load and Extract Data ---\n",
        "data_dir = 'data'\n",
        "zip_path = os.path.join(data_dir, 'deu-eng.zip')\n",
        "extracted_path = os.path.join(data_dir, 'deu.txt')\n",
        "\n",
        "# Ensure data directory exists\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "if not os.path.exists(extracted_path):\n",
        "    if not os.path.exists(zip_path):\n",
        "        print(f\"Downloading 'deu-eng.zip' to '{data_dir}'...\")\n",
        "        # Use !wget to download the file directly in Colab\n",
        "        !wget -P {data_dir} http://www.manythings.org/anki/deu-eng.zip\n",
        "        print(\"Download complete.\")\n",
        "\n",
        "    print(\"Extracting data...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(data_dir)\n",
        "    print(\"Extraction complete.\")\n",
        "else:\n",
        "    print(\"Data already extracted.\")\n",
        "\n",
        "# --- 2. Read data into pandas ---\n",
        "df = pd.read_csv(extracted_path, delimiter='\\t', header=None)\n",
        "df.columns = [\"EN\", \"DE\", \"Attribution\"]\n",
        "df = df[[\"EN\", \"DE\"]]\n",
        "\n",
        "# Clean up problematic unicode characters from the book's example\n",
        "clean_inds = [i for i in range(len(df)) if b\"\\xc2\" not in df.iloc[i][\"DE\"].encode(\"utf-8\")]\n",
        "df = df.iloc[clean_inds]\n",
        "\n",
        "# --- 3. Sample and Preprocess Data ---\n",
        "n_samples = 50000\n",
        "df = df.sample(n=n_samples, random_state=random_seed)\n",
        "\n",
        "# Add 'sos' (start of sentence) and 'eos' (end of sentence) tokens\n",
        "# These are crucial for the decoder during training and inference.\n",
        "start_token = 'sos'\n",
        "end_token = 'eos'\n",
        "df[\"DE\"] = start_token + ' ' + df[\"DE\"] + ' ' + end_token\n",
        "\n",
        "print(f\"Loaded and processed {len(df)} samples.\")\n",
        "print(\"\\nSample data:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMEaDQUnunak",
        "outputId": "d0ffbdd9-bb5d-49f9-c422-3b69eed6a8c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training samples: 40000\n",
            "Validation samples: 5000\n",
            "Test samples: 5000\n",
            "Vocabulary size (>= 10 freq): 2173\n",
            "Vocabulary size (>= 10 freq): 2453\n",
            "EN max sequence length (99th percentile + 5): 19\n",
            "DE max sequence length (99th percentile + 5): 21\n"
          ]
        }
      ],
      "source": [
        "# --- 4. Create train/validation/test splits ---\n",
        "n_test = int(n_samples / 10)\n",
        "n_valid = int(n_samples / 10)\n",
        "\n",
        "test_df = df.sample(n=n_test, random_state=random_seed)\n",
        "valid_df = df.loc[~df.index.isin(test_df.index)].sample(n=n_valid, random_state=random_seed)\n",
        "train_df = df.loc[~(df.index.isin(test_df.index) | df.index.isin(valid_df.index))]\n",
        "\n",
        "print(f\"\\nTraining samples: {len(train_df)}\")\n",
        "print(f\"Validation samples: {len(valid_df)}\")\n",
        "print(f\"Test samples: {len(test_df)}\")\n",
        "\n",
        "# --- 5. Analyze Vocabulary and Sequence Length ---\n",
        "# (Using helper function from Listing 11.1)\n",
        "def get_vocabulary_size_greater_than(words, n, verbose=True):\n",
        "    counter = Counter(words)\n",
        "    freq_df = pd.Series(list(counter.values()), index=list(counter.keys())).sort_values(ascending=False)\n",
        "    n_vocab = (freq_df >= n).sum()\n",
        "    if verbose: print(f\"Vocabulary size (>= {n} freq): {n_vocab}\")\n",
        "    return n_vocab\n",
        "\n",
        "en_words = train_df[\"EN\"].str.split().sum()\n",
        "de_words = train_df[\"DE\"].str.split().sum()\n",
        "\n",
        "en_vocab = get_vocabulary_size_greater_than(en_words, n=10)\n",
        "de_vocab = get_vocabulary_size_greater_than(de_words, n=10)\n",
        "\n",
        "# Get 99th percentile for sequence lengths\n",
        "en_seq_length = int(train_df[\"EN\"].str.split().str.len().quantile(0.99)) + 5\n",
        "de_seq_length = int(train_df[\"DE\"].str.split().str.len().quantile(0.99)) + 5\n",
        "\n",
        "print(f\"EN max sequence length (99th percentile + 5): {en_seq_length}\")\n",
        "print(f\"DE max sequence length (99th percentile + 5): {de_seq_length}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUtSVIvkunal"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOP0sgfmunal"
      },
      "source": [
        "## 11.2 Writing an English-German seq2seq machine translator\n",
        "\n",
        "A seq2seq model consists of two main parts:\n",
        "1.  **Encoder**: An RNN (we'll use a GRU) that reads the input English sentence one token at a time and compresses its meaning into a single vector, known as the **context vector** or \"thought vector\". This is the final hidden state of the encoder.\n",
        "2.  **Decoder**: Another RNN (also a GRU) that takes the encoder's context vector as its *initial hidden state*. It then generates the output German sentence one token at a time.\n",
        "\n",
        "### 11.2.1 The `TextVectorization` Layer\n",
        "\n",
        "Instead of preprocessing our text into integers *before* feeding it to the model, we can build the preprocessing *into* the model using the `TextVectorization` layer. This layer will:\n",
        "1.  Be `adapt`ed (fitted) on our training corpus to build a vocabulary.\n",
        "2.  When the model is running, it will take raw strings as input.\n",
        "3.  It will automatically tokenize, convert to integers, and pad the sequences to a fixed length, all inside the model graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "to5A-M05unam",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bd13ff4-3034-47b8-b1ef-19fd4d463cf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Vocabulary size: 2175\n",
            "German Vocabulary size: 2455\n",
            "\n",
            "Test EN Vectorizer:\n",
            "tf.Tensor(\n",
            "[[   5   31  941 1115    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]], shape=(1, 19), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "# Based on Listing 11.3\n",
        "def get_vectorizer(corpus, n_vocab, max_length=None, return_vocabulary=True, name=None):\n",
        "    \"\"\"Creates a TextVectorization layer/model.\"\"\"\n",
        "\n",
        "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name=f'{name}_input')\n",
        "\n",
        "    # We add 2 to the vocab size for the <PAD> (ID 0) and [UNK] (ID 1) tokens\n",
        "    vectorize_layer = TextVectorization(\n",
        "        max_tokens=n_vocab + 2,\n",
        "        output_mode='int',\n",
        "        output_sequence_length=max_length,\n",
        "        name=name\n",
        "    )\n",
        "\n",
        "    # Build the vocabulary\n",
        "    vectorize_layer.adapt(corpus)\n",
        "    vectorized_out = vectorize_layer(inp)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=inp, outputs=vectorized_out)\n",
        "\n",
        "    if return_vocabulary:\n",
        "        return model, vectorize_layer.get_vocabulary()\n",
        "    return model\n",
        "\n",
        "# Create the vectorizers for English and German\n",
        "# Note: The decoder's max_length is de_seq_length - 1\n",
        "# This is because we will feed it 'sos ... word_n' (length N) to predict 'word_1 ... eos' (length N)\n",
        "en_vectorizer, en_vocabulary = get_vectorizer(\n",
        "    corpus=np.array(train_df[\"EN\"].tolist()),\n",
        "    n_vocab=en_vocab,\n",
        "    max_length=en_seq_length,\n",
        "    name='en_vectorizer'\n",
        ")\n",
        "de_vectorizer, de_vocabulary = get_vectorizer(\n",
        "    corpus=np.array(train_df[\"DE\"].tolist()),\n",
        "    n_vocab=de_vocab,\n",
        "    max_length=de_seq_length - 1,\n",
        "    name='de_vectorizer'\n",
        ")\n",
        "\n",
        "print(f\"English Vocabulary size: {len(en_vocabulary)}\")\n",
        "print(f\"German Vocabulary size: {len(de_vocabulary)}\")\n",
        "\n",
        "# Test the English vectorizer\n",
        "print(\"\\nTest EN Vectorizer:\")\n",
        "print(en_vectorizer(np.array([[\"I like machine learning\"]])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYeiwnuhunav"
      },
      "source": [
        "### 11.2.3 & 11.2.4 Defining the Encoder and Decoder\n",
        "\n",
        "Now we build the full seq2seq model using the Keras Functional API.\n",
        "\n",
        "**Encoder (Listing 11.4):**\n",
        "1.  Input (Raw English strings)\n",
        "2.  `en_vectorizer` (Text -> Integer IDs)\n",
        "3.  `Embedding` Layer (IDs -> Dense Vectors)\n",
        "4.  `Bidirectional(GRU)`: Reads the sequence forwards and backwards. The final hidden state is the context vector.\n",
        "\n",
        "**Decoder (Listing 11.5):**\n",
        "1.  Input (Raw German strings, e.g., \"sos Ich möchte ein...\")\n",
        "2.  `de_vectorizer` (Text -> Integer IDs)\n",
        "3.  `Embedding` Layer (IDs -> Dense Vectors)\n",
        "4.  `GRU`: This GRU's **initial_state** is set to the **encoder's context vector**.\n",
        "5.  `Dense` Layer (with Softmax): Predicts the next word in the German vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09TN4Vwjunaw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "c379e6e8-209d-482e-d346-78373d411c3f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"final_seq2seq\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"final_seq2seq\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ d_input             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ d_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ e_input_final       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ d_embedding         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m314,240\u001b[0m │ functional_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m476,544\u001b[0m │ e_input_final[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ d_gru (\u001b[38;5;33mGRU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m296,448\u001b[0m │ d_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│                     │                   │            │ encoder[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ d_dense_1 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │    \u001b[38;5;34m131,584\u001b[0m │ d_gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ d_dense_final       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m2455\u001b[0m)  │  \u001b[38;5;34m1,259,415\u001b[0m │ d_dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ d_input             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ d_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ e_input_final       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ d_embedding         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">314,240</span> │ functional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">476,544</span> │ e_input_final[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ d_gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">296,448</span> │ d_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                     │                   │            │ encoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ d_dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ d_gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ d_dense_final       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2455</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,259,415</span> │ d_dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,478,231\u001b[0m (9.45 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,478,231</span> (9.45 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,478,231\u001b[0m (9.45 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,478,231</span> (9.45 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "K.clear_session()\n",
        "\n",
        "# --- Define Encoder ---\n",
        "def get_encoder(n_vocab, vectorizer):\n",
        "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input')\n",
        "    vectorized_out = vectorizer(inp)\n",
        "    emb_layer = layers.Embedding(\n",
        "        n_vocab + 2, 128, mask_zero=True, name='e_embedding'\n",
        "    )\n",
        "    emb_out = emb_layer(vectorized_out)\n",
        "    gru_layer = layers.Bidirectional(\n",
        "        layers.GRU(128, name='e_gru'), name='e_bidirectional_gru'\n",
        "    )\n",
        "    gru_out = gru_layer(emb_out)\n",
        "    encoder = tf.keras.models.Model(inputs=inp, outputs=gru_out, name='encoder')\n",
        "    return encoder\n",
        "\n",
        "# --- Define Final Seq2Seq Model ---\n",
        "def get_final_seq2seq_model(n_vocab, encoder, vectorizer):\n",
        "    e_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input_final')\n",
        "    d_init_state = encoder(e_inp)\n",
        "\n",
        "    d_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_input')\n",
        "    d_vectorized_out = vectorizer(d_inp)\n",
        "\n",
        "    d_emb_layer = layers.Embedding(\n",
        "        n_vocab + 2, 128, mask_zero=True, name='d_embedding'\n",
        "    )\n",
        "    d_emb_out = d_emb_layer(d_vectorized_out)\n",
        "\n",
        "    d_gru_layer = layers.GRU(\n",
        "        256, return_sequences=True, name='d_gru' # 256 units = 128 (fwd) + 128 (bwd) from encoder\n",
        "    )\n",
        "    # The encoder's state is fed as the initial state to the decoder's GRU\n",
        "    d_gru_out = d_gru_layer(d_emb_out, initial_state=d_init_state)\n",
        "\n",
        "    d_dense_layer_1 = layers.Dense(512, activation='relu', name='d_dense_1')\n",
        "    d_dense1_out = d_dense_layer_1(d_gru_out)\n",
        "\n",
        "    d_final_layer = layers.Dense(n_vocab + 2, activation='softmax', name='d_dense_final')\n",
        "    d_final_out = d_final_layer(d_dense1_out)\n",
        "\n",
        "    seq2seq = tf.keras.models.Model(\n",
        "        inputs=[e_inp, d_inp], outputs=d_final_out, name='final_seq2seq'\n",
        "    )\n",
        "    return seq2seq\n",
        "\n",
        "# Get the models\n",
        "encoder = get_encoder(n_vocab=en_vocab, vectorizer=en_vectorizer)\n",
        "final_model = get_final_seq2seq_model(n_vocab=de_vocab, encoder=encoder, vectorizer=de_vectorizer)\n",
        "\n",
        "# Compile the model\n",
        "final_model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "final_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmrs-FUmunay"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u1w06osunaz"
      },
      "source": [
        "## 11.3 Training and evaluating the model\n",
        "\n",
        "To train this model, we use **teacher forcing**.\n",
        "\n",
        "This means for a translation pair `(\"I like cats\", \"sos Ich mag Katzen eos\")`:\n",
        "* `x` (inputs) = `(\"I like cats\", \"sos Ich mag Katzen\")`\n",
        "* `y` (target) = `(\"Ich\", \"mag\", \"Katzen\", \"eos\")`\n",
        "\n",
        "The decoder receives the *true* previous word (e.g., \"mag\") as input to help it predict the next word (e.g., \"Katzen\"). This stabilizes and speeds up training.\n",
        "\n",
        "We also need to define a custom training loop to correctly calculate the **BLEU score**, a standard metric for machine translation that measures the overlap of n-grams between the predicted and reference translations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTvbz2qYuna0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6f53590-b3bd-4b50-8e54-e6e34db383a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shapes:\n",
            "(40000, 1) (40000, 1) (40000, 20)\n",
            "\n",
            "Starting model training (1 epoch for demo)...\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m747s\u001b[0m 596ms/step - accuracy: 0.1566 - loss: 3.1215 - val_accuracy: 0.1781 - val_loss: 2.6620\n",
            "Training complete.\n",
            "Model saved to models/seq2seq_ch11.keras\n"
          ]
        }
      ],
      "source": [
        "# Based on Listing 11.6 - Prepare data for teacher forcing\n",
        "def prepare_data(df):\n",
        "    # Reshape all string arrays to (N, 1) to match TextVectorization model input shape (None, 1)\n",
        "    en_inputs = np.array(df[\"EN\"].tolist()).reshape(-1, 1)\n",
        "    # Explicitly convert to TensorFlow string tensor\n",
        "    en_inputs_tf = tf.constant(en_inputs, dtype=tf.string)\n",
        "\n",
        "    # Decoder inputs = 'sos ... word_n'\n",
        "    de_inputs = np.array(df[\"DE\"].str.rsplit(n=1, expand=True).iloc[:, 0].tolist()).reshape(-1, 1)\n",
        "    # Explicitly convert to TensorFlow string tensor\n",
        "    de_inputs_tf = tf.constant(de_inputs, dtype=tf.string)\n",
        "\n",
        "    # Decoder labels = 'word_1 ... eos'\n",
        "    de_labels_str = np.array(df[\"DE\"].str.split(n=1, expand=True).iloc[:, 1].tolist()).reshape(-1, 1)\n",
        "    # Explicitly convert to TensorFlow string tensor for vectorizer input\n",
        "    de_labels_str_tf = tf.constant(de_labels_str, dtype=tf.string)\n",
        "\n",
        "    # The labels need to be vectorized *without* the 'sos' token,\n",
        "    # so we create a separate vectorizer for them.\n",
        "    # We still adapt on the full German corpus including 'sos' and 'eos' to get a complete vocabulary.\n",
        "    # The corpus for adapt is fine as a 1D array of strings.\n",
        "    de_label_vectorizer_model = get_vectorizer(\n",
        "        corpus=np.array(train_df[\"DE\"].tolist()), # This is for adapt, 1D array is fine here.\n",
        "        n_vocab=de_vocab,\n",
        "        max_length=de_seq_length - 1,\n",
        "        return_vocabulary=False, # We just need the model here\n",
        "        name='de_label_vectorizer'\n",
        "    )\n",
        "\n",
        "    # Convert string labels to token IDs using the new model.\n",
        "    de_labels_vec = de_label_vectorizer_model(de_labels_str_tf)\n",
        "    return en_inputs_tf, de_inputs_tf, de_labels_vec\n",
        "\n",
        "en_train, de_train_in, de_train_out = prepare_data(train_df)\n",
        "en_valid, de_valid_in, de_valid_out = prepare_data(valid_df)\n",
        "\n",
        "print(\"Training data shapes:\")\n",
        "print(en_train.shape, de_train_in.shape, de_train_out.shape)\n",
        "\n",
        "# Train the model (simplified .fit() call from the book)\n",
        "# The book uses a custom training loop (Listing 11.10) to calculate BLEU.\n",
        "# For simplicity, we will use model.fit() here.\n",
        "\n",
        "print(\"\\nStarting model training (1 epoch for demo)...\")\n",
        "history = final_model.fit(\n",
        "    x=[en_train, de_train_in],\n",
        "    y=de_train_out,\n",
        "    validation_data=([en_valid, de_valid_in], de_valid_out),\n",
        "    epochs=1, # Book uses 5\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# Save the model in the native Keras format\n",
        "os.makedirs('models', exist_ok=True)\n",
        "model_path = os.path.join('models', 'seq2seq_ch11.keras') # Changed to .keras\n",
        "final_model.save(model_path)\n",
        "print(f\"Model saved to {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9r5cNN8Buna2"
      },
      "source": [
        "## 11.4 From training to inference: Defining the inference model\n",
        "\n",
        "We can't use the trained model directly for inference because it relies on **teacher forcing** (i.e., it expects the *true* German sentence as an input to the decoder).\n",
        "\n",
        "For inference, we must build a new model that generates text **recursively**:\n",
        "1.  Feed the English sentence to the **Encoder** to get the context vector.\n",
        "2.  Feed the context vector (as the initial state) and the `sos` token to the **Decoder**.\n",
        "3.  The Decoder predicts the first word (e.g., \"Ich\").\n",
        "4.  Feed the *new* state and the predicted word (\"Ich\") back into the Decoder.\n",
        "5.  The Decoder predicts the second word (e.g., \"möchte\").\n",
        "6.  Repeat this process until the Decoder predicts the `eos` token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "032fa5d8",
        "outputId": "2425b529-4847-4477-cb84-f5c01afd08c6"
      },
      "source": [
        "# Based on Listing 11.11 - Create the inference models\n",
        "\n",
        "def get_inference_model(save_path, de_vocab_size):\n",
        "    print(\"Loading trained model and building inference models...\")\n",
        "    K.clear_session()\n",
        "    model = load_model(save_path)\n",
        "\n",
        "    # 1. Get the Encoder\n",
        "    # Need to reconstruct the encoder with a fixed batch_shape\n",
        "    # The original encoder's input was tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input')\n",
        "    # We need to recreate it to ensure a fixed batch size for inference.\n",
        "    # Get the layers from the original encoder in the loaded model\n",
        "    original_encoder = model.get_layer(\"encoder\")\n",
        "    # Corrected: Use the globally available en_vectorizer, not from original_encoder\n",
        "    global en_vectorizer # Ensure en_vectorizer is accessible\n",
        "\n",
        "    # Clone encoder layers to ensure proper re-use in a new graph\n",
        "    encoder_embedding_config = original_encoder.get_layer('e_embedding').get_config()\n",
        "    encoder_embedding = layers.Embedding.from_config(encoder_embedding_config)\n",
        "    _ = encoder_embedding(tf.zeros((1, en_seq_length), dtype=tf.int32)) # Build layer\n",
        "    encoder_embedding.set_weights(original_encoder.get_layer('e_embedding').get_weights())\n",
        "\n",
        "    encoder_gru_config = original_encoder.get_layer('e_bidirectional_gru').get_config()\n",
        "    encoder_gru = layers.Bidirectional.from_config(encoder_gru_config)\n",
        "    _ = encoder_gru(tf.zeros((1, en_seq_length, 128))) # Build layer\n",
        "    encoder_gru.set_weights(original_encoder.get_layer('e_bidirectional_gru').get_weights())\n",
        "\n",
        "\n",
        "    # Define a new input for the encoder with fixed batch_shape\n",
        "    e_infer_input = tf.keras.Input(batch_shape=(1, 1), dtype=tf.string, name='e_infer_input')\n",
        "    e_vectorized_out = en_vectorizer(e_infer_input)\n",
        "    e_emb_out = encoder_embedding(e_vectorized_out)\n",
        "    e_gru_out = encoder_gru(e_emb_out)\n",
        "    en_model = tf.keras.models.Model(inputs=e_infer_input, outputs=e_gru_out, name='encoder_inference')\n",
        "\n",
        "    # 2. Build the Decoder\n",
        "    # We need to define new inputs for the decoder's state\n",
        "    d_inp = tf.keras.Input(batch_shape=(1, 1), dtype=tf.string, name='d_infer_input')\n",
        "    d_state_inp = tf.keras.Input(batch_shape=(1, 256), name='d_infer_state') # 256 = GRU units\n",
        "\n",
        "    # Recreate the TextVectorization layer for German text\n",
        "    # We use the globally available 'de_vocabulary' for this.\n",
        "    global de_vocabulary, de_seq_length\n",
        "    recreated_de_vectorizer = TextVectorization(\n",
        "        max_tokens=len(de_vocabulary), # Adjust max_tokens to match the actual vocabulary size\n",
        "        output_mode='int',\n",
        "        output_sequence_length=1, # <--- CRUCIAL FIX: Set sequence length to 1 for inference\n",
        "        name='recreated_de_vectorizer' # Assign a unique name\n",
        "    )\n",
        "    recreated_de_vectorizer.set_vocabulary(de_vocabulary)\n",
        "    # Explicitly build the vectorizer with a dummy input of the correct batch_shape\n",
        "    _ = recreated_de_vectorizer(tf.zeros(shape=(1, 1), dtype=tf.string))\n",
        "\n",
        "    # Clone decoder layers to ensure proper re-use in a new graph\n",
        "    d_emb_layer_config = model.get_layer('d_embedding').get_config()\n",
        "    d_emb_layer = layers.Embedding.from_config(d_emb_layer_config)\n",
        "    _ = d_emb_layer(tf.zeros((1, 1), dtype=tf.int32)) # Build layer\n",
        "    d_emb_layer.set_weights(model.get_layer('d_embedding').get_weights())\n",
        "\n",
        "    # Rebuild GRU layer explicitly with correct settings and input_shape\n",
        "    d_gru_layer_original = model.get_layer(\"d_gru\")\n",
        "    d_gru_units = d_gru_layer_original.units\n",
        "    d_emb_output_dim = d_emb_layer.output_dim\n",
        "    d_gru_layer = layers.GRU(\n",
        "        units=d_gru_units,\n",
        "        return_sequences=False,\n",
        "        return_state=True,\n",
        "        # Removed input_shape as it can conflict with inferred shapes in functional API\n",
        "        name='d_gru_inference' # Give it a new name\n",
        "    )\n",
        "    # Build the layer with dummy input matching the batch_shape of d_emb_out\n",
        "    _ = d_gru_layer(tf.zeros((1, 1, d_emb_output_dim)), initial_state=tf.zeros((1, d_gru_units)))\n",
        "    d_gru_layer.set_weights(d_gru_layer_original.get_weights())\n",
        "\n",
        "    d_dense_layer_1_config = model.get_layer(\"d_dense_1\").get_config()\n",
        "    d_dense_layer_1 = layers.Dense.from_config(d_dense_layer_1_config)\n",
        "    _ = d_dense_layer_1(tf.zeros((1, 256))) # Build layer\n",
        "    d_dense_layer_1.set_weights(model.get_layer(\"d_dense_1\").get_weights())\n",
        "\n",
        "    d_final_layer_config = model.get_layer(\"d_dense_final\").get_config()\n",
        "    d_final_layer = layers.Dense.from_config(d_final_layer_config)\n",
        "    _ = d_final_layer(tf.zeros((1, 512))) # Build layer\n",
        "    d_final_layer.set_weights(model.get_layer(\"d_dense_final\").get_weights())\n",
        "\n",
        "    # Build the graph using the recreated vectorizer\n",
        "    d_vectorized_out = recreated_de_vectorizer(d_inp)\n",
        "    d_emb_out = d_emb_layer(d_vectorized_out)\n",
        "\n",
        "    # GRU now returns output and state\n",
        "    d_gru_output, d_new_state = d_gru_layer(d_emb_out, initial_state=d_state_inp)\n",
        "\n",
        "    d_dense1_out = d_dense_layer_1(d_gru_output) # Use d_gru_output for dense layers\n",
        "    d_final_out = d_final_layer(d_dense1_out)\n",
        "\n",
        "    de_model = tf.keras.models.Model(\n",
        "        inputs=[d_inp, d_state_inp],\n",
        "        outputs=[d_final_out, d_new_state] # Output prediction AND new state\n",
        "    )\n",
        "    return en_model, de_model\n",
        "\n",
        "# Update model_path to use the new .keras extension\n",
        "model_path = os.path.join('models', 'seq2seq_ch11.keras')\n",
        "en_model, de_model = get_inference_model(model_path, de_vocab_size=de_vocab)\n",
        "print(\"Inference models built.\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading trained model and building inference models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 17 variables whereas the saved optimizer has 32 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference models built.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        },
        "id": "7ff67b0f",
        "outputId": "36a0ff21-3f4c-4385-8fad-9cca7ac406a7"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Based on Listing 11.11 - Create the inference models\n",
        "\n",
        "def get_inference_model(save_path, de_vocab_size):\n",
        "    print(\"Loading trained model and building inference models...\")\n",
        "    K.clear_session()\n",
        "    model = load_model(save_path)\n",
        "\n",
        "    # 1. Get the Encoder\n",
        "    # Need to reconstruct the encoder with a fixed batch_shape\n",
        "    # The original encoder's input was tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input')\n",
        "    # We need to recreate it to ensure a fixed batch size for inference.\n",
        "    # Get the layers from the original encoder in the loaded model\n",
        "    original_encoder = model.get_layer(\"encoder\")\n",
        "    # Use the globally available en_vectorizer, not from original_encoder\n",
        "    global en_vectorizer # Ensure en_vectorizer is accessible\n",
        "\n",
        "    # Clone encoder layers to ensure proper re-use in a new graph\n",
        "    encoder_embedding_config = original_encoder.get_layer('e_embedding').get_config()\n",
        "    encoder_embedding = layers.Embedding.from_config(encoder_embedding_config)\n",
        "    _ = encoder_embedding(tf.zeros((1, en_seq_length), dtype=tf.int32)) # Build layer\n",
        "    encoder_embedding.set_weights(original_encoder.get_layer('e_embedding').get_weights())\n",
        "\n",
        "    encoder_gru_config = original_encoder.get_layer('e_bidirectional_gru').get_config()\n",
        "    encoder_gru = layers.Bidirectional.from_config(encoder_gru_config)\n",
        "    # The Bidirectional GRU input shape is (batch_size, sequence_length, embedding_dim)\n",
        "    # where batch_size is 1, sequence_length is en_seq_length, embedding_dim is 128\n",
        "    _ = encoder_gru(tf.zeros((1, en_seq_length, 128))) # Build layer\n",
        "    encoder_gru.set_weights(original_encoder.get_layer('e_bidirectional_gru').get_weights())\n",
        "\n",
        "\n",
        "    # Define a new input for the encoder with fixed batch_shape\n",
        "    e_infer_input = tf.keras.Input(batch_shape=(1, 1), dtype=tf.string, name='e_infer_input')\n",
        "    e_vectorized_out = en_vectorizer(e_infer_input) # Output (1, en_seq_length)\n",
        "    e_emb_out = encoder_embedding(e_vectorized_out) # Output (1, en_seq_length, 128)\n",
        "    e_gru_out = encoder_gru(e_emb_out) # Output (1, 256)\n",
        "    en_model = tf.keras.models.Model(inputs=e_infer_input, outputs=e_gru_out, name='encoder_inference')\n",
        "\n",
        "    # 2. Build the Decoder\n",
        "    # We need to define new inputs for the decoder's state\n",
        "    d_inp = tf.keras.Input(batch_shape=(1, 1), dtype=tf.string, name='d_infer_input') # Decoder's current word input\n",
        "    d_state_inp = tf.keras.Input(batch_shape=(1, 256), name='d_infer_state') # Decoder's recurrent state input (from encoder or previous step)\n",
        "\n",
        "    # Recreate the TextVectorization layer for German text\n",
        "    global de_vocabulary, de_seq_length\n",
        "    recreated_de_vectorizer = TextVectorization(\n",
        "        max_tokens=len(de_vocabulary),\n",
        "        output_mode='int',\n",
        "        output_sequence_length=1, # Crucial: single token at a time for inference\n",
        "        name='recreated_de_vectorizer'\n",
        "    )\n",
        "    recreated_de_vectorizer.set_vocabulary(de_vocabulary)\n",
        "    _ = recreated_de_vectorizer(tf.zeros(shape=(1, 1), dtype=tf.string)) # Build with dummy input\n",
        "\n",
        "    # Rebuild Embedding layer explicitly\n",
        "    d_emb_layer_original = model.get_layer('d_embedding')\n",
        "    d_emb_layer = layers.Embedding(\n",
        "        input_dim=d_emb_layer_original.input_dim,\n",
        "        output_dim=d_emb_layer_original.output_dim,\n",
        "        mask_zero=d_emb_layer_original.mask_zero,\n",
        "        name='d_embedding_inference'\n",
        "    )\n",
        "    _ = d_emb_layer(tf.zeros((1, 1), dtype=tf.int32)) # Build layer with appropriate dummy input\n",
        "    d_emb_layer.set_weights(d_emb_layer_original.get_weights())\n",
        "\n",
        "\n",
        "    # Rebuild GRU layer explicitly with correct settings\n",
        "    d_gru_layer_original = model.get_layer(\"d_gru\")\n",
        "    d_gru_units = d_gru_layer_original.units\n",
        "    d_emb_output_dim = d_emb_layer.output_dim # This will be 128 (embedding_dim)\n",
        "    d_gru_layer = layers.GRU(\n",
        "        units=d_gru_units,\n",
        "        return_sequences=False, # Single output per step\n",
        "        return_state=True,     # Return the updated state\n",
        "        name='d_gru_inference' # Give it a new name\n",
        "    )\n",
        "    # Build the GRU layer with dummy input matching its expected sequence input and initial state\n",
        "    _ = d_gru_layer(tf.zeros((1, 1, d_emb_output_dim)), initial_state=tf.zeros((1, d_gru_units)))\n",
        "    d_gru_layer.set_weights(d_gru_layer_original.get_weights()) # Set weights after building\n",
        "\n",
        "    d_dense_layer_1_config = model.get_layer(\"d_dense_1\").get_config()\n",
        "    d_dense_layer_1 = layers.Dense.from_config(d_dense_layer_1_config)\n",
        "    _ = d_dense_layer_1(tf.zeros((1, 256))) # Build layer\n",
        "    d_dense_layer_1.set_weights(model.get_layer(\"d_dense_1\").get_weights())\n",
        "\n",
        "    d_final_layer_config = model.get_layer(\"d_dense_final\").get_config()\n",
        "    d_final_layer = layers.Dense.from_config(d_final_layer_config)\n",
        "    _ = d_final_layer(tf.zeros((1, 512))) # Build layer\n",
        "    d_final_layer.set_weights(model.get_layer(\"d_dense_final\").get_weights())\n",
        "\n",
        "    # Build the graph using the recreated vectorizer\n",
        "    d_vectorized_out = recreated_de_vectorizer(d_inp) # Output (1, 1)\n",
        "    d_emb_out = d_emb_layer(d_vectorized_out)         # Output (1, 1, 128)\n",
        "\n",
        "    # GRU now returns output and state\n",
        "    d_gru_output, d_new_state = d_gru_layer(d_emb_out, initial_state=d_state_inp) # d_gru_output (1, 256), d_new_state (1, 256)\n",
        "\n",
        "    d_dense1_out = d_dense_layer_1(d_gru_output) # Use d_gru_output for dense layers, output (1, 512)\n",
        "    d_final_out = d_final_layer(d_dense1_out)   # Output (1, vocab_size+2)\n",
        "\n",
        "    de_model_base = tf.keras.models.Model(\n",
        "        inputs=[d_inp, d_state_inp],\n",
        "        outputs=[d_final_out, d_new_state] # Output prediction AND new state\n",
        "    )\n",
        "\n",
        "    # Wrap the decoder model in tf.function with explicit input_signature\n",
        "    @tf.function(input_signature=[\n",
        "        tf.TensorSpec(shape=(1, 1), dtype=tf.string), # Input for d_inp (current word token)\n",
        "        tf.TensorSpec(shape=(1, 256), dtype=tf.float32) # Input for d_state_inp (previous GRU state)\n",
        "    ])\n",
        "    def de_model_inference_step(input_token, prev_state):\n",
        "        pred_logits, new_state_raw = de_model_base(inputs=[input_token, prev_state], training=False)\n",
        "        # Explicitly ensure the shape of the output state\n",
        "        new_state = tf.ensure_shape(new_state_raw, (1, 256))\n",
        "        return pred_logits, new_state\n",
        "\n",
        "    return en_model, de_model_inference_step\n",
        "\n",
        "# Update model_path to use the new .keras extension\n",
        "model_path = os.path.join('models', 'seq2seq_ch11.keras')\n",
        "en_model, de_model_inference_step = get_inference_model(model_path, de_vocab_size=de_vocab)\n",
        "print(\"Inference models built.\")\n",
        "\n",
        "def generate_new_translation(en_model, de_model_inference_step, de_vocabulary, sample_en_text, max_len=20):\n",
        "    print(f\"Input: {sample_en_text}\")\n",
        "\n",
        "    # 1. Get the context vector from the encoder\n",
        "    d_state = en_model.predict(tf.constant([[sample_en_text]], dtype=tf.string), verbose=0)\n",
        "\n",
        "    # 2. Start the decoder with the 'sos' token\n",
        "    de_word = start_token\n",
        "    de_translation = []\n",
        "\n",
        "    # 3. Recursive loop\n",
        "    for _ in range(max_len):\n",
        "        # Predict the next word and get the new state using the tf.function wrapped step\n",
        "        # Pass inputs directly to the tf.function, not through .predict()\n",
        "        de_pred, d_state = de_model_inference_step(tf.constant([[de_word]], dtype=tf.string), d_state)\n",
        "\n",
        "        # Get the word ID with the highest probability\n",
        "        de_word_id = np.argmax(de_pred[0])\n",
        "\n",
        "        # Look up the word from the ID\n",
        "        de_word = de_vocabulary[de_word_id]\n",
        "\n",
        "        if de_word == end_token:\n",
        "            break\n",
        "\n",
        "        de_translation.append(de_word)\n",
        "\n",
        "    print(f\"Translation: {' '.join(de_translation)}\\n\")\n",
        "\n",
        "# --- Test the inference model ---\n",
        "for i in range(5):\n",
        "    sample_en_text = test_df[\"EN\"].iloc[i]\n",
        "    generate_new_translation(en_model, de_model_inference_step, de_vocabulary, sample_en_text)\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading trained model and building inference models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 17 variables whereas the saved optimizer has 32 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference models built.\n",
            "Input: She pushed him out the door.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/tmp/ipython-input-2864226451.py\", line 114, in de_model_inference_step  *\n        pred_logits, new_state_raw = de_model_base(inputs=[input_token, prev_state], training=False)\n    File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling GRU.call().\n    \n    \u001b[1mInput tensor `prev_state:0` enters the loop with shape (1, 256), but has shape (None, 256) after one iteration. To allow the shape to vary across iterations, use the `shape_invariants` argument of tf.while_loop to specify a less-specific shape.\u001b[0m\n    \n    Arguments received by GRU.call():\n      • sequences=tf.Tensor(shape=(None, 1, 128), dtype=float32)\n      • initial_state=tf.Tensor(shape=(1, 256), dtype=float32)\n      • mask=None\n      • training=False\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4027529912.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0msample_en_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"EN\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0mgenerate_new_translation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mde_model_inference_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mde_vocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_en_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4027529912.py\u001b[0m in \u001b[0;36mgenerate_new_translation\u001b[0;34m(en_model, de_model_inference_step, de_vocabulary, sample_en_text, max_len)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# Predict the next word and get the new state using the tf.function wrapped step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# Pass inputs directly to the tf.function, not through .predict()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mde_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mde_model_inference_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mde_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;31m# Get the word ID with the highest probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_filennom6f_s.py\u001b[0m in \u001b[0;36mtf__de_model_inference_step\u001b[0;34m(input_token, prev_state)\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0mpred_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mde_model_base\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_handler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/tmp/ipython-input-2864226451.py\", line 114, in de_model_inference_step  *\n        pred_logits, new_state_raw = de_model_base(inputs=[input_token, prev_state], training=False)\n    File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling GRU.call().\n    \n    \u001b[1mInput tensor `prev_state:0` enters the loop with shape (1, 256), but has shape (None, 256) after one iteration. To allow the shape to vary across iterations, use the `shape_invariants` argument of tf.while_loop to specify a less-specific shape.\u001b[0m\n    \n    Arguments received by GRU.call():\n      • sequences=tf.Tensor(shape=(None, 1, 128), dtype=float32)\n      • initial_state=tf.Tensor(shape=(1, 256), dtype=float32)\n      • mask=None\n      • training=False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76eaada9"
      },
      "source": [
        "# Task\n",
        "The `ValueError` arises because the `d_gru_layer` expects a static batch size for its `sequences` input. To resolve this, I will add `d_emb_out = tf.ensure_shape(d_emb_out, (1, 1, d_emb_layer.output_dim))` before `d_emb_out` is passed to the `d_gru_layer` within the `get_inference_model` function in cell `7ff67b0f`. This will explicitly enforce the required static shape for the batch dimension. I will then re-execute the cell to update the inference model. Finally, I will re-run the translation test to ensure the fix is successful and no new errors are introduced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f91ca15"
      },
      "source": [
        "## ensure_static_embedding_output_shape\n",
        "\n",
        "### Subtask:\n",
        "Modify the `get_inference_model` function in cell `7ff67b0f` to add `d_emb_out = tf.ensure_shape(d_emb_out, (1, 1, d_emb_layer.output_dim))` before it is passed to the `d_gru_layer`. This will explicitly enforce a static batch size for the GRU's `sequences` input, which is currently reported as having a dynamic batch dimension by TensorFlow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb83002f"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires modifying the `get_inference_model` function in cell `7ff67b0f` to include `tf.ensure_shape` for `d_emb_out` to fix the dynamic batch dimension issue before passing it to the GRU layer. I will replicate the existing code and insert the specified line.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "5e41e461",
        "outputId": "43803e23-bf59-459f-bf97-42e5e8b660e8"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Based on Listing 11.11 - Create the inference models\n",
        "\n",
        "def get_inference_model(save_path, de_vocab_size):\n",
        "    print(\"Loading trained model and building inference models...\")\n",
        "    K.clear_session()\n",
        "    model = load_model(save_path)\n",
        "\n",
        "    # 1. Get the Encoder\n",
        "    # Need to reconstruct the encoder with a fixed batch_shape\n",
        "    # The original encoder's input was tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input')\n",
        "    # We need to recreate it to ensure a fixed batch size for inference.\n",
        "    # Get the layers from the original encoder in the loaded model\n",
        "    original_encoder = model.get_layer(\"encoder\")\n",
        "    # Use the globally available en_vectorizer, not from original_encoder\n",
        "    global en_vectorizer # Ensure en_vectorizer is accessible\n",
        "\n",
        "    # Clone encoder layers to ensure proper re-use in a new graph\n",
        "    encoder_embedding_config = original_encoder.get_layer('e_embedding').get_config()\n",
        "    encoder_embedding = layers.Embedding.from_config(encoder_embedding_config)\n",
        "    _ = encoder_embedding(tf.zeros((1, en_seq_length), dtype=tf.int32)) # Build layer\n",
        "    encoder_embedding.set_weights(original_encoder.get_layer('e_embedding').get_weights())\n",
        "\n",
        "    encoder_gru_config = original_encoder.get_layer('e_bidirectional_gru').get_config()\n",
        "    encoder_gru = layers.Bidirectional.from_config(encoder_gru_config)\n",
        "    # The Bidirectional GRU input shape is (batch_size, sequence_length, embedding_dim)\n",
        "    # where batch_size is 1, sequence_length is en_seq_length, embedding_dim is 128\n",
        "    _ = encoder_gru(tf.zeros((1, en_seq_length, 128))) # Build layer\n",
        "    encoder_gru.set_weights(original_encoder.get_layer('e_bidirectional_gru').get_weights())\n",
        "\n",
        "\n",
        "    # Define a new input for the encoder with fixed batch_shape\n",
        "    e_infer_input = tf.keras.Input(batch_shape=(1, 1), dtype=tf.string, name='e_infer_input')\n",
        "    e_vectorized_out = en_vectorizer(e_infer_input) # Output (1, en_seq_length)\n",
        "    e_emb_out = encoder_embedding(e_vectorized_out) # Output (1, en_seq_length, 128)\n",
        "    e_gru_out = encoder_gru(e_emb_out) # Output (1, 256)\n",
        "    en_model = tf.keras.models.Model(inputs=e_infer_input, outputs=e_gru_out, name='encoder_inference')\n",
        "\n",
        "    # 2. Build the Decoder\n",
        "    # We need to define new inputs for the decoder's state\n",
        "    d_inp = tf.keras.Input(batch_shape=(1, 1), dtype=tf.string, name='d_infer_input') # Decoder's current word input\n",
        "    d_state_inp = tf.keras.Input(batch_shape=(1, 256), name='d_infer_state') # Decoder's recurrent state input (from encoder or previous step)\n",
        "\n",
        "    # Recreate the TextVectorization layer for German text\n",
        "    global de_vocabulary, de_seq_length\n",
        "    recreated_de_vectorizer = TextVectorization(\n",
        "        max_tokens=len(de_vocabulary),\n",
        "        output_mode='int',\n",
        "        output_sequence_length=1, # Crucial: single token at a time for inference\n",
        "        name='recreated_de_vectorizer'\n",
        "    )\n",
        "    recreated_de_vectorizer.set_vocabulary(de_vocabulary)\n",
        "    _ = recreated_de_vectorizer(tf.zeros(shape=(1, 1), dtype=tf.string)) # Build with dummy input\n",
        "\n",
        "    # Rebuild Embedding layer explicitly\n",
        "    d_emb_layer_original = model.get_layer('d_embedding')\n",
        "    d_emb_layer = layers.Embedding(\n",
        "        input_dim=d_emb_layer_original.input_dim,\n",
        "        output_dim=d_emb_layer_original.output_dim,\n",
        "        mask_zero=d_emb_layer_original.mask_zero,\n",
        "        name='d_embedding_inference'\n",
        "    )\n",
        "    _ = d_emb_layer(tf.zeros((1, 1), dtype=tf.int32)) # Build layer with appropriate dummy input\n",
        "    d_emb_layer.set_weights(d_emb_layer_original.get_weights())\n",
        "\n",
        "\n",
        "    # Rebuild GRU layer explicitly with correct settings\n",
        "    d_gru_layer_original = model.get_layer(\"d_gru\")\n",
        "    d_gru_units = d_gru_layer_original.units\n",
        "    d_emb_output_dim = d_emb_layer.output_dim # This will be 128 (embedding_dim)\n",
        "    d_gru_layer = layers.GRU(\n",
        "        units=d_gru_units,\n",
        "        return_sequences=False, # Single output per step\n",
        "        return_state=True,     # Return the updated state\n",
        "        name='d_gru_inference' # Give it a new name\n",
        "    )\n",
        "    # Build the GRU layer with dummy input matching its expected sequence input and initial state\n",
        "    _ = d_gru_layer(tf.zeros((1, 1, d_emb_output_dim)), initial_state=tf.zeros((1, d_gru_units)))\n",
        "    d_gru_layer.set_weights(d_gru_layer_original.get_weights()) # Set weights after building\n",
        "\n",
        "    d_dense_layer_1_config = model.get_layer(\"d_dense_1\").get_config()\n",
        "    d_dense_layer_1 = layers.Dense.from_config(d_dense_layer_1_config)\n",
        "    _ = d_dense_layer_1(tf.zeros((1, 256))) # Build layer\n",
        "    d_dense_layer_1.set_weights(model.get_layer(\"d_dense_1\").get_weights())\n",
        "\n",
        "    d_final_layer_config = model.get_layer(\"d_dense_final\").get_config()\n",
        "    d_final_layer = layers.Dense.from_config(d_final_layer_config)\n",
        "    _ = d_final_layer(tf.zeros((1, 512))) # Build layer\n",
        "    d_final_layer.set_weights(model.get_layer(\"d_dense_final\").get_weights())\n",
        "\n",
        "    # Build the graph using the recreated vectorizer\n",
        "    d_vectorized_out = recreated_de_vectorizer(d_inp) # Output (1, 1)\n",
        "    d_emb_out = d_emb_layer(d_vectorized_out)         # Output (1, 1, 128)\n",
        "    d_emb_out = tf.ensure_shape(d_emb_out, (1, 1, d_emb_layer.output_dim)) # Enforce static shape\n",
        "\n",
        "    # GRU now returns output and state\n",
        "    d_gru_output, d_new_state = d_gru_layer(d_emb_out, initial_state=d_state_inp) # d_gru_output (1, 256), d_new_state (1, 256)\n",
        "\n",
        "    d_dense1_out = d_dense_layer_1(d_gru_output) # Use d_gru_output for dense layers, output (1, 512)\n",
        "    d_final_out = d_final_layer(d_dense1_out)   # Output (1, vocab_size+2)\n",
        "\n",
        "    de_model_base = tf.keras.models.Model(\n",
        "        inputs=[d_inp, d_state_inp],\n",
        "        outputs=[d_final_out, d_new_state] # Output prediction AND new state\n",
        "    )\n",
        "\n",
        "    # Wrap the decoder model in tf.function with explicit input_signature\n",
        "    @tf.function(input_signature=[\n",
        "        tf.TensorSpec(shape=(1, 1), dtype=tf.string), # Input for d_inp (current word token)\n",
        "        tf.TensorSpec(shape=(1, 256), dtype=tf.float32) # Input for d_state_inp (previous GRU state)\n",
        "    ])\n",
        "    def de_model_inference_step(input_token, prev_state):\n",
        "        pred_logits, new_state_raw = de_model_base(inputs=[input_token, prev_state], training=False)\n",
        "        # Explicitly ensure the shape of the output state\n",
        "        new_state = tf.ensure_shape(new_state_raw, (1, 256))\n",
        "        return pred_logits, new_state\n",
        "\n",
        "    return en_model, de_model_inference_step\n",
        "\n",
        "# Update model_path to use the new .keras extension\n",
        "model_path = os.path.join('models', 'seq2seq_ch11.keras')\n",
        "en_model, de_model_inference_step = get_inference_model(model_path, de_vocab_size=de_vocab)\n",
        "print(\"Inference models built.\")\n",
        "\n",
        "def generate_new_translation(en_model, de_model_inference_step, de_vocabulary, sample_en_text, max_len=20):\n",
        "    print(f\"Input: {sample_en_text}\")\n",
        "\n",
        "    # 1. Get the context vector from the encoder\n",
        "    d_state = en_model.predict(tf.constant([[sample_en_text]], dtype=tf.string), verbose=0)\n",
        "\n",
        "    # 2. Start the decoder with the 'sos' token\n",
        "    de_word = start_token\n",
        "    de_translation = []\n",
        "\n",
        "    # 3. Recursive loop\n",
        "    for _ in range(max_len):\n",
        "        # Predict the next word and get the new state using the tf.function wrapped step\n",
        "        # Pass inputs directly to the tf.function, not through .predict()\n",
        "        de_pred, d_state = de_model_inference_step(tf.constant([[de_word]], dtype=tf.string), d_state)\n",
        "\n",
        "        # Get the word ID with the highest probability\n",
        "        de_word_id = np.argmax(de_pred[0])\n",
        "\n",
        "        # Look up the word from the ID\n",
        "        de_word = de_vocabulary[de_word_id]\n",
        "\n",
        "        if de_word == end_token:\n",
        "            break\n",
        "\n",
        "        de_translation.append(de_word)\n",
        "\n",
        "    print(f\"Translation: {' '.join(de_translation)}\\n\")\n",
        "\n",
        "# --- Test the inference model ---\n",
        "for i in range(5):\n",
        "    sample_en_text = test_df[\"EN\"].iloc[i]\n",
        "    generate_new_translation(en_model, de_model_inference_step, de_vocabulary, sample_en_text)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading trained model and building inference models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 17 variables whereas the saved optimizer has 32 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.ops`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1043705063.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;31m# Update model_path to use the new .keras extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seq2seq_ch11.keras'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m \u001b[0men_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mde_model_inference_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_inference_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mde_vocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mde_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Inference models built.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1043705063.py\u001b[0m in \u001b[0;36mget_inference_model\u001b[0;34m(save_path, de_vocab_size)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0md_vectorized_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecreated_de_vectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_inp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Output (1, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0md_emb_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_emb_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_vectorized_out\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# Output (1, 1, 128)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0md_emb_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_emb_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_emb_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Enforce static shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;31m# GRU now returns output and state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/common/keras_tensor.py\u001b[0m in \u001b[0;36m__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__tf_tensor__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0;34m\"A KerasTensor cannot be used as input to a TensorFlow function. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;34m\"A KerasTensor is a symbolic placeholder for a shape and dtype, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.ops`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c4591ae"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to use `tf.ensure_shape` directly on a KerasTensor resulted in a `ValueError`. To fix this, I will wrap `tf.ensure_shape` in a `layers.Lambda` layer, which is the correct way to apply arbitrary TensorFlow functions within the Keras Functional API model construction. This ensures the output of the embedding layer has the expected static shape before being fed to the GRU.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82094c43",
        "outputId": "31484814-4d20-4d79-ef54-0f8c7b2a7ae1"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Based on Listing 11.11 - Create the inference models\n",
        "\n",
        "def get_inference_model(save_path, de_vocab_size):\n",
        "    print(\"Loading trained model and building inference models...\")\n",
        "    K.clear_session()\n",
        "    model = load_model(save_path)\n",
        "\n",
        "    # 1. Get the Encoder\n",
        "    # Need to reconstruct the encoder with a fixed batch_shape\n",
        "    # The original encoder's input was tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input')\n",
        "    # We need to recreate it to ensure a fixed batch size for inference.\n",
        "    # Get the layers from the original encoder in the loaded model\n",
        "    original_encoder = model.get_layer(\"encoder\")\n",
        "    # Use the globally available en_vectorizer, not from original_encoder\n",
        "    global en_vectorizer # Ensure en_vectorizer is accessible\n",
        "\n",
        "    # Clone encoder layers to ensure proper re-use in a new graph\n",
        "    encoder_embedding_config = original_encoder.get_layer('e_embedding').get_config()\n",
        "    encoder_embedding = layers.Embedding.from_config(encoder_embedding_config)\n",
        "    _ = encoder_embedding(tf.zeros((1, en_seq_length), dtype=tf.int32)) # Build layer\n",
        "    encoder_embedding.set_weights(original_encoder.get_layer('e_embedding').get_weights())\n",
        "\n",
        "    encoder_gru_config = original_encoder.get_layer('e_bidirectional_gru').get_config()\n",
        "    encoder_gru = layers.Bidirectional.from_config(encoder_gru_config)\n",
        "    # The Bidirectional GRU input shape is (batch_size, sequence_length, embedding_dim)\n",
        "    # where batch_size is 1, sequence_length is en_seq_length, embedding_dim is 128\n",
        "    _ = encoder_gru(tf.zeros((1, en_seq_length, 128))) # Build layer\n",
        "    encoder_gru.set_weights(original_encoder.get_layer('e_bidirectional_gru').get_weights())\n",
        "\n",
        "\n",
        "    # Define a new input for the encoder with fixed batch_shape\n",
        "    e_infer_input = tf.keras.Input(batch_shape=(1, 1), dtype=tf.string, name='e_infer_input')\n",
        "    e_vectorized_out = en_vectorizer(e_infer_input) # Output (1, en_seq_length)\n",
        "    e_emb_out = encoder_embedding(e_vectorized_out) # Output (1, en_seq_length, 128)\n",
        "    e_gru_out = encoder_gru(e_emb_out) # Output (1, 256)\n",
        "    en_model = tf.keras.models.Model(inputs=e_infer_input, outputs=e_gru_out, name='encoder_inference')\n",
        "\n",
        "    # 2. Build the Decoder\n",
        "    # We need to define new inputs for the decoder's state\n",
        "    d_inp = tf.keras.Input(batch_shape=(1, 1), dtype=tf.string, name='d_infer_input') # Decoder's current word input\n",
        "    d_state_inp = tf.keras.Input(batch_shape=(1, 256), name='d_infer_state') # Decoder's recurrent state input (from encoder or previous step)\n",
        "\n",
        "    # Recreate the TextVectorization layer for German text\n",
        "    global de_vocabulary, de_seq_length\n",
        "    recreated_de_vectorizer = TextVectorization(\n",
        "        max_tokens=len(de_vocabulary),\n",
        "        output_mode='int',\n",
        "        output_sequence_length=1, # Crucial: single token at a time for inference\n",
        "        name='recreated_de_vectorizer'\n",
        "    )\n",
        "    recreated_de_vectorizer.set_vocabulary(de_vocabulary)\n",
        "    _ = recreated_de_vectorizer(tf.zeros(shape=(1, 1), dtype=tf.string)) # Build with dummy input\n",
        "\n",
        "    # Rebuild Embedding layer explicitly\n",
        "    d_emb_layer_original = model.get_layer('d_embedding')\n",
        "    d_emb_layer = layers.Embedding(\n",
        "        input_dim=d_emb_layer_original.input_dim,\n",
        "        output_dim=d_emb_layer_original.output_dim,\n",
        "        mask_zero=d_emb_layer_original.mask_zero,\n",
        "        name='d_embedding_inference'\n",
        "    )\n",
        "    _ = d_emb_layer(tf.zeros((1, 1), dtype=tf.int32)) # Build layer with appropriate dummy input\n",
        "    d_emb_layer.set_weights(d_emb_layer_original.get_weights())\n",
        "\n",
        "\n",
        "    # Rebuild GRU layer explicitly with correct settings\n",
        "    d_gru_layer_original = model.get_layer(\"d_gru\")\n",
        "    d_gru_units = d_gru_layer_original.units\n",
        "    d_emb_output_dim = d_emb_layer.output_dim # This will be 128 (embedding_dim)\n",
        "    d_gru_layer = layers.GRU(\n",
        "        units=d_gru_units,\n",
        "        return_sequences=False, # Single output per step\n",
        "        return_state=True,     # Return the updated state\n",
        "        name='d_gru_inference' # Give it a new name\n",
        "    )\n",
        "    # Build the GRU layer with dummy input matching its expected sequence input and initial state\n",
        "    _ = d_gru_layer(tf.zeros((1, 1, d_emb_output_dim)), initial_state=tf.zeros((1, d_gru_units)))\n",
        "    d_gru_layer.set_weights(d_gru_layer_original.get_weights()) # Set weights after building\n",
        "\n",
        "    d_dense_layer_1_config = model.get_layer(\"d_dense_1\").get_config()\n",
        "    d_dense_layer_1 = layers.Dense.from_config(d_dense_layer_1_config)\n",
        "    _ = d_dense_layer_1(tf.zeros((1, 256))) # Build layer\n",
        "    d_dense_layer_1.set_weights(model.get_layer(\"d_dense_1\").get_weights())\n",
        "\n",
        "    d_final_layer_config = model.get_layer(\"d_dense_final\").get_config()\n",
        "    d_final_layer = layers.Dense.from_config(d_final_layer_config)\n",
        "    _ = d_final_layer(tf.zeros((1, 512))) # Build layer\n",
        "    d_final_layer.set_weights(model.get_layer(\"d_dense_final\").get_weights())\n",
        "\n",
        "    # Build the graph using the recreated vectorizer\n",
        "    d_vectorized_out = recreated_de_vectorizer(d_inp) # Output (1, 1)\n",
        "    d_emb_out = d_emb_layer(d_vectorized_out)         # Output (1, 1, 128)\n",
        "\n",
        "    # Wrap tf.ensure_shape in a Lambda layer to use it within the functional API\n",
        "    d_emb_out = layers.Lambda(lambda x: tf.ensure_shape(x, (1, 1, d_emb_layer.output_dim)),\n",
        "                              name='ensure_emb_shape')(d_emb_out)\n",
        "\n",
        "    # GRU now returns output and state\n",
        "    d_gru_output, d_new_state = d_gru_layer(d_emb_out, initial_state=d_state_inp) # d_gru_output (1, 256), d_new_state (1, 256)\n",
        "\n",
        "    d_dense1_out = d_dense_layer_1(d_gru_output) # Use d_gru_output for dense layers, output (1, 512)\n",
        "    d_final_out = d_final_layer(d_dense1_out)   # Output (1, vocab_size+2)\n",
        "\n",
        "    de_model_base = tf.keras.models.Model(\n",
        "        inputs=[d_inp, d_state_inp],\n",
        "        outputs=[d_final_out, d_new_state] # Output prediction AND new state\n",
        "    )\n",
        "\n",
        "    # Wrap the decoder model in tf.function with explicit input_signature\n",
        "    @tf.function(input_signature=[\n",
        "        tf.TensorSpec(shape=(1, 1), dtype=tf.string), # Input for d_inp (current word token)\n",
        "        tf.TensorSpec(shape=(1, 256), dtype=tf.float32) # Input for d_state_inp (previous GRU state)\n",
        "    ])\n",
        "    def de_model_inference_step(input_token, prev_state):\n",
        "        pred_logits, new_state_raw = de_model_base(inputs=[input_token, prev_state], training=False)\n",
        "        # Explicitly ensure the shape of the output state\n",
        "        new_state = tf.ensure_shape(new_state_raw, (1, 256))\n",
        "        return pred_logits, new_state\n",
        "\n",
        "    return en_model, de_model_inference_step\n",
        "\n",
        "# Update model_path to use the new .keras extension\n",
        "model_path = os.path.join('models', 'seq2seq_ch11.keras')\n",
        "en_model, de_model_inference_step = get_inference_model(model_path, de_vocab_size=de_vocab)\n",
        "print(\"Inference models built.\")\n",
        "\n",
        "def generate_new_translation(en_model, de_model_inference_step, de_vocabulary, sample_en_text, max_len=20):\n",
        "    print(f\"Input: {sample_en_text}\")\n",
        "\n",
        "    # 1. Get the context vector from the encoder\n",
        "    d_state = en_model.predict(tf.constant([[sample_en_text]], dtype=tf.string), verbose=0)\n",
        "\n",
        "    # 2. Start the decoder with the 'sos' token\n",
        "    de_word = start_token\n",
        "    de_translation = []\n",
        "\n",
        "    # 3. Recursive loop\n",
        "    for _ in range(max_len):\n",
        "        # Predict the next word and get the new state using the tf.function wrapped step\n",
        "        # Pass inputs directly to the tf.function, not through .predict()\n",
        "        de_pred, d_state = de_model_inference_step(tf.constant([[de_word]], dtype=tf.string), d_state)\n",
        "\n",
        "        # Get the word ID with the highest probability\n",
        "        de_word_id = np.argmax(de_pred[0])\n",
        "\n",
        "        # Look up the word from the ID\n",
        "        de_word = de_vocabulary[de_word_id]\n",
        "\n",
        "        if de_word == end_token:\n",
        "            break\n",
        "\n",
        "        de_translation.append(de_word)\n",
        "\n",
        "    print(f\"Translation: {' '.join(de_translation)}\\n\")\n",
        "\n",
        "# --- Test the inference model ---\n",
        "for i in range(5):\n",
        "    sample_en_text = test_df[\"EN\"].iloc[i]\n",
        "    generate_new_translation(en_model, de_model_inference_step, de_vocabulary, sample_en_text)\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading trained model and building inference models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 17 variables whereas the saved optimizer has 32 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'ensure_emb_shape' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference models built.\n",
            "Input: She pushed him out the door.\n",
            "Translation: sie [UNK] ihn die [UNK]\n",
            "\n",
            "Input: Tom doesn't use salt in his cooking.\n",
            "Translation: tom [UNK] sich nicht auf seine [UNK]\n",
            "\n",
            "Input: We're safe in here, aren't we?\n",
            "Translation: wir sind [UNK] das problem\n",
            "\n",
            "Input: Is anybody thirsty?\n",
            "Translation: ist jemand [UNK]\n",
            "\n",
            "Input: You should have your head examined.\n",
            "Translation: sie hätten ihre [UNK] [UNK]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "207ca2e2"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous modification successfully resolved the `ValueError` by wrapping `tf.ensure_shape` within a `layers.Lambda` layer, allowing the inference model to be built and executed without crashing. The generated translations confirm that the fix was successful.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3ffa1cf"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Based on Listing 11.11 - Create the inference models\n",
        "\n",
        "def get_inference_model(save_path, de_vocab_size):\n",
        "    print(\"Loading trained model and building inference models...\")\n",
        "    K.clear_session()\n",
        "    model = load_model(save_path)\n",
        "\n",
        "    # 1. Get the Encoder\n",
        "    # Need to reconstruct the encoder with a fixed batch_shape\n",
        "    # The original encoder's input was tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input')\n",
        "    # We need to recreate it to ensure a fixed batch size for inference.\n",
        "    # Get the layers from the original encoder in the loaded model\n",
        "    original_encoder = model.get_layer(\"encoder\")\n",
        "    # Use the globally available en_vectorizer, not from original_encoder\n",
        "    global en_vectorizer # Ensure en_vectorizer is accessible\n",
        "\n",
        "    # Clone encoder layers to ensure proper re-use in a new graph\n",
        "    encoder_embedding_config = original_encoder.get_layer('e_embedding').get_config()\n",
        "    encoder_embedding = layers.Embedding.from_config(encoder_embedding_config)\n",
        "    _ = encoder_embedding(tf.zeros((1, en_seq_length), dtype=tf.int32)) # Build layer\n",
        "    encoder_embedding.set_weights(original_encoder.get_layer('e_embedding').get_weights())\n",
        "\n",
        "    encoder_gru_config = original_encoder.get_layer('e_bidirectional_gru').get_config()\n",
        "    encoder_gru = layers.Bidirectional.from_config(encoder_gru_config)\n",
        "    # The Bidirectional GRU input shape is (batch_size, sequence_length, embedding_dim)\n",
        "    # where batch_size is 1, sequence_length is en_seq_length, embedding_dim is 128\n",
        "    _ = encoder_gru(tf.zeros((1, en_seq_length, 128))) # Build layer\n",
        "    encoder_gru.set_weights(original_encoder.get_layer('e_bidirectional_gru').get_weights())\n",
        "\n",
        "\n",
        "    # Define a new input for the encoder with fixed batch_shape\n",
        "    e_infer_input = tf.keras.Input(batch_shape=(1, 1), dtype=tf.string, name='e_infer_input')\n",
        "    e_vectorized_out = en_vectorizer(e_infer_input) # Output (1, en_seq_length)\n",
        "    e_emb_out = encoder_embedding(e_vectorized_out) # Output (1, en_seq_length, 128)\n",
        "    e_gru_out = encoder_gru(e_emb_out) # Output (1, 256)\n",
        "    en_model = tf.keras.models.Model(inputs=e_infer_input, outputs=e_gru_out, name='encoder_inference')\n",
        "\n",
        "    # 2. Build the Decoder\n",
        "    # We need to define new inputs for the decoder's state\n",
        "    d_inp = tf.keras.Input(batch_shape=(1, 1), dtype=tf.string, name='d_infer_input') # Decoder's current word input\n",
        "    d_state_inp = tf.keras.Input(batch_shape=(1, 256), name='d_infer_state') # Decoder's recurrent state input (from encoder or previous step)\n",
        "\n",
        "    # Recreate the TextVectorization layer for German text\n",
        "    global de_vocabulary, de_seq_length\n",
        "    recreated_de_vectorizer = TextVectorization(\n",
        "        max_tokens=len(de_vocabulary),\n",
        "        output_mode='int',\n",
        "        output_sequence_length=1, # Crucial: single token at a time for inference\n",
        "        name='recreated_de_vectorizer'\n",
        "    )\n",
        "    recreated_de_vectorizer.set_vocabulary(de_vocabulary)\n",
        "    _ = recreated_de_vectorizer(tf.zeros(shape=(1, 1), dtype=tf.string)) # Build with dummy input\n",
        "\n",
        "    # Rebuild Embedding layer explicitly\n",
        "    d_emb_layer_original = model.get_layer('d_embedding')\n",
        "    d_emb_layer = layers.Embedding(\n",
        "        input_dim=d_emb_layer_original.input_dim,\n",
        "        output_dim=d_emb_layer_original.output_dim,\n",
        "        mask_zero=d_emb_layer_original.mask_zero,\n",
        "        name='d_embedding_inference'\n",
        "    )\n",
        "    _ = d_emb_layer(tf.zeros((1, 1), dtype=tf.int32)) # Build layer with appropriate dummy input\n",
        "    d_emb_layer.set_weights(d_emb_layer_original.get_weights())\n",
        "\n",
        "\n",
        "    # Rebuild GRU layer explicitly with correct settings\n",
        "    d_gru_layer_original = model.get_layer(\"d_gru\")\n",
        "    d_gru_units = d_gru_layer_original.units\n",
        "    d_emb_output_dim = d_emb_layer.output_dim # This will be 128 (embedding_dim)\n",
        "    d_gru_layer = layers.GRU(\n",
        "        units=d_gru_units,\n",
        "        return_sequences=False, # Single output per step\n",
        "        return_state=True,     # Return the updated state\n",
        "        name='d_gru_inference' # Give it a new name\n",
        "    )\n",
        "    # Build the GRU layer with dummy input matching its expected sequence input and initial state\n",
        "    _ = d_gru_layer(tf.zeros((1, 1, d_emb_output_dim)), initial_state=tf.zeros((1, d_gru_units)))\n",
        "    d_gru_layer.set_weights(d_gru_layer_original.get_weights()) # Set weights after building\n",
        "\n",
        "    d_dense_layer_1_config = model.get_layer(\"d_dense_1\").get_config()\n",
        "    d_dense_layer_1 = layers.Dense.from_config(d_dense_layer_1_config)\n",
        "    _ = d_dense_layer_1(tf.zeros((1, 256))) # Build layer\n",
        "    d_dense_layer_1.set_weights(model.get_layer(\"d_dense_1\").get_weights())\n",
        "\n",
        "    d_final_layer_config = model.get_layer(\"d_dense_final\").get_config()\n",
        "    d_final_layer = layers.Dense.from_config(d_final_layer_config)\n",
        "    _ = d_final_layer(tf.zeros((1, 512))) # Build layer\n",
        "    d_final_layer.set_weights(model.get_layer(\"d_dense_final\").get_weights())\n",
        "\n",
        "    # Build the graph using the recreated vectorizer\n",
        "    d_vectorized_out = recreated_de_vectorizer(d_inp) # Output (1, 1)\n",
        "    d_emb_out = d_emb_layer(d_vectorized_out)         # Output (1, 1, 128)\n",
        "\n",
        "    # Wrap tf.ensure_shape in a Lambda layer to use it within the functional API\n",
        "    d_emb_out = layers.Lambda(lambda x: tf.ensure_shape(x, (1, 1, d_emb_layer.output_dim)),\n",
        "                              name='ensure_emb_shape')(d_emb_out)\n",
        "\n",
        "    # GRU now returns output and state\n",
        "    d_gru_output, d_new_state = d_gru_layer(d_emb_out, initial_state=d_state_inp) # d_gru_output (1, 256), d_new_state (1, 256)\n",
        "\n",
        "    d_dense1_out = d_dense_layer_1(d_gru_output) # Use d_gru_output for dense layers, output (1, 512)\n",
        "    d_final_out = d_final_layer(d_dense1_out)   # Output (1, vocab_size+2)\n",
        "\n",
        "    de_model_base = tf.keras.models.Model(\n",
        "        inputs=[d_inp, d_state_inp],\n",
        "        outputs=[d_final_out, d_new_state] # Output prediction AND new state\n",
        "    )\n",
        "\n",
        "    # Wrap the decoder model in tf.function with explicit input_signature\n",
        "    @tf.function(input_signature=[\n",
        "        tf.TensorSpec(shape=(1, 1), dtype=tf.string), # Input for d_inp (current word token)\n",
        "        tf.TensorSpec(shape=(1, 256), dtype=tf.float32) # Input for d_state_inp (previous GRU state)\n",
        "    ])\n",
        "    def de_model_inference_step(input_token, prev_state):\n",
        "        pred_logits, new_state_raw = de_model_base(inputs=[input_token, prev_state], training=False)\n",
        "        # Explicitly ensure the shape of the output state\n",
        "        new_state = tf.ensure_shape(new_state_raw, (1, 256))\n",
        "        return pred_logits, new_state\n",
        "\n",
        "    return en_model, de_model_inference_step\n",
        "\n",
        "# Update model_path to use the new .keras extension\n",
        "model_path = os.path.join('models', 'seq2seq_ch11.keras')\n",
        "en_model, de_model_inference_step = get_inference_model(model_path, de_vocab_size=de_vocab)\n",
        "print(\"Inference models built.\")\n",
        "\n",
        "def generate_new_translation(en_model, de_model_inference_step, de_vocabulary, sample_en_text, max_len=20):\n",
        "    print(f\"Input: {sample_en_text}\")\n",
        "\n",
        "    # 1. Get the context vector from the encoder\n",
        "    d_state = en_model.predict(tf.constant([[sample_en_text]], dtype=tf.string), verbose=0)\n",
        "\n",
        "    # 2. Start the decoder with the 'sos' token\n",
        "    de_word = start_token\n",
        "    de_translation = []\n",
        "\n",
        "    # 3. Recursive loop\n",
        "    for _ in range(max_len):\n",
        "        # Predict the next word and get the new state using the tf.function wrapped step\n",
        "        # Pass inputs directly to the tf.function, not through .predict()\n",
        "        de_pred, d_state = de_model_inference_step(tf.constant([[de_word]], dtype=tf.string), d_state)\n",
        "\n",
        "        # Get the word ID with the highest probability\n",
        "        de_word_id = np.argmax(de_pred[0])\n",
        "\n",
        "        # Look up the word from the ID\n",
        "        de_word = de_vocabulary[de_word_id]\n",
        "\n",
        "        if de_word == end_token:\n",
        "            break\n",
        "\n",
        "        de_translation.append(de_word)\n",
        "\n",
        "    print(f\"Translation: {' '.join(de_translation)}\\n\")\n",
        "\n",
        "# --- Test the inference model ---\n",
        "for i in range(5):\n",
        "    sample_en_text = test_df[\"EN\"].iloc[i]\n",
        "    generate_new_translation(en_model, de_model_inference_step, de_vocabulary, sample_en_text)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}