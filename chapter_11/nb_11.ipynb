{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Sequence-to-sequence learning: Part 1\n",
    "\n",
    "This notebook reproduces the code and summarizes the theoretical concepts from Chapter 11 of *'TensorFlow in Action'* by Thushan Ganegedara.\n",
    "\n",
    "This chapter introduces **sequence-to-sequence (seq2seq)** models, a powerful architecture for tasks that map an input sequence of one length to an output sequence of another length (e.g., machine translation).\n",
    "\n",
    "We will cover:\n",
    "1.  **Data Preparation**: Loading and processing a parallel English-to-German text corpus.\n",
    "2.  **The `TextVectorization` Layer**: Using this Keras layer to build an end-to-end model that accepts raw strings.\n",
    "3.  **Seq2seq Model Architecture**: Building an encoder-decoder model using GRUs (Gated Recurrent Units).\n",
    "4.  **Training (Teacher Forcing)**: How to train a seq2seq model using the \"teacher forcing\" technique.\n",
    "5.  **Inference Model**: Building a separate model for generating new translations recursively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 Understanding the machine translation data\n",
    "\n",
    "We will use an English-to-German parallel corpus from `manythings.org`. The data is a text file where each line contains an English sentence, a tab, and its German translation.\n",
    "\n",
    "**Note**: The book requires you to manually download the file `deu-eng.zip` from `http://www.manythings.org/anki/deu-eng.zip` and place it in a `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "random_seed = 4321\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# --- 1. Load and Extract Data ---\n",
    "data_dir = 'data'\n",
    "zip_path = os.path.join(data_dir, 'deu-eng.zip')\n",
    "extracted_path = os.path.join(data_dir, 'deu.txt')\n",
    "\n",
    "if not os.path.exists(extracted_path):\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(\"Error: Please download 'deu-eng.zip' from http://www.manythings.org/anki/ and place it in the 'data' folder.\")\n",
    "    else:\n",
    "        print(\"Extracting data...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(data_dir)\n",
    "        print(\"Extraction complete.\")\n",
    "else:\n",
    "    print(\"Data already extracted.\")\n",
    "\n",
    "# --- 2. Read data into pandas ---\n",
    "df = pd.read_csv(extracted_path, delimiter='\\t', header=None)\n",
    "df.columns = [\"EN\", \"DE\", \"Attribution\"]\n",
    "df = df[[\"EN\", \"DE\"]]\n",
    "\n",
    "# Clean up problematic unicode characters from the book's example\n",
    "clean_inds = [i for i in range(len(df)) if b\"\\xc2\" not in df.iloc[i][\"DE\"].encode(\"utf-8\")]\n",
    "df = df.iloc[clean_inds]\n",
    "\n",
    "# --- 3. Sample and Preprocess Data ---\n",
    "n_samples = 50000\n",
    "df = df.sample(n=n_samples, random_state=random_seed)\n",
    "\n",
    "# Add 'sos' (start of sentence) and 'eos' (end of sentence) tokens\n",
    "# These are crucial for the decoder during training and inference.\n",
    "start_token = 'sos'\n",
    "end_token = 'eos'\n",
    "df[\"DE\"] = start_token + ' ' + df[\"DE\"] + ' ' + end_token\n",
    "\n",
    "print(f\"Loaded and processed {len(df)} samples.\")\n",
    "print(\"\\nSample data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Create train/validation/test splits ---\n",
    "n_test = int(n_samples / 10)\n",
    "n_valid = int(n_samples / 10)\n",
    "\n",
    "test_df = df.sample(n=n_test, random_state=random_seed)\n",
    "valid_df = df.loc[~df.index.isin(test_df.index)].sample(n=n_valid, random_state=random_seed)\n",
    "train_df = df.loc[~(df.index.isin(test_df.index) | df.index.isin(valid_df.index))]\n",
    "\n",
    "print(f\"\\nTraining samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(valid_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "# --- 5. Analyze Vocabulary and Sequence Length ---\n",
    "# (Using helper function from Listing 11.1)\n",
    "def get_vocabulary_size_greater_than(words, n, verbose=True):\n",
    "    counter = Counter(words)\n",
    "    freq_df = pd.Series(list(counter.values()), index=list(counter.keys())).sort_values(ascending=False)\n",
    "    n_vocab = (freq_df >= n).sum()\n",
    "    if verbose: print(f\"Vocabulary size (>= {n} freq): {n_vocab}\")\n",
    "    return n_vocab\n",
    "\n",
    "en_words = train_df[\"EN\"].str.split().sum()\n",
    "de_words = train_df[\"DE\"].str.split().sum()\n",
    "\n",
    "en_vocab = get_vocabulary_size_greater_than(en_words, n=10)\n",
    "de_vocab = get_vocabulary_size_greater_than(de_words, n=10)\n",
    "\n",
    "# Get 99th percentile for sequence lengths\n",
    "en_seq_length = int(train_df[\"EN\"].str.split().str.len().quantile(0.99)) + 5\n",
    "de_seq_length = int(train_df[\"DE\"].str.split().str.len().quantile(0.99)) + 5\n",
    "\n",
    "print(f\"EN max sequence length (99th percentile + 5): {en_seq_length}\")\n",
    "print(f\"DE max sequence length (99th percentile + 5): {de_seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 Writing an English-German seq2seq machine translator\n",
    "\n",
    "A seq2seq model consists of two main parts:\n",
    "1.  **Encoder**: An RNN (we'll use a GRU) that reads the input English sentence one token at a time and compresses its meaning into a single vector, known as the **context vector** or \"thought vector\". This is the final hidden state of the encoder.\n",
    "2.  **Decoder**: Another RNN (also a GRU) that takes the encoder's context vector as its *initial hidden state*. It then generates the output German sentence one token at a time.\n",
    "\n",
    "### 11.2.1 The `TextVectorization` Layer\n",
    "\n",
    "Instead of preprocessing our text into integers *before* feeding it to the model, we can build the preprocessing *into* the model using the `TextVectorization` layer. This layer will:\n",
    "1.  Be `adapt`ed (fitted) on our training corpus to build a vocabulary.\n",
    "2.  When the model is running, it will take raw strings as input.\n",
    "3.  It will automatically tokenize, convert to integers, and pad the sequences to a fixed length, all inside the model graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Listing 11.3\n",
    "def get_vectorizer(corpus, n_vocab, max_length=None, return_vocabulary=True, name=None):\n",
    "    \"\"\"Creates a TextVectorization layer/model.\"\"\"\n",
    "    \n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name=f'{name}_input')\n",
    "    \n",
    "    # We add 2 to the vocab size for the <PAD> (ID 0) and [UNK] (ID 1) tokens\n",
    "    vectorize_layer = TextVectorization(\n",
    "        max_tokens=n_vocab + 2, \n",
    "        output_mode='int',\n",
    "        output_sequence_length=max_length,\n",
    "        name=name\n",
    "    )\n",
    "    \n",
    "    # Build the vocabulary\n",
    "    vectorize_layer.adapt(corpus)\n",
    "    vectorized_out = vectorize_layer(inp)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=vectorized_out)\n",
    "    \n",
    "    if return_vocabulary:\n",
    "        return model, vectorize_layer.get_vocabulary()\n",
    "    return model\n",
    "\n",
    "# Create the vectorizers for English and German\n",
    "# Note: The decoder's max_length is de_seq_length - 1\n",
    "# This is because we will feed it 'sos ... word_n' (length N) to predict 'word_1 ... eos' (length N)\n",
    "en_vectorizer, en_vocabulary = get_vectorizer(\n",
    "    corpus=np.array(train_df[\"EN\"].tolist()), \n",
    "    n_vocab=en_vocab, \n",
    "    max_length=en_seq_length, \n",
    "    name='en_vectorizer'\n",
    ")\n",
    "de_vectorizer, de_vocabulary = get_vectorizer(\n",
    "    corpus=np.array(train_df[\"DE\"].tolist()), \n",
    "    n_vocab=de_vocab,\n",
    "    max_length=de_seq_length - 1, \n",
    "    name='de_vectorizer'\n",
    ")\n",
    "\n",
    "print(f\"English Vocabulary size: {len(en_vocabulary)}\")\n",
    "print(f\"German Vocabulary size: {len(de_vocabulary)}\")\n",
    "\n",
    "# Test the English vectorizer\n",
    "print(\"\\nTest EN Vectorizer:\")\n",
    "print(en_vectorizer(np.array([[\"I like machine learning\"]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.3 & 11.2.4 Defining the Encoder and Decoder\n",
    "\n",
    "Now we build the full seq2seq model using the Keras Functional API.\n",
    "\n",
    "**Encoder (Listing 11.4):**\n",
    "1.  Input (Raw English strings)\n",
    "2.  `en_vectorizer` (Text -> Integer IDs)\n",
    "3.  `Embedding` Layer (IDs -> Dense Vectors)\n",
    "4.  `Bidirectional(GRU)`: Reads the sequence forwards and backwards. The final hidden state is the context vector.\n",
    "\n",
    "**Decoder (Listing 11.5):**\n",
    "1.  Input (Raw German strings, e.g., \"sos Ich möchte ein...\")\n",
    "2.  `de_vectorizer` (Text -> Integer IDs)\n",
    "3.  `Embedding` Layer (IDs -> Dense Vectors)\n",
    "4.  `GRU`: This GRU's **initial_state** is set to the **encoder's context vector**.\n",
    "5.  `Dense` Layer (with Softmax): Predicts the next word in the German vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "# --- Define Encoder --- \n",
    "def get_encoder(n_vocab, vectorizer):\n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input')\n",
    "    vectorized_out = vectorizer(inp)\n",
    "    emb_layer = layers.Embedding(\n",
    "        n_vocab + 2, 128, mask_zero=True, name='e_embedding'\n",
    "    )\n",
    "    emb_out = emb_layer(vectorized_out)\n",
    "    gru_layer = layers.Bidirectional(\n",
    "        layers.GRU(128, name='e_gru'), name='e_bidirectional_gru'\n",
    "    )\n",
    "    gru_out = gru_layer(emb_out)\n",
    "    encoder = tf.keras.models.Model(inputs=inp, outputs=gru_out, name='encoder')\n",
    "    return encoder\n",
    "\n",
    "# --- Define Final Seq2Seq Model ---\n",
    "def get_final_seq2seq_model(n_vocab, encoder, vectorizer):\n",
    "    e_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input_final')\n",
    "    d_init_state = encoder(e_inp)\n",
    "    \n",
    "    d_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_input')\n",
    "    d_vectorized_out = vectorizer(d_inp)\n",
    "    \n",
    "    d_emb_layer = layers.Embedding(\n",
    "        n_vocab + 2, 128, mask_zero=True, name='d_embedding'\n",
    "    )\n",
    "    d_emb_out = d_emb_layer(d_vectorized_out)\n",
    "    \n",
    "    d_gru_layer = layers.GRU(\n",
    "        256, return_sequences=True, name='d_gru' # 256 units = 128 (fwd) + 128 (bwd) from encoder\n",
    "    )\n",
    "    # The encoder's state is fed as the initial state to the decoder's GRU\n",
    "    d_gru_out = d_gru_layer(d_emb_out, initial_state=d_init_state)\n",
    "    \n",
    "    d_dense_layer_1 = layers.Dense(512, activation='relu', name='d_dense_1')\n",
    "    d_dense1_out = d_dense_layer_1(d_gru_out)\n",
    "    \n",
    "    d_final_layer = layers.Dense(n_vocab + 2, activation='softmax', name='d_dense_final')\n",
    "    d_final_out = d_final_layer(d_dense1_out)\n",
    "    \n",
    "    seq2seq = tf.keras.models.Model(\n",
    "        inputs=[e_inp, d_inp], outputs=d_final_out, name='final_seq2seq'\n",
    "    )\n",
    "    return seq2seq\n",
    "\n",
    "# Get the models\n",
    "encoder = get_encoder(n_vocab=en_vocab, vectorizer=en_vectorizer)\n",
    "final_model = get_final_seq2seq_model(n_vocab=de_vocab, encoder=encoder, vectorizer=de_vectorizer)\n",
    "\n",
    "# Compile the model\n",
    "final_model.compile(\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3 Training and evaluating the model\n",
    "\n",
    "To train this model, we use **teacher forcing**. \n",
    "\n",
    "This means for a translation pair `(\"I like cats\", \"sos Ich mag Katzen eos\")`:\n",
    "* `x` (inputs) = `(\"I like cats\", \"sos Ich mag Katzen\")`\n",
    "* `y` (target) = `(\"Ich\", \"mag\", \"Katzen\", \"eos\")`\n",
    "\n",
    "The decoder receives the *true* previous word (e.g., \"mag\") as input to help it predict the next word (e.g., \"Katzen\"). This stabilizes and speeds up training.\n",
    "\n",
    "We also need to define a custom training loop to correctly calculate the **BLEU score**, a standard metric for machine translation that measures the overlap of n-grams between the predicted and reference translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Listing 11.6 - Prepare data for teacher forcing\n",
    "def prepare_data(df):\n",
    "    en_inputs = np.array(df[\"EN\"].tolist())\n",
    "    # Decoder inputs = 'sos ... word_n'\n",
    "    de_inputs = np.array(df[\"DE\"].str.rsplit(n=1, expand=True).iloc[:, 0].tolist())\n",
    "    # Decoder labels = 'word_1 ... eos'\n",
    "    de_labels = np.array(df[\"DE\"].str.split(n=1, expand=True).iloc[:, 1].tolist())\n",
    "    \n",
    "    # The labels need to be vectorized *without* the 'sos' token, \n",
    "    # so we create a separate vectorizer for them.\n",
    "    de_label_vectorizer = get_vectorizer(\n",
    "        corpus=np.array(train_df[\"DE\"].tolist()), \n",
    "        n_vocab=de_vocab,\n",
    "        max_length=de_seq_length - 1, \n",
    "        return_vocabulary=False\n",
    "    )\n",
    "    \n",
    "    # Convert string labels to token IDs\n",
    "    de_labels_vec = de_label_vectorizer(de_labels)\n",
    "    return en_inputs, de_inputs, de_labels_vec\n",
    "\n",
    "en_train, de_train_in, de_train_out = prepare_data(train_df)\n",
    "en_valid, de_valid_in, de_valid_out = prepare_data(valid_df)\n",
    "\n",
    "print(\"Training data shapes:\")\n",
    "print(en_train.shape, de_train_in.shape, de_train_out.shape)\n",
    "\n",
    "# Train the model (simplified .fit() call from the book)\n",
    "# The book uses a custom training loop (Listing 11.10) to calculate BLEU.\n",
    "# For simplicity, we will use model.fit() here.\n",
    "\n",
    "print(\"\\nStarting model training (1 epoch for demo)...\")\n",
    "history = final_model.fit(\n",
    "    x=[en_train, de_train_in],\n",
    "    y=de_train_out,\n",
    "    validation_data=([en_valid, de_valid_in], de_valid_out),\n",
    "    epochs=1, # Book uses 5\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Save the model\n",
    "os.makedirs('models', exist_ok=True)\n",
    "model_path = os.path.join('models', 'seq2seq_ch11.h5')\n",
    "final_model.save(model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4 From training to inference: Defining the inference model\n",
    "\n",
    "We can't use the trained model directly for inference because it relies on **teacher forcing** (i.e., it expects the *true* German sentence as an input to the decoder).\n",
    "\n",
    "For inference, we must build a new model that generates text **recursively**:\n",
    "1.  Feed the English sentence to the **Encoder** to get the context vector.\n",
    "2.  Feed the context vector (as the initial state) and the `sos` token to the **Decoder**.\n",
    "3.  The Decoder predicts the first word (e.g., \"Ich\").\n",
    "4.  Feed the *new* state and the predicted word (\"Ich\") back into the Decoder.\n",
    "5.  The Decoder predicts the second word (e.g., \"möchte\").\n",
    "6.  Repeat this process until the Decoder predicts the `eos` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Listing 11.11 - Create the inference models\n",
    "\n",
    "def get_inference_model(save_path, de_vocab_size):\n",
    "    print(\"Loading trained model and building inference models...\")\n",
    "    K.clear_session()\n",
    "    model = load_model(save_path)\n",
    "    \n",
    "    # 1. Get the Encoder\n",
    "    en_model = model.get_layer(\"encoder\")\n",
    "    \n",
    "    # 2. Build the Decoder\n",
    "    # We need to define new inputs for the decoder's state\n",
    "    d_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_infer_input')\n",
    "    d_state_inp = tf.keras.Input(shape=(256,), name='d_infer_state') # 256 = GRU units\n",
    "    \n",
    "    # Get the layers from the trained model\n",
    "    d_vectorizer = model.get_layer('d_vectorizer')\n",
    "    d_emb_layer = model.get_layer('d_embedding')\n",
    "    d_gru_layer = model.get_layer(\"d_gru\")\n",
    "    d_gru_layer.return_sequences = False # Only need the last output\n",
    "    d_dense_layer_1 = model.get_layer(\"d_dense_1\")\n",
    "    d_final_layer = model.get_layer(\"d_dense_final\")\n",
    "    \n",
    "    # Build the graph\n",
    "    d_vectorized_out = d_vectorizer(d_inp)\n",
    "    d_emb_out = d_emb_layer(d_vectorized_out)\n",
    "    d_gru_out = d_gru_layer(d_emb_out, initial_state=d_state_inp)\n",
    "    d_dense1_out = d_dense_layer_1(d_gru_out)\n",
    "    d_final_out = d_final_layer(d_dense1_out)\n",
    "    \n",
    "    de_model = tf.keras.models.Model(\n",
    "        inputs=[d_inp, d_state_inp], \n",
    "        outputs=[d_final_out, d_gru_out] # Output prediction AND new state\n",
    "    )\n",
    "    return en_model, de_model\n",
    "\n",
    "en_model, de_model = get_inference_model(model_path, de_vocab_size=de_vocab)\n",
    "print(\"Inference models built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Listing 11.12 - Function to generate a translation\n",
    "\n",
    "def generate_new_translation(en_model, de_model, de_vocabulary, sample_en_text, max_len=20):\n",
    "    print(f\"Input: {sample_en_text}\")\n",
    "    \n",
    "    # 1. Get the context vector from the encoder\n",
    "    d_state = en_model.predict(np.array([sample_en_text]))\n",
    "    \n",
    "    # 2. Start the decoder with the 'sos' token\n",
    "    de_word = start_token\n",
    "    de_translation = []\n",
    "    \n",
    "    # 3. Recursive loop\n",
    "    for _ in range(max_len):\n",
    "        # Predict the next word and get the new state\n",
    "        de_pred, d_state = de_model.predict([np.array([de_word]), d_state])\n",
    "        \n",
    "        # Get the word ID with the highest probability\n",
    "        de_word_id = np.argmax(de_pred[0])\n",
    "        \n",
    "        # Look up the word from the ID\n",
    "        de_word = de_vocabulary[de_word_id]\n",
    "        \n",
    "        if de_word == end_token:\n",
    "            break\n",
    "        \n",
    "        de_translation.append(de_word)\n",
    "    \n",
    "    print(f\"Translation: {' '.join(de_translation)}\\n\")\n",
    "\n",
    "# --- Test the inference model ---\n",
    "for i in range(5):\n",
    "    sample_en_text = test_df[\"EN\"].iloc[i]\n",
    "    generate_new_translation(en_model, de_model, de_vocabulary, sample_en_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
