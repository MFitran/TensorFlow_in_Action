{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Teaching machines to see: Image classification with CNNs\n",
    "\n",
    "This notebook reproduces the code and summarizes the theoretical concepts from Chapter 6 of *'TensorFlow in Action'* by Thushan Ganegedara.\n",
    "\n",
    "This chapter covers the end-to-end process of building a sophisticated image classifier. The key steps include:\n",
    "1.  **Exploratory Data Analysis (EDA)**: Understanding our image dataset's structure, classes, and potential issues.\n",
    "2.  **Data Pipelines**: Using the `ImageDataGenerator` to efficiently load and augment images from directories.\n",
    "3.  **Advanced Model Implementation**: Building a complex, state-of-the-art CNN (Inception net v1) using the Keras Functional API.\n",
    "4.  **Training and Evaluation**: Training the model and evaluating its performance on a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Putting the data under the microscope: Exploratory Data Analysis (EDA)\n",
    "\n",
    "Before building any model, we must first understand our data. EDA helps us answer critical questions:\n",
    "* What classes are we working with?\n",
    "* Is the dataset balanced? (i.e., equal number of images per class)\n",
    "* What are the image properties (e.g., size, color channels)?\n",
    "\n",
    "We will be using the **tiny-imagenet-200** dataset, a smaller version of the famous ImageNet dataset. It contains 200 different classes of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a random seed for reproducibility\n",
    "random_seed = 4321\n",
    "\n",
    "# 1. Download and Extract the Dataset\n",
    "data_dir = 'data'\n",
    "zip_path = os.path.join(data_dir, 'tiny-imagenet-200.zip')\n",
    "extract_path = os.path.join(data_dir, 'tiny-imagenet-200')\n",
    "\n",
    "if not os.path.exists(extract_path):\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(\"Downloading tiny-imagenet-200.zip (238 MB)...\")\n",
    "        url = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
    "        r = requests.get(url)\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "        with open(zip_path, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        print(\"Download complete.\")\n",
    "    else:\n",
    "        print(\"Zip file already exists.\")\n",
    "    \n",
    "    print(\"Extracting data...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_dir)\n",
    "    print(\"Extraction complete.\")\n",
    "else:\n",
    "    print(\"Data already downloaded and extracted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 & 6.1.2 Understanding the Data Structure and Classes\n",
    "\n",
    "The dataset uses **WordNet IDs (wnids)** to label its classes. We need to map these IDs to human-readable names using the provided `words.txt` file. We will also count the number of training images for each class to check for balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join('data', 'tiny-imagenet-200')\n",
    "wnids_path = os.path.join(data_dir, 'wnids.txt')\n",
    "words_path = os.path.join(data_dir, 'words.txt')\n",
    "\n",
    "def get_tiny_imagenet_classes(wnids_path, words_path):\n",
    "    \"\"\"Reads wnids.txt and words.txt to create a mapping from class ID to class name.\"\"\"\n",
    "    # Read the list of 200 wnids used in the dataset\n",
    "    with open(wnids_path, 'r') as f:\n",
    "        wnids = [x.strip() for x in f]\n",
    "    \n",
    "    # Read the full mapping of all wnids to names\n",
    "    words = pd.read_csv(words_path, sep='\\t', index_col=0, header=None, names=['wnid', 'class'])\n",
    "    \n",
    "    # Filter the full mapping to only include the 200 classes we care about\n",
    "    words_200 = words.loc[wnids].reset_index()\n",
    "    return words_200\n",
    "\n",
    "labels_df = get_tiny_imagenet_classes(wnids_path, words_path)\n",
    "print(\"Class ID to Name Mapping (First 5):\")\n",
    "print(labels_df.head())\n",
    "\n",
    "def get_image_count(data_dir):\n",
    "    \"\"\"Counts the number of JPEG files in a given folder.\"\"\"\n",
    "    if not os.path.exists(data_dir):\n",
    "        return 0\n",
    "    return len([f for f in os.listdir(data_dir) if f.lower().endswith('jpeg')])\n",
    "\n",
    "# Apply the count function to each class's training images folder\n",
    "train_image_dir = os.path.join(data_dir, 'train')\n",
    "labels_df[\"n_train\"] = labels_df[\"wnid\"].apply(\n",
    "    lambda x: get_image_count(os.path.join(train_image_dir, x, 'images'))\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Image Count Statistics:\")\n",
    "print(labels_df[\"n_train\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The description shows `count    200.0`, `mean    500.0`, `std    0.0`. This is excellent: it confirms the training set is **perfectly balanced**, with exactly 500 images for each of the 200 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 Computing Simple Statistics on the Data Set\n",
    "\n",
    "Next, we check the dimensions of the images. CNNs require a fixed input size, so we need to know if our images are already uniform or if they will need resizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sizes = []\n",
    "# We only check the first 25 classes for speed\n",
    "for wnid in labels_df[\"wnid\"].iloc[:25]:\n",
    "    img_dir = os.path.join(train_image_dir, wnid, 'images')\n",
    "    for f in os.listdir(img_dir):\n",
    "        if f.endswith('JPEG'):\n",
    "            image_sizes.append(Image.open(os.path.join(img_dir, f)).size)\n",
    "\n",
    "img_df = pd.DataFrame.from_records(image_sizes)\n",
    "img_df.columns = [\"width\", \"height\"]\n",
    "\n",
    "print(\"Image Dimension Statistics:\")\n",
    "print(img_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistics confirm that all images in the dataset are **64x64**. This is also great, as it means we won't have to handle variable image sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Creating data pipelines using the Keras ImageDataGenerator\n",
    "\n",
    "Instead of loading all 100,000+ images into memory, we will use `ImageDataGenerator`. This Keras utility can read images from disk in batches, preprocess them (like normalizing), and augment them (like rotating/zooming) on the fly. This saves memory and helps reduce overfitting.\n",
    "\n",
    "The model we will build (Inception) has 3 outputs (1 main, 2 auxiliary). Therefore, our generator must be wrapped to output a tuple of `(x, (y, y, y))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from functools import partial\n",
    "\n",
    "batch_size = 128\n",
    "target_size = (56, 56) # We use 56x56 for the Inception model\n",
    "\n",
    "# 1. Define the generator for Training and Validation\n",
    "# We split 10% of the training data off for validation\n",
    "image_gen = ImageDataGenerator(\n",
    "    samplewise_center=True, # Normalizes by subtracting the image's mean pixel value\n",
    "    validation_split=0.1\n",
    ")\n",
    "\n",
    "# 2. Use 'partial' to create a base generator function with common args\n",
    "partial_flow_func = partial(\n",
    "    image_gen.flow_from_directory,\n",
    "    directory=train_image_dir,\n",
    "    target_size=target_size,\n",
    "    classes=None, # Infers classes from subdirectory names\n",
    "    class_mode='categorical', # Returns one-hot encoded labels\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=random_seed\n",
    ")\n",
    "\n",
    "# 3. Create the training and validation generators\n",
    "train_gen = partial_flow_func(subset='training')\n",
    "valid_gen = partial_flow_func(subset='validation')\n",
    "\n",
    "# 4. Define the generator for Test data (from the 'val' folder)\n",
    "val_dir = os.path.join(data_dir, 'val')\n",
    "val_ann_path = os.path.join(val_dir, 'val_annotations.txt')\n",
    "\n",
    "def get_test_labels_df(test_labels_path):\n",
    "    test_df = pd.read_csv(test_labels_path, sep='\\t', index_col=None, header=None)\n",
    "    test_df = test_df.iloc[:, [0, 1]].rename({0: \"filename\", 1: \"class\"}, axis=1)\n",
    "    return test_df\n",
    "\n",
    "test_df = get_test_labels_df(val_ann_path)\n",
    "\n",
    "# Use a separate generator for test data (no validation split)\n",
    "image_gen_test = ImageDataGenerator(samplewise_center=True)\n",
    "\n",
    "test_gen = image_gen_test.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    directory=os.path.join(val_dir, 'images'),\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=target_size,\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False # No need to shuffle test data\n",
    ")\n",
    "\n",
    "# 5. Define the auxiliary wrapper for 3 outputs\n",
    "def data_gen_aux(gen):\n",
    "    for x, y in gen:\n",
    "        yield x, (y, y, y) # Return the label 3 times\n",
    "\n",
    "train_gen_aux = data_gen_aux(train_gen)\n",
    "valid_gen_aux = data_gen_aux(valid_gen)\n",
    "test_gen_aux = data_gen_aux(test_gen)\n",
    "\n",
    "print(f\"Created {len(train_gen)} training batches.\")\n",
    "print(f\"Created {len(valid_gen)} validation batches.\")\n",
    "print(f\"Created {len(test_gen)} test batches.\")\n",
    "\n",
    "# Check output shape\n",
    "x_sample, (y_sample1, y_sample2, y_sample3) = next(train_gen_aux)\n",
    "print(f\"Sample X shape: {x_sample.shape}\")\n",
    "print(f\"Sample Y1 shape: {y_sample1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Inception net: Implementing a state-of-the-art image classifier\n",
    "\n",
    "Now we build the **Inception net v1 (GoogLeNet)** model. This model's architecture is more complex than a simple sequential CNN. We will use the Keras Functional API to build it.\n",
    "\n",
    "**Key Components:**\n",
    "1.  **Stem**: The first few layers of standard convolution and pooling to reduce the initial dimensions.\n",
    "2.  **Inception Block**: The core idea. This block runs multiple convolutions (1x1, 3x3, 5x5) and a pooling operation in parallel and concatenates their outputs. This allows the network to capture features at multiple scales simultaneously.\n",
    "3.  **1x1 Convolutions**: Used *before* the 3x3 and 5x5 convolutions as a dimensionality reduction \"bottleneck\" to reduce the number of parameters and computations.\n",
    "4.  **Auxiliary Outputs**: Two extra \"mini-classifiers\" added to intermediate layers. During training, their loss is added to the main loss. This helps combat the vanishing gradient problem in very deep networks and provides extra regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Model\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Define the Stem (initial layers)\n",
    "def stem(inp):\n",
    "    conv1 = layers.Conv2D(64, (7,7), strides=(1,1), activation='relu', padding='same')(inp) \n",
    "    maxpool2 = layers.MaxPool2D((3,3), strides=(2,2), padding='same')(conv1) \n",
    "    # Local Response Normalization (LRN) - less common now, often replaced by BatchNormalization\n",
    "    lrn3 = layers.Lambda(lambda x: tf.nn.local_response_normalization(x), name='lrn3')(maxpool2) \n",
    "    conv4 = layers.Conv2D(64, (1,1), strides=(1,1), padding='same', activation='relu')(lrn3) \n",
    "    conv5 = layers.Conv2D(192, (3,3), strides=(1,1), activation='relu', padding='same')(conv4) \n",
    "    lrn6 = layers.Lambda(lambda x: tf.nn.local_response_normalization(x), name='lrn6')(conv5) \n",
    "    maxpool7 = layers.MaxPool2D((3,3), strides=(1,1), padding='same')(lrn6) \n",
    "    return maxpool7\n",
    "\n",
    "# Define the Inception Block\n",
    "def inception(inp, n_filters):\n",
    "    # n_filters is a list: [(1x1), (1x1_reduce, 3x3), (1x1_reduce, 5x5), (pool_proj)]\n",
    "    \n",
    "    # Branch 1: 1x1 convolution\n",
    "    out1 = layers.Conv2D(n_filters[0][0], (1,1), strides=(1,1), activation='relu', padding='same')(inp)\n",
    "    \n",
    "    # Branch 2: 1x1 conv -> 3x3 conv\n",
    "    out2_1 = layers.Conv2D(n_filters[1][0], (1,1), strides=(1,1), activation='relu', padding='same')(inp)\n",
    "    out2_2 = layers.Conv2D(n_filters[1][1], (3,3), strides=(1,1), activation='relu', padding='same')(out2_1)\n",
    "    \n",
    "    # Branch 3: 1x1 conv -> 5x5 conv\n",
    "    out3_1 = layers.Conv2D(n_filters[2][0], (1,1), strides=(1,1), activation='relu', padding='same')(inp)\n",
    "    out3_2 = layers.Conv2D(n_filters[2][1], (5,5), strides=(1,1), activation='relu', padding='same')(out3_1)\n",
    "    \n",
    "    # Branch 4: 3x3 pool -> 1x1 conv\n",
    "    out4_1 = layers.MaxPool2D((3,3), strides=(1,1), padding='same')(inp)\n",
    "    out4_2 = layers.Conv2D(n_filters[3][0], (1,1), strides=(1,1), activation='relu', padding='same')(out4_1)\n",
    "    \n",
    "    # Concatenate all branches along the channel axis\n",
    "    out = layers.Concatenate(axis=-1)([out1, out2_2, out3_2, out4_2])\n",
    "    return out\n",
    "\n",
    "# Define the Auxiliary Output classifier\n",
    "def aux_out(inp, name=None):\n",
    "    avgpool1 = layers.AvgPool2D((5,5), strides=(3,3), padding='valid')(inp) \n",
    "    conv1 = layers.Conv2D(128, (1,1), activation='relu', padding='same')(avgpool1) \n",
    "    flat = layers.Flatten()(conv1) \n",
    "    dense1 = layers.Dense(1024, activation='relu')(flat) \n",
    "    aux_out = layers.Dense(200, activation='softmax', name=name)(dense1) # 200 classes\n",
    "    return aux_out\n",
    "\n",
    "# Define the full Inception v1 model\n",
    "def inception_v1(input_shape=(56, 56, 3), num_classes=200):\n",
    "    K.clear_session()\n",
    "    \n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Stem\n",
    "    stem_out = stem(inp)\n",
    "    \n",
    "    # Inception blocks\n",
    "    inc_3a = inception(stem_out, [(64,),(96,128),(16,32),(32,)])\n",
    "    inc_3b = inception(inc_3a, [(128,),(128,192),(32,96),(64,)])\n",
    "    \n",
    "    maxpool_3 = layers.MaxPool2D((3,3), strides=(2,2), padding='same')(inc_3b)\n",
    "    \n",
    "    inc_4a = inception(maxpool_3, [(192,),(96,208),(16,48),(64,)])\n",
    "    inc_4b = inception(inc_4a, [(160,),(112,224),(24,64),(64,)])\n",
    "    inc_4c = inception(inc_4b, [(128,),(128,256),(24,64),(64,)])\n",
    "    inc_4d = inception(inc_4c, [(112,),(144,288),(32,64),(64,)])\n",
    "    inc_4e = inception(inc_4d, [(256,),(160,320),(32,128),(128,)])\n",
    "    \n",
    "    maxpool_4 = layers.MaxPool2D((3,3), strides=(2,2), padding='same')(inc_4e)\n",
    "    \n",
    "    inc_5a = inception(maxpool_4, [(256,),(160,320),(32,128),(128,)])\n",
    "    inc_5b = inception(inc_5a, [(384,),(192,384),(48,128),(128,)])\n",
    "    \n",
    "    # --- Classifiers ---\n",
    "    \n",
    "    # Auxiliary Output 1 (from 4a)\n",
    "    aux_output_1 = aux_out(inc_4a, name='aux1')\n",
    "    \n",
    "    # Auxiliary Output 2 (from 4d)\n",
    "    aux_output_2 = aux_out(inc_4d, name='aux2')\n",
    "    \n",
    "    # Main Output (from 5b)\n",
    "    avgpool_final = layers.AvgPool2D((7,7), strides=(1,1), padding='valid')(inc_5b)\n",
    "    flat_out = layers.Flatten()(avgpool_final) \n",
    "    main_output = layers.Dense(num_classes, activation='softmax', name='final')(flat_out) \n",
    "    \n",
    "    # Create the model\n",
    "    model = models.Model(inputs=inp, outputs=[main_output, aux_output_1, aux_output_2])\n",
    "    \n",
    "    # Compile the model\n",
    "    # We provide a loss for each of the 3 outputs. We can weigh them differently.\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  loss_weights={'final': 1.0, 'aux1': 0.3, 'aux2': 0.3}, # As per the paper\n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = inception_v1()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Training the model and evaluating performance\n",
    "\n",
    "With the data generators and the complex model defined, we can now start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.models import load_model\n",
    "import time\n",
    "\n",
    "def get_steps_per_epoch(n_data, batch_size):\n",
    "    if n_data % batch_size == 0:\n",
    "        return int(n_data / batch_size)\n",
    "    else:\n",
    "        return int(n_data * 1.0 / batch_size) + 1\n",
    "\n",
    "# Create directories for logs and models\n",
    "os.makedirs('eval', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "csv_logger = CSVLogger(os.path.join('eval', '1_eval_base.log'))\n",
    "\n",
    "n_train = len(train_gen.filenames)\n",
    "n_valid = len(valid_gen.filenames)\n",
    "n_test = len(test_gen.filenames)\n",
    "\n",
    "train_steps = get_steps_per_epoch(n_train, batch_size)\n",
    "valid_steps = get_steps_per_epoch(n_valid, batch_size)\n",
    "test_steps = get_steps_per_epoch(n_test, batch_size)\n",
    "\n",
    "# The book runs for 50 epochs. We will run for 5 to make it executable.\n",
    "epochs_to_run = 5 \n",
    "print(f\"Starting training for {epochs_to_run} epochs (Book uses 50)...\")\n",
    "\n",
    "history = model.fit(\n",
    "    x=train_gen_aux,\n",
    "    validation_data=valid_gen_aux,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_steps=valid_steps,\n",
    "    epochs=epochs_to_run, \n",
    "    callbacks=[csv_logger]\n",
    ")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Save the trained model\n",
    "model_path = os.path.join('models', 'inception_v1_base.h5')\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model back and evaluate on the test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "loaded_model = load_model(model_path)\n",
    "\n",
    "# Note: The output names (e.g., 'final_accuracy') match the layer names we set.\n",
    "test_res = loaded_model.evaluate(test_gen_aux, steps=test_steps)\n",
    "test_res_dict = dict(zip(loaded_model.metrics_names, test_res))\n",
    "\n",
    "print(\"Test Results:\")\n",
    "print(test_res_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Book Observation)*: The model in the book achieves high training accuracy (~94%) but low validation/test accuracy (~30%). This is a classic sign of **overfitting**. Chapter 7 will explore techniques like data augmentation, dropout, and using a better-suited architecture (Minception) to combat this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
