{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Dipping Toes in Deep Learning\n",
    "\n",
    "This notebook reproduces the code and summarizes the theoretical concepts from Chapter 4 of *'TensorFlow in Action'* by Thushan Ganegedara.\n",
    "\n",
    "This chapter provides the first hands-on implementations of the three most common types of deep neural networks:\n",
    "1.  **Fully Connected Networks (FCNs)**: Using an autoencoder to restore corrupted images.\n",
    "2.  **Convolutional Neural Networks (CNNs)**: To classify images from the CIFAR-10 dataset.\n",
    "3.  **Recurrent Neural Networks (RNNs)**: To forecast a time-series of CO2 data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Fully Connected Networks (FCNs)\n",
    "\n",
    "A Fully Connected Network (FCN), or Multilayer Perceptron (MLP), is a network where every node in one layer is connected to every node in the next layer. \n",
    "\n",
    "For this example, we'll build an **autoencoder**, which is a type of FCN often used for unsupervised learning. An autoencoder has two parts:\n",
    "1.  **Encoder**: Compresses the input data into a smaller, latent representation.\n",
    "2.  **Decoder**: Reconstructs the original data from the compressed representation.\n",
    "\n",
    "Our goal is to train a **denoising autoencoder** to restore corrupted images of handwritten digits from the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Understanding the Data (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "\n",
    "print(f\"Original x_train shape: {x_train.shape}\")\n",
    "\n",
    "# Preprocess the data:\n",
    "# 1. Normalize pixel values from [0, 255] to [-1, 1] to match the tanh activation\n",
    "# 2. Reshape from (60000, 28, 28) to (60000, 784) to feed into a Dense layer\n",
    "norm_x_train = ((x_train - 128.0) / 128.0).reshape([-1, 784])\n",
    "norm_x_test = ((x_test - 128.0) / 128.0).reshape([-1, 784])\n",
    "\n",
    "print(f\"Normalized and reshaped x_train shape: {norm_x_train.shape}\")\n",
    "\n",
    "# Define a function to create 'corrupted' inputs\n",
    "def generate_masked_inputs(x, p, seed=None):\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    # Create a random binary mask (1s and 0s) and multiply it with the input\n",
    "    mask = np.random.binomial(n=1, p=p, size=x.shape).astype('float32')\n",
    "    return x * mask\n",
    "\n",
    "# Create the corrupted (masked) training and test data\n",
    "masked_x_train = generate_masked_inputs(norm_x_train, 0.5, seed=42)\n",
    "masked_x_test = generate_masked_inputs(norm_x_test, 0.5, seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Autoencoder Model\n",
    "\n",
    "We will build the autoencoder with the Keras Sequential API. The architecture is symmetrical:\n",
    "* **Encoder:** 784 -> 64 -> 32\n",
    "* **Decoder:** 32 -> 64 -> 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "K_ = tf.keras.backend\n",
    "K_.clear_session()\n",
    "\n",
    "# Define the autoencoder model\n",
    "autoencoder = models.Sequential([\n",
    "    # Encoder part\n",
    "    layers.Dense(64, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    \n",
    "    # Decoder part\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(784, activation='tanh') # Tanh activation for [-1, 1] output\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "# We use 'mse' (mean squared error) loss to measure how different\n",
    "# the reconstructed image is from the original.\n",
    "autoencoder.compile(loss='mse', optimizer='adam')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# Note: The input (x) is the corrupted image, \n",
    "# and the target (y) is the original, clean image\n",
    "print(\"Training autoencoder...\")\n",
    "history = autoencoder.fit(masked_x_train, \n",
    "                          norm_x_train, \n",
    "                          batch_size=64, \n",
    "                          epochs=10, \n",
    "                          validation_split=0.1)\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the corrupted test images\n",
    "y_pred = autoencoder.predict(masked_x_test)\n",
    "\n",
    "# Visualize the results (Original, Corrupted, Restored)\n",
    "n = 10  # Number of digits to display\n",
    "plt.figure(figsize=(20, 6))\n",
    "for i in range(n):\n",
    "    # Display original (clean)\n",
    "    ax = plt.subplot(3, n, i + 1)\n",
    "    plt.imshow(norm_x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if i == 0: ax.set_title(\"Original\")\n",
    "\n",
    "    # Display corrupted (input)\n",
    "    ax = plt.subplot(3, n, i + 1 + n)\n",
    "    plt.imshow(masked_x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if i == 0: ax.set_title(\"Corrupted\")\n",
    "\n",
    "    # Display reconstruction (predicted)\n",
    "    ax = plt.subplot(3, n, i + 1 + 2 * n)\n",
    "    plt.imshow(y_pred[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if i == 0: ax.set_title(\"Reconstructed\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Convolutional Neural Networks (CNNs)\n",
    "\n",
    "CNNs are specialized networks for processing grid-like data, such as images. Unlike FCNs, they use **convolution** and **pooling** layers. This allows them to:\n",
    "* **Preserve spatial information:** They look at 2D patches, not just flattened vectors.\n",
    "* **Be parameter efficient:** A single filter (kernel) is shared across the entire image, learning to detect the same feature (e.g., an edge) everywhere.\n",
    "\n",
    "Our task is to build a CNN to classify images from the **CIFAR-10 dataset**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Understanding the Data (CIFAR-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load the CIFAR-10 dataset using tfds\n",
    "data = tfds.load('cifar10')\n",
    "\n",
    "# Define a formatting function\n",
    "def format_data(x, depth=10):\n",
    "    # Cast image to float32 and normalize to [0, 1]\n",
    "    image = tf.cast(x[\"image\"], 'float32') / 255.0\n",
    "    # One-hot encode the label\n",
    "    label = tf.one_hot(x[\"label\"], depth=depth)\n",
    "    return image, label\n",
    "\n",
    "# Create the tf.data pipeline\n",
    "batch_size = 32\n",
    "tr_data = data[\"train\"].map(lambda x: format_data(x, depth=10)).batch(batch_size)\n",
    "ts_data = data[\"test\"].map(lambda x: format_data(x, depth=10)).batch(batch_size)\n",
    "\n",
    "# Inspect a batch\n",
    "for images, labels in tr_data.take(1):\n",
    "    print(f\"Batch of images shape: {images.shape}\")\n",
    "    print(f\"Batch of labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Implementing the Network\n",
    "\n",
    "This CNN architecture (based on Listing 4.3) interleaves `Conv2D` and `MaxPool2D` layers to progressively reduce the spatial dimensions (height, width) while increasing the feature depth (channels). The `Flatten` layer converts the final 3D feature map into a 1D vector for the final `Dense` classification layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_.clear_session()\n",
    "\n",
    "# Define the CNN model\n",
    "cnn = models.Sequential([\n",
    "    # Input shape is (32, 32, 3)\n",
    "    layers.Conv2D(filters=16, kernel_size=(3,3), strides=(2,2), activation='relu', \n",
    "                  padding='same', input_shape=(32,32,3)),\n",
    "    layers.MaxPool2D(pool_size=(2,2), strides=(2,2), padding='same'),\n",
    "    \n",
    "    layers.Conv2D(32, (3,3), activation='relu', padding='same'),\n",
    "    layers.MaxPool2D(pool_size=(2,2), strides=(2,2), padding='same'),\n",
    "    \n",
    "    # Flatten the 3D output to 1D for the Dense layers\n",
    "    layers.Flatten(),\n",
    "    \n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax') # 10 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CNN\n",
    "print(\"Training CNN...\")\n",
    "history_cnn = cnn.fit(tr_data, epochs=25, validation_data=ts_data)\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Recurrent Neural Networks (RNNs)\n",
    "\n",
    "RNNs are designed for sequential data, like time series or text. Unlike FCNs or CNNs, RNNs have a **\"memory\"** (or hidden state) that is passed from one time step to the next. This allows the network's prediction at the current time step to be influenced by all previous time steps.\n",
    "\n",
    "Our task is to predict future CO2 concentration levels based on past data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Understanding the Data (CO2 Time Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Define the data download function\n",
    "def download_data():\n",
    "    save_dir = \"data\"\n",
    "    save_path = os.path.join(save_dir, 'co2-mm-gl.csv')\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    if not os.path.exists(save_path):\n",
    "        print(\"Downloading CO2 data...\")\n",
    "        url = \"https://datahub.io/core/co2-ppm/r/co2-mm-gl.csv\"\n",
    "        r = requests.get(url)\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        print(\"Download complete.\")\n",
    "    else:\n",
    "        print(\"CO2 data already exists.\")\n",
    "    return save_path\n",
    "\n",
    "save_path = download_data()\n",
    "\n",
    "# Load data with pandas\n",
    "data = pd.read_csv(save_path)\n",
    "data = data.set_index('Date')\n",
    "\n",
    "# The data has an upward trend, which is hard for a model to learn.\n",
    "# We make it 'stationary' by calculating the difference from the previous month.\n",
    "data[\"Average Diff\"] = data[\"Average\"] - data[\"Average\"].shift(1)\n",
    "# Fill the first NaN value\n",
    "data = data.fillna(method='bfill')\n",
    "\n",
    "print(\"Data with 'Average Diff':\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original vs. differenced data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(data[\"Average\"])\n",
    "plt.title(\"Original CO2 Concentration (Non-Stationary)\")\n",
    "plt.ylabel(\"Average\")\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(data[\"Average Diff\"])\n",
    "plt.title(\"Differenced CO2 Concentration (Stationary)\")\n",
    "plt.ylabel(\"Average Diff\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates sequences of data.\n",
    "# x = 12 past values, y = 13th value\n",
    "# We use the corrected version from Listing 4.5\n",
    "def generate_data(co2_arr, n_seq):\n",
    "    x, y = [], []\n",
    "    for i in range(co2_arr.shape[0] - n_seq):\n",
    "        x.append(co2_arr[i:i + n_seq - 1])\n",
    "        y.append(co2_arr[i + n_seq - 1:i + n_seq])\n",
    "    \n",
    "    # Reshape x to [batch, timesteps, features] for the RNN layer\n",
    "    x = np.array(x).reshape(-1, n_seq - 1, 1)\n",
    "    y = np.array(y)\n",
    "    return x, y\n",
    "\n",
    "# We'll use 12 months to predict the 13th (n_seq = 13)\n",
    "n_seq = 13\n",
    "x_rnn, y_rnn = generate_data(data[\"Average Diff\"].values, n_seq=n_seq)\n",
    "\n",
    "print(f\"Generated x shape: {x_rnn.shape}\")\n",
    "print(f\"Generated y shape: {y_rnn.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Implementing the Model\n",
    "\n",
    "We use a `SimpleRNN` layer. This layer requires its input to be 3D: `(batch_size, timesteps, features)`. This is why we reshaped our data to `(429, 12, 1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_.clear_session()\n",
    "\n",
    "# Define the RNN model\n",
    "rnn = models.Sequential([\n",
    "    # input_shape is (timesteps, features)\n",
    "    layers.SimpleRNN(64, input_shape=(n_seq - 1, 1)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1) # Output a single value (the next 'Average Diff')\n",
    "])\n",
    "\n",
    "# Compile for regression using Mean Squared Error (MSE)\n",
    "rnn.compile(loss='mse', optimizer='adam')\n",
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the RNN\n",
    "print(\"Training RNN...\")\n",
    "history_rnn = rnn.fit(x_rnn, y_rnn, shuffle=True, batch_size=64, epochs=25)\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 Predicting Future CO2 Values\n",
    "\n",
    "To predict the future, we use the model recursively. We take the last 12 known `Average Diff` values, predict the next value, add that prediction to our history, and repeat the process for 60 months (5 years)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last 12 diffs from the original data as the starting point\n",
    "history = data[\"Average Diff\"].values[-12:].reshape(1, -1, 1)\n",
    "\n",
    "# Get the very last *actual* CO2 value to build our predictions upon\n",
    "prev_true = data[\"Average\"].values[-1]\n",
    "\n",
    "true_vals = [] # List to hold the predicted *absolute* CO2 values\n",
    "\n",
    "print(\"Predicting future values...\")\n",
    "# Predict 60 steps (5 years) into the future\n",
    "for i in range(60):\n",
    "    # Predict the next diff (p_diff)\n",
    "    p_diff = rnn.predict(history).reshape(1, -1, 1)\n",
    "    \n",
    "    # Update the history: drop the oldest value, append the newest prediction\n",
    "    history = np.concatenate((history[:, 1:, :], p_diff), axis=1)\n",
    "    \n",
    "    # Calculate the absolute CO2 value by adding the predicted diff to the last true value\n",
    "    predicted_val = prev_true + p_diff[0, 0, 0]\n",
    "    true_vals.append(predicted_val)\n",
    "    \n",
    "    # Update prev_true for the next loop\n",
    "    prev_true = predicted_val\n",
    "\n",
    "print(\"Prediction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original data and the future predictions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(data[\"Average\"].values, label=\"Current Trend\")\n",
    "\n",
    "# Create an index for the predicted values, starting from the end of the original data\n",
    "predict_index = range(len(data), len(data) + 60)\n",
    "plt.plot(predict_index, true_vals, label=\"Predicted Trend\", linestyle='--')\n",
    "\n",
    "plt.title(\"CO2 Concentration Forecast\")\n",
    "plt.ylabel(\"CO2 Concentration\")\n",
    "plt.xlabel(\"Months since start\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
