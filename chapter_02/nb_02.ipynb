{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: TensorFlow 2\n",
    "\n",
    "This notebook reproduces the code and summarizes the theoretical concepts from Chapter 2 of *'TensorFlow in Action'* by Thushan Ganegedara. \n",
    "\n",
    "This chapter covers the fundamentals of TensorFlow 2, its basic data structures (`tf.Variable`, `tf.Tensor`), core operations (`tf.matmul`, `tf.nn.convolution`), and how it differs from TensorFlow 1 by using eager execution by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 First steps with TensorFlow 2\n",
    "\n",
    "This section implements a simple Multilayer Perceptron (MLP), also known as a fully connected network. An MLP consists of an input layer, one or more hidden layers, and an output layer. Each layer's output is computed using the equation $h = \\sigma(xW + b)$, where $x$ is the input, $W$ and $b$ are the layer's weights and biases, and $\\sigma$ is a non-linear activation function (like sigmoid or ReLU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1. Define input and variables\n",
    "# Input x is a NumPy array\n",
    "x = np.random.normal(size=[1, 4]).astype('float32')\n",
    "\n",
    "# Use an initializer for weights\n",
    "init = tf.keras.initializers.RandomNormal()\n",
    "\n",
    "# Define model parameters as tf.Variable objects\n",
    "w1 = tf.Variable(init(shape=[4, 3]))\n",
    "b1 = tf.Variable(init(shape=[1, 3]))\n",
    "w2 = tf.Variable(init(shape=[3, 2]))\n",
    "b2 = tf.Variable(init(shape=[1, 2]))\n",
    "\n",
    "# 2. Define the forward pass function\n",
    "# The @tf.function decorator compiles the Python function into a high-performance TensorFlow graph.\n",
    "@tf.function\n",
    "def forward(x, W, b, act):\n",
    "    # tf.matmul performs matrix multiplication\n",
    "    return act(tf.matmul(x, W) + b)\n",
    "\n",
    "# 3. Execute the model's forward pass\n",
    "# Compute the hidden layer output 'h' using sigmoid activation\n",
    "h = forward(x, w1, b1, tf.nn.sigmoid)\n",
    "\n",
    "# Compute the final output 'y' using softmax activation\n",
    "# Softmax normalizes the output into a probability distribution\n",
    "y = forward(h, w2, b2, tf.nn.softmax)\n",
    "\n",
    "print(\"Input:\")\n",
    "print(x)\n",
    "print(\"\\nHidden Layer Output:\")\n",
    "print(h)\n",
    "print(\"\\nFinal Output:\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 How does TensorFlow operate under the hood?\n",
    "\n",
    "TensorFlow 2 uses **imperative style execution** (also called **eager execution**) by default. This means operations are computed immediately, just like standard Python code, making it easy to debug and iterate.\n",
    "\n",
    "However, for performance, TensorFlow can convert Python functions into **data-flow graphs** using the **`@tf.function` decorator** (a feature called **AutoGraph**). This graph represents the computations as nodes and the tensors flowing between them as edges. TensorFlow can then optimize this graph and run it efficiently on hardware like GPUs or TPUs.\n",
    "\n",
    "This contrasts with TensorFlow 1, which used **declarative graph-based execution**. In TF1, you first had to explicitly define the entire graph and then separately execute it within a `Session`.\n",
    "\n",
    "#### Key Differences: TensorFlow 1 vs. TensorFlow 2\n",
    "\n",
    "| TensorFlow 1 | TensorFlow 2 |\n",
    "| :--- | :--- |\n",
    "| Uses declarative graph execution (Define then Run) | Uses eager execution by default (Define by Run) |\n",
    "| Requires explicit `Session.run()` calls to execute code | Operations run immediately, like NumPy |\n",
    "| Must explicitly define the data-flow graph | AutoGraph (`@tf.function`) automatically creates graphs from Python code |\n",
    "| Debugging is difficult as errors occur inside the graph | Easy to debug using standard Python tools (like `print()` or `pdb`) |\n",
    "| Code can be non-intuitive and split into graph definition and execution | Code is more readable and follows a standard Python structure |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 TensorFlow building blocks\n",
    "\n",
    "There are three major basic elements in TensorFlow 2:\n",
    "1.  **`tf.Variable`**: Holds mutable (changeable) state, like a model's weights. These are the parameters that get updated during training.\n",
    "2.  **`tf.Tensor`**: Represents an immutable (unchangeable) $n$-dimensional array. It's the primary data structure, and it's what flows between operations in the graph.\n",
    "3.  **`tf.Operation`**: A node in the graph that performs a computation (e.g., `tf.matmul`, `tf.add`). It takes `tf.Tensor` objects as input and produces `tf.Tensor` objects as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Understanding `tf.Variable`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tf.Variable objects\n",
    "\n",
    "# From a tf.constant\n",
    "v1 = tf.Variable(tf.constant(2.0, shape=[4]), dtype='float32')\n",
    "print(\"v1:\", v1)\n",
    "\n",
    "# From a NumPy array\n",
    "v2 = tf.Variable(np.ones(shape=[4, 3]), dtype='float32')\n",
    "print(\"\\nv2:\", v2)\n",
    "\n",
    "# Using a Keras initializer\n",
    "v3 = tf.Variable(tf.keras.initializers.RandomNormal()(shape=[3, 4, 5]), dtype='float32')\n",
    "print(\"\\nv3 shape:\", v3.shape)\n",
    "\n",
    "# Converting a Variable to a NumPy array\n",
    "arr = v1.numpy()\n",
    "print(\"\\nv1 as numpy:\", arr)\n",
    "\n",
    "# Modifying a Variable using .assign()\n",
    "v = tf.Variable(np.zeros(shape=[4, 3]), dtype='float32')\n",
    "print(\"\\nOriginal v:\\n\", v.numpy())\n",
    "\n",
    "# Assign a single element\n",
    "v[0, 2].assign(1.0)\n",
    "print(\"\\nAfter v[0, 2].assign(1.0):\\n\", v.numpy())\n",
    "\n",
    "# Assign a slice\n",
    "v[2:, :2].assign(tf.constant([[3.0, 3.0], [3.0, 3.0]]))\n",
    "print(\"\\nAfter slice assignment:\\n\", v.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Understanding `tf.Tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.Tensor objects are the immutable outputs of operations\n",
    "\n",
    "# 'b' is a tf.Tensor (specifically, an EagerTensor)\n",
    "b = v1 * 3.0\n",
    "print(f\"Type of 'b': {type(b).__name__}\")\n",
    "print(f\"Is 'b' a tf.Tensor? {isinstance(b, tf.Tensor)}\")\n",
    "\n",
    "# 'c' is also a tf.Tensor\n",
    "a = tf.constant(2, shape=[4], dtype='float32')\n",
    "c = tf.add(a, b)\n",
    "print(f\"\\n'c' (a + b): {c.numpy()}\")\n",
    "\n",
    "# Trying to modify a tf.Tensor will raise an error\n",
    "try:\n",
    "    c[0].assign(2.0)\n",
    "except AttributeError as e:\n",
    "    print(f\"\\nError when trying to modify a tensor: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Understanding `tf.Operation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic arithmetic operations\n",
    "a = tf.constant(4, shape=[4], dtype='float32')\n",
    "b = tf.constant(2, shape=[4], dtype='float32')\n",
    "print(f\"a + b = {(a + b).numpy()}\")\n",
    "print(f\"a * b = {(a * b).numpy()}\")\n",
    "\n",
    "# Logical comparisons\n",
    "a = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "b = tf.constant([[5, 4, 3], [3, 2, 1]])\n",
    "print(f\"\\na == b:\\n {a == b}\")\n",
    "print(f\"\\na <= b:\\n {a <= b}\")\n",
    "\n",
    "# Reduction operations\n",
    "a = tf.constant(np.random.normal(size=[5, 4, 3]), dtype='float32')\n",
    "\n",
    "# Sum of all elements\n",
    "red_all = tf.reduce_sum(a)\n",
    "print(f\"\\nSum of all elements: {red_all.numpy()}\")\n",
    "\n",
    "# Product along axis 0\n",
    "red_a2 = tf.reduce_prod(a, axis=0)\n",
    "print(f\"\\nProduct along axis 0 shape: {red_a2.shape}\")\n",
    "\n",
    "# Minimum over axes 0 and 1\n",
    "red_a3 = tf.reduce_min(a, axis=[0, 1])\n",
    "print(f\"\\nMinimum over axes 0 and 1 shape: {red_a3.shape}\")\n",
    "\n",
    "# Demonstrating keepdims\n",
    "red_keepdims_false = tf.reduce_min(a, axis=1, keepdims=False)\n",
    "print(f\"\\nkeepdims=False shape: {red_keepdims_false.shape}\")\n",
    "\n",
    "red_keepdims_true = tf.reduce_min(a, axis=1, keepdims=True)\n",
    "print(f\"keepdims=True shape: {red_keepdims_true.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Neural network-related computations in TensorFlow\n",
    "\n",
    "This section demonstrates core neural network operations using a computer vision example (image manipulation). We'll use the famous 'baboon' image, which requires downloading it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the baboon image for the examples\n",
    "import os\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/en/7/7d/Mandrill_Image.png\"\n",
    "image_path = \"baboon.jpg\"\n",
    "\n",
    "if not os.path.exists(image_path):\n",
    "    print(\"Downloading image...\")\n",
    "    r = requests.get(image_url)\n",
    "    with open(image_path, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    print(\"Image downloaded.\")\n",
    "else:\n",
    "    print(\"Image already exists.\")\n",
    "\n",
    "# Resize image to 512x512 as used in the book's example\n",
    "img = Image.open(image_path)\n",
    "img = img.resize((512, 512))\n",
    "img.save(image_path) # Save the resized image\n",
    "\n",
    "print(f\"Image resized to {img.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Matrix Multiplication (RGB to Grayscale)\n",
    "\n",
    "We can convert an RGB image (shape `[H, W, 3]`) to grayscale (shape `[H, W, 1]`) by performing a matrix multiplication with a weights vector `[0.3], [0.59], [0.11]`. `tf.squeeze` is used to remove the last dimension of size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the RGB image as a NumPy array\n",
    "x_rgb = np.array(Image.open(image_path)).astype('float32')\n",
    "\n",
    "# Convert NumPy array to tf.Tensor\n",
    "x_rgb_tf = tf.constant(x_rgb)\n",
    "\n",
    "# Define the RGB-to-grayscale conversion weights\n",
    "grays = tf.constant([[0.3], [0.59], [0.11]], dtype='float32')\n",
    "\n",
    "# Perform matrix multiplication\n",
    "x_gray = tf.matmul(x_rgb_tf, grays)\n",
    "\n",
    "# Remove the last dimension (of size 1)\n",
    "x_gray_squeezed = tf.squeeze(x_gray)\n",
    "\n",
    "print(f\"Original shape: {x_rgb_tf.shape}\")\n",
    "print(f\"Grayscale shape before squeeze: {x_gray.shape}\")\n",
    "print(f\"Grayscale shape after squeeze: {x_gray_squeezed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Convolution Operation (Edge Detection)\n",
    "\n",
    "The convolution operation (`tf.nn.convolution`) is fundamental to CNNs. It slides a small window (filter or kernel) over the data, performing element-wise multiplications and summing the results. We can use it for effects like edge detection.\n",
    "\n",
    "Note: `tf.nn.convolution` expects 4D tensors: `[batch, height, width, channels]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the squeezed grayscale image from the previous step\n",
    "y = tf.constant(x_gray_squeezed)\n",
    "\n",
    "# Define an edge detection filter (Laplacian filter)\n",
    "filter_val = np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]]).astype('float32')\n",
    "edge_filter = tf.Variable(filter_val)\n",
    "\n",
    "# Reshape input and filter to 4D for tf.nn.convolution\n",
    "# Input shape: [1, 512, 512, 1]\n",
    "y_reshaped = tf.reshape(y, [1, 512, 512, 1])\n",
    "\n",
    "# Filter shape: [3, 3, 1, 1]\n",
    "filter_reshaped = tf.reshape(edge_filter, [3, 3, 1, 1])\n",
    "\n",
    "# Perform convolution\n",
    "y_conv = tf.nn.convolution(y_reshaped, filter_reshaped)\n",
    "\n",
    "print(f\"Input shape: {y_reshaped.shape}\")\n",
    "print(f\"Filter shape: {filter_reshaped.shape}\")\n",
    "print(f\"Convolution output shape: {y_conv.shape}\") # Output is 510x510 due to 'valid' padding by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Pooling Operation (Resizing)\n",
    "\n",
    "Pooling (`tf.nn.max_pool` or `tf.nn.avg_pool`) is used to downsample or resize feature maps, making the network more translation-invariant. It slides a window and takes either the maximum or average value from that window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the convolution output from the previous step\n",
    "# Note: The book uses y_conv, which is 510x510. We'll use that.\n",
    "\n",
    "# ksize: The size of the pooling window [batch, height, width, channels]\n",
    "# strides: How much to move the window [batch, height, width, channels]\n",
    "ksize = [1, 2, 2, 1]\n",
    "strides = [1, 2, 2, 1]\n",
    "\n",
    "# Perform average pooling\n",
    "z_avg = tf.nn.avg_pool(y_conv, ksize=ksize, strides=strides, padding='VALID')\n",
    "\n",
    "# Perform max pooling\n",
    "z_max = tf.nn.max_pool(y_conv, ksize=ksize, strides=strides, padding='VALID')\n",
    "\n",
    "print(f\"Input shape: {y_conv.shape}\")\n",
    "print(f\"Avg Pool output shape: {z_avg.shape}\")\n",
    "print(f\"Max Pool output shape: {z_max.shape}\")\n",
    "\n",
    "# Squeeze to visualize\n",
    "z_avg_squeezed = tf.squeeze(z_avg)\n",
    "z_max_squeezed = tf.squeeze(z_max)\n",
    "print(f\"\\nSqueezed Max Pool shape: {z_max_squeezed.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
