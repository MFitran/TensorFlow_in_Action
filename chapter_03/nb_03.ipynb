{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: TensorFlow 2\n",
    "\n",
    "This notebook reproduces the code and summarizes the theoretical concepts from Chapter 2 of *'TensorFlow in Action'* by Thushan Ganegedara. \n",
    "\n",
    [cite_start]"This chapter covers the fundamentals of TensorFlow 2, its basic data structures (`tf.Variable`, `tf.Tensor`), core operations (`tf.matmul`, `tf.nn.convolution`), and how it differs from TensorFlow 1 by using eager execution by default [cite: 298-300]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 First steps with TensorFlow 2\n",
    "\n",
    [cite_start]"This section implements a simple **Multilayer Perceptron (MLP)**, also known as a fully connected network[cite: 307]. [cite_start]An MLP consists of an input layer, one or more hidden layers, and an output layer[cite: 307]. [cite_start]Each layer's output is computed using the equation $h = \\sigma(xW + b)$, where $x$ is the input, $W$ and $b$ are the layer's weights and biases, and $\\sigma$ is a non-linear activation function (like sigmoid or ReLU) [cite: 324-325]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1. Define input and variables\n",
    [cite_start]"# Input x is a NumPy array [cite: 339]\n",
    "x = np.random.normal(size=[1, 4]).astype('float32')\n",
    "\n",
    [cite_start]"# Use an initializer for weights [cite: 339]\n",
    "init = tf.keras.initializers.RandomNormal()\n",
    "\n",
    [cite_start]"# Define model parameters as tf.Variable objects [cite: 340-342]\n",
    "w1 = tf.Variable(init(shape=[4, 3]))\n",
    "b1 = tf.Variable(init(shape=[1, 3]))\n",
    "w2 = tf.Variable(init(shape=[3, 2]))\n",
    "b2 = tf.Variable(init(shape=[1, 2]))\n",
    "\n",
    "# 2. Define the forward pass function\n",
    [cite_start]"# The @tf.function decorator compiles the Python function into a high-performance TensorFlow graph. [cite: 345, 363]\n",
    "@tf.function\n",
    "def forward(x, W, b, act):\n",
    [cite_start]"    # tf.matmul performs matrix multiplication [cite: 346]\n",
    "    return act(tf.matmul(x, W) + b)\n",
    "\n",
    "# 3. Execute the model's forward pass\n",
    [cite_start]"# Compute the hidden layer output 'h' using sigmoid activation [cite: 364-365]\n",
    "h = forward(x, w1, b1, tf.nn.sigmoid)\n",
    "\n",
    [cite_start]"# Compute the final output 'y' using softmax activation [cite: 368]\n",
    [cite_start]"# Softmax normalizes the output into a probability distribution [cite: 334]\n",
    "y = forward(h, w2, b2, tf.nn.softmax)\n",
    "\n",
    "print(\"Input:\")\n",
    "print(x)\n",
    "print(\"\\nHidden Layer Output:\")\n",
    "print(h)\n",
    "print(\"\\nFinal Output:\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 How does TensorFlow operate under the hood?\n",
    "\n",
    [cite_start]"TensorFlow 2 uses **imperative style execution** (also called **eager execution**) by default[cite: 388, 427]. This means operations are computed immediately, just like standard Python code, making it easy to debug and iterate.\n",
    "\n",
    [cite_start]"However, for performance, TensorFlow can convert Python functions into **data-flow graphs** using the **`@tf.function` decorator** (a feature called **AutoGraph**)[cite: 398, 422]. [cite_start]This graph represents the computations as nodes (operations) and the tensors flowing between them as edges [cite: 388-389]. [cite_start]TensorFlow can then optimize this graph and run it efficiently on hardware like GPUs or TPUs[cite: 398].\n",
    "\n",
    [cite_start]"This contrasts with TensorFlow 1, which used **declarative graph-based execution**[cite: 425]. [cite_start]In TF1, you first had to explicitly define the entire graph and then separately execute it within a `Session` [cite: 425-426].\n",
    "\n",
    [cite_start]"#### Key Differences: TensorFlow 1 vs. TensorFlow 2 [cite: 427, 430]\n",
    "\n",
    "| TensorFlow 1 | TensorFlow 2 |\n",
    "| :--- | :--- |\n",
    "| Uses declarative graph execution (Define then Run) | Uses eager execution by default (Define by Run) |\n",
    "| Requires explicit `Session.run()` calls to execute code | Operations run immediately, like NumPy |\n",
    "| Must explicitly define the data-flow graph | AutoGraph (`@tf.function`) automatically creates graphs from Python code |\n",
    "| Debugging is difficult as errors occur inside the graph | Easy to debug using standard Python tools (like `print()`) |\n",
    "| Code can be non-intuitive and split into graph definition and execution | Code is more readable and follows a standard Python structure |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 TensorFlow building blocks\n",
    "\n",
    [cite_start]"There are three major basic elements in TensorFlow 2 [cite: 439-441]:\n",
    "1.  [cite_start]**`tf.Variable`**: Holds mutable (changeable) state, like a model's weights[cite: 445]. These are the parameters that get updated during training.\n",
    "2.  [cite_start]**`tf.Tensor`**: Represents an immutable (unchangeable) $n$-dimensional array[cite: 490, 519]. It's the primary data structure, and it's what flows between operations in the graph.\n",
    "3.  [cite_start]**`tf.Operation`**: A node in the graph that performs a computation (e.g., `tf.matmul`, `tf.add`)[cite: 530]. It takes `tf.Tensor` objects as input and produces `tf.Tensor` objects as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Understanding `tf.Variable`\n",
    "\n",
    [cite_start]"A `tf.Variable` is used to represent parameters that change over time, such as the weights and biases in a neural network[cite: 445]. [cite_start]It must be initialized with a value and can have its value changed using operations like `.assign()`[cite: 447, 481]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tf.Variable objects\n",
    "\n",
    [cite_start]"# From a tf.constant [cite: 455]\n",
    "v1 = tf.Variable(tf.constant(2.0, shape=[4]), dtype='float32')\n",
    "print(\"v1:\", v1)\n",
    "\n",
    [cite_start]"# From a NumPy array [cite: 457]\n",
    "v2 = tf.Variable(np.ones(shape=[4, 3]), dtype='float32')\n",
    "print(\"\\nv2:\", v2)\n",
    "\n",
    [cite_start]"# Using a Keras initializer [cite: 463]\n",
    "v3 = tf.Variable(tf.keras.initializers.RandomNormal()(shape=[3, 4, 5]), dtype='float32')\n",
    "print(\"\\nv3 shape:\", v3.shape)\n",
    "\n",
    [cite_start]"# Converting a Variable to a NumPy array [cite: 475]\n",
    "arr = v1.numpy()\n",
    "print(\"\\nv1 as numpy:\", arr)\n",
    "\n",
    [cite_start]"# Modifying a Variable using .assign() [cite: 479-481]\n",
    "v = tf.Variable(np.zeros(shape=[4, 3]), dtype='float32')\n",
    "print(\"\\nOriginal v:\\n\", v.numpy())\n",
    "\n",
    [cite_start]"# Assign a single element [cite: 481]\n",
    "v[0, 2].assign(1.0)\n",
    "print(\"\\nAfter v[0, 2].assign(1.0):\\n\", v.numpy())\n",
    "\n",
    [cite_start]"# Assign a slice [cite: 485]\n",
    "v[2:, :2].assign(tf.constant([[3.0, 3.0], [3.0, 3.0]]))\n",
    "print(\"\\nAfter slice assignment:\\n\", v.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Understanding `tf.Tensor`\n",
    "\n",
    "A `tf.Tensor` is the primary data structure in TensorFlow. [cite_start]Unlike a `tf.Variable`, a `tf.Tensor` is **immutable**, meaning its value cannot be changed after creation[cite: 519]. [cite_start]It represents the output of any `tf.Operation`[cite: 490]. [cite_start]In eager execution mode, the output of an operation is an `EagerTensor`, which is a concrete value[cite: 511]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.Tensor objects are the immutable outputs of operations\n",
    "\n",
    [cite_start]"# 'b' is a tf.Tensor (specifically, an EagerTensor) [cite: 509-510]\n",
    "b = v1 * 3.0\n",
    "print(f\"Type of 'b': {type(b).__name__}\")\n",
    "print(f\"Is 'b' a tf.Tensor? {isinstance(b, tf.Tensor)}\")\n",
    "\n",
    [cite_start]"# 'c' is also a tf.Tensor [cite: 514-517]\n",
    "a = tf.constant(2, shape=[4], dtype='float32')\n",
    "c = tf.add(a, b)\n",
    "print(f\"\\n'c' (a + b): {c.numpy()}\")\n",
    "\n",
    [cite_start]"# Trying to modify a tf.Tensor will raise an error [cite: 520-524]\n",
    "try:\n",
    "    c[0].assign(2.0)\n",
    "except AttributeError as e:\n",
    "    print(f\"\\nError when trying to modify a tensor: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Understanding `tf.Operation`\n",
    "\n",
    [cite_start]"Operations are the nodes of the TensorFlow graph that perform computations[cite: 530]. This includes basic arithmetic, logical comparisons, and more complex operations like matrix multiplication (`tf.matmul`) or reductions (`tf.reduce_sum`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    [cite_start]"# Basic arithmetic operations [cite: 532-542]\n",
    "a = tf.constant(4, shape=[4], dtype='float32')\n",
    "b = tf.constant(2, shape=[4], dtype='float32')\n",
    "print(f\"a + b = {(a + b).numpy()}\")\n",
    "print(f\"a * b = {(a * b).numpy()}\")\n",
    "\n",
    [cite_start]"# Logical comparisons [cite: 543-552]\n",
    "a = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "b = tf.constant([[5, 4, 3], [3, 2, 1]])\n",
    "print(f\"\\na == b:\\n {a == b}\")\n",
    "print(f\"\\na <= b:\\n {a <= b}\")\n",
    "\n",
    [cite_start]"# Reduction operations [cite: 553-571]\n",
    "a = tf.constant(np.random.normal(size=[5, 4, 3]), dtype='float32')\n",
    "\n",
    "# Sum of all elements\n",
    "red_all = tf.reduce_sum(a)\n",
    "print(f\"\\nSum of all elements: {red_all.numpy()}\")\n",
    "\n",
    "# Product along axis 0\n",
    "red_a2 = tf.reduce_prod(a, axis=0)\n",
    "print(f\"\\nProduct along axis 0 shape: {red_a2.shape}\")\n",
    "\n",
    "# Minimum over axes 0 and 1\n",
    "red_a3 = tf.reduce_min(a, axis=[0, 1])\n",
    "print(f\"\\nMinimum over axes 0 and 1 shape: {red_a3.shape}\")\n",
    "\n",
    [cite_start]"# Demonstrating keepdims [cite: 572-578]\n",
    "red_keepdims_false = tf.reduce_min(a, axis=1, keepdims=False)\n",
    "print(f\"\\nkeepdims=False shape: {red_keepdims_false.shape}\")\n",
    "\n",
    "red_keepdims_true = tf.reduce_min(a, axis=1, keepdims=True)\n",
    "print(f\"keepdims=True shape: {red_keepdims_true.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Neural network-related computations in TensorFlow\n",
    "\n",
    [cite_start]"This section demonstrates core neural network operations using a computer vision example (image manipulation) [cite: 583-584]. We'll use the famous 'baboon' image, which requires downloading it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the baboon image for the examples\n",
    "import os\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# The book uses 'baboon.jpg'. We'll download a common version.\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/en/7/7d/Mandrill_Image.png\"\n",
    "image_path = \"baboon.jpg\"\n",
    "\n",
    "if not os.path.exists(image_path):\n",
    "    print(\"Downloading image...\")\n",
    "    r = requests.get(image_url)\n",
    "    with open(image_path, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    print(\"Image downloaded.\")\n",
    "else:\n",
    "    print(\"Image already exists.\")\n",
    "\n",
    [cite_start]"# Resize image to 512x512 as used in the book's example [cite: 605]\n",
    "img = Image.open(image_path)\n",
    "img = img.resize((512, 512))\n",
    "img.save(image_path) # Save the resized image\n",
    "\n",
    "print(f\"Image resized to {img.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Matrix Multiplication (RGB to Grayscale)\n",
    "\n",
    [cite_start]"We can convert an RGB image (shape `[H, W, 3]`) to grayscale (shape `[H, W, 1]`) by performing a matrix multiplication with a weights vector `[0.3], [0.59], [0.11]` [cite: 604-605]. [cite_start]`tf.squeeze` is used to remove the last dimension of size 1[cite: 605]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    [cite_start]"# Load the RGB image as a NumPy array [cite: 610]\n",
    "x_rgb = np.array(Image.open(image_path)).astype('float32')\n",
    "\n",
    [cite_start]"# Convert NumPy array to tf.Tensor [cite: 612]\n",
    "x_rgb_tf = tf.constant(x_rgb)\n",
    "\n",
    [cite_start]"# Define the RGB-to-grayscale conversion weights [cite: 614]\n",
    "grays = tf.constant([[0.3], [0.59], [0.11]], dtype='float32')\n",
    "\n",
    [cite_start]"# Perform matrix multiplication [cite: 615]\n",
    "x_gray = tf.matmul(x_rgb_tf, grays)\n",
    "\n",
    [cite_start]"# Remove the last dimension (of size 1) [cite: 615]\n",
    "x_gray_squeezed = tf.squeeze(x_gray)\n",
    "\n",
    "print(f\"Original shape: {x_rgb_tf.shape}\")\n",
    "print(f\"Grayscale shape before squeeze: {x_gray.shape}\")\n",
    "print(f\"Grayscale shape after squeeze: {x_gray_squeezed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Convolution Operation (Edge Detection)\n",
    "\n",
    [cite_start]"The convolution operation (`tf.nn.convolution`) is fundamental to CNNs[cite: 627]. [cite_start]It slides a small window (filter or kernel) over the data, performing element-wise multiplications and summing the results[cite: 627]. [cite_start]We can use it for effects like edge detection[cite: 626, 644].\n",
    "\n",
    [cite_start]"Note: `tf.nn.convolution` expects 4D tensors: `[batch, height, width, channels]` [cite: 646-647]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    [cite_start]"# Use the squeezed grayscale image from the previous step [cite: 644]\n",
    "y = tf.constant(x_gray_squeezed)\n",
    "\n",
    [cite_start]"# Define an edge detection filter (Laplacian filter) [cite: 645]\n",
    "filter_val = np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]]).astype('float32')\n",
    "edge_filter = tf.Variable(filter_val)\n",
    "\n",
    [cite_start]"# Reshape input and filter to 4D for tf.nn.convolution [cite: 648-651]\n",
    "# Input shape: [1, 512, 512, 1]\n",
    "y_reshaped = tf.reshape(y, [1, 512, 512, 1])\n",
    "\n",
    "# Filter shape: [3, 3, 1, 1]\n",
    "filter_reshaped = tf.reshape(edge_filter, [3, 3, 1, 1])\n",
    "\n",
    [cite_start]"# Perform convolution [cite: 652]\n",
    "y_conv = tf.nn.convolution(y_reshaped, filter_reshaped)\n",
    "\n",
    "print(f\"Input shape: {y_reshaped.shape}\")\n",
    "print(f\"Filter shape: {filter_reshaped.shape}\")\n",
    "print(f\"Convolution output shape: {y_conv.shape}\") # Output is 510x510 due to 'valid' padding by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Pooling Operation (Resizing)\n",
    "\n",
    [cite_start]"Pooling (`tf.nn.max_pool` or `tf.nn.avg_pool`) is used to downsample or resize feature maps, making the network more translation-invariant[cite: 655, 659]. [cite_start]It slides a window and takes either the maximum or average value from that window[cite: 659]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    [cite_start]"# Use the convolution output from the previous step [cite: 670]\n",
    "# Note: The book uses y_conv, which is 510x510. We'll use that.\n",
    "\n",
    [cite_start]"# ksize: The size of the pooling window [batch, height, width, channels] [cite: 670]\n",
    [cite_start]"# strides: How much to move the window [batch, height, width, channels] [cite: 670]\n",
    "ksize = [1, 2, 2, 1]\n",
    "strides = [1, 2, 2, 1]\n",
    "\n",
    [cite_start]"# Perform average pooling [cite: 670]\n",
    "z_avg = tf.nn.avg_pool(y_conv, ksize=ksize, strides=strides, padding='VALID')\n",
    "\n",
    [cite_start]"# Perform max pooling [cite: 670]\n",
    "z_max = tf.nn.max_pool(y_conv, ksize=ksize, strides=strides, padding='VALID')\n",
    "\n",
    "print(f\"Input shape: {y_conv.shape}\")\n",
    "print(f\"Avg Pool output shape: {z_avg.shape}\")\n",
    "print(f\"Max Pool output shape: {z_max.shape}\")\n",
    "\n",
    [cite_start]"# Squeeze to visualize [cite: 671]\n",
    "z_avg_squeezed = tf.squeeze(z_avg)\n",
    "z_max_squeezed = tf.squeeze(z_max)\n",
    "print(f\"\\nSqueezed Max Pool shape: {z_max_squeezed.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
