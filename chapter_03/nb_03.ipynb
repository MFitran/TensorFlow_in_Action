{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Keras and data retrieval in TensorFlow 2\n",
    "\n",
    "This notebook reproduces the code and summarizes the theoretical concepts from Chapter 3 of *'TensorFlow in Action'* by Thushan Ganegedara.\n",
    "\n",
    "This chapter covers two core topics:\n",
    "1.  **Keras Model-Building APIs**: How to build models using the Sequential, Functional, and Sub-classing APIs.\n",
    "2.  **Data Retrieval in TensorFlow**: How to load and preprocess data using `tf.data`, Keras DataGenerators, and the `tensorflow-datasets` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Keras model-building APIs\n",
    "\n",
    "Keras is a high-level API integrated into TensorFlow that simplifies model building. It offers three main APIs for different levels of complexity: Sequential, Functional, and Sub-classing.\n",
    "\n",
    "We will use the **Iris dataset** for these examples. The goal is to classify a flower's species (Iris-setosa, Iris-versicolor, Iris-virginica) based on four features: sepal length, sepal width, petal length, and petal width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Introducing the Data (Iris Dataset)\n",
    "\n",
    "First, we download and preprocess the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Download the data\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "r = requests.get(url)\n",
    "\n",
    "with open('iris.data', 'wb') as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "# Load data into pandas\n",
    "iris_df = pd.read_csv('iris.data', header=None)\n",
    "\n",
    "# Add column names and map labels to integers\n",
    "iris_df.columns = ['sepal_length', 'sepal_width', 'petal_width', 'petal_length', 'label']\n",
    "iris_df[\"label\"] = iris_df[\"label\"].map({'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2})\n",
    "\n",
    "# Shuffle the data\n",
    "iris_df = iris_df.sample(frac=1.0, random_state=4321)\n",
    "\n",
    "# Separate features (x) and labels (y)\n",
    "# We also center the feature data by subtracting the mean\n",
    "x = iris_df[[\"sepal_length\", \"sepal_width\", \"petal_width\", \"petal_length\"]]\n",
    "x = x - x.mean(axis=0)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y = tf.one_hot(iris_df[\"label\"], depth=3)\n",
    "\n",
    "print(\"Features (x) head:\")\n",
    "print(x.head())\n",
    "print(\"\\nLabels (y) head:\")\n",
    "print(y.numpy()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 The Sequential API\n",
    "\n",
    "The **Sequential API** is the simplest. It's used for models where layers are stacked in a linear, sequential order (one input, one output).\n",
    "\n",
    "We will build **Model A**: a simple MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Clear any previous session state\n",
    "K.clear_session()\n",
    "\n",
    "# Define the model using the Sequential API\n",
    "model_A = Sequential([\n",
    "    # The first layer must specify the input_shape\n",
    "    Dense(32, activation='relu', input_shape=(4,)),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(3, activation='softmax') # Output layer for 3 classes\n",
    "])\n",
    "\n",
    "# Compile the model with a loss function, optimizer, and metrics\n",
    "model_A.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['acc'])\n",
    "\n",
    "# Display the model's architecture\n",
    "model_A.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using model.fit()\n",
    "history = model_A.fit(x, y, batch_size=64, epochs=25, validation_split=0.1)\n",
    "print(\"\\nTraining complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 The Functional API\n",
    "\n",
    "The **Functional API** is more flexible and is used for complex models, such as those with multiple input layers or multiple output branches.\n",
    "\n",
    "We will build **Model B**, which takes two inputs:\n",
    "1.  The raw features (4 values).\n",
    "2.  The first two Principal Components (PCA) of the features (2 values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 1. Create the additional PCA features\n",
    "pca_model = PCA(n_components=2, random_state=4321)\n",
    "x_pca = pca_model.fit_transform(x)\n",
    "\n",
    "print(f\"Original features shape: {x.shape}\")\n",
    "print(f\"PCA features shape: {x_pca.shape}\")\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# 2. Define the two input layers\n",
    "inp1 = Input(shape=(4,), name=\"raw_features\")\n",
    "inp2 = Input(shape=(2,), name=\"pca_features\")\n",
    "\n",
    "# 3. Define the parallel branches for each input\n",
    "out1 = Dense(16, activation='relu')(inp1)\n",
    "out2 = Dense(16, activation='relu')(inp2)\n",
    "\n",
    "# 4. Concatenate the outputs of the parallel branches\n",
    "out = Concatenate(axis=1)([out1, out2])\n",
    "\n",
    "# 5. Add final layers\n",
    "out = Dense(16, activation='relu')(out)\n",
    "out = Dense(3, activation='softmax')(out)\n",
    "\n",
    "# 6. Create the Model object, specifying inputs and outputs\n",
    "model_B = Model(inputs=[inp1, inp2], outputs=out)\n",
    "\n",
    "# 7. Compile the model\n",
    "model_B.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "# 8. Display the summary\n",
    "model_B.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model architecture\n",
    "tf.keras.utils.plot_model(model_B, show_shapes=True, to_file='model_B.png')\n",
    "\n",
    "# To train a multi-input model, pass a list of inputs to model.fit()\n",
    "history = model_B.fit([x, x_pca], y, batch_size=64, epochs=10, validation_split=0.1)\n",
    "print(\"\\nTraining complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 The Sub-classing API\n",
    "\n",
    "The **Sub-classing API** is the most flexible. It allows you to create fully custom layers or models by writing a Python class that inherits from `tf.keras.layers.Layer` or `tf.keras.Model`.\n",
    "\n",
    "This is necessary when you need to define custom computations in the forward pass.\n",
    "When sub-classing a layer, you must override three key methods:\n",
    "* `__init__()`: To define hyperparameters.\n",
    "* `build()`: To create the layer's trainable weights (e.g., `self.w`, `self.b`).\n",
    "* `call()`: To define the forward pass computation.\n",
    "\n",
    "We will build **Model C**, which uses a custom layer (`MulBiasDense`) with a *multiplicative* bias in addition to the standard additive bias: $h = \\alpha([xW + b] \\times b_{mul})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# 1. Define the custom layer by sub-classing\n",
    "class MulBiasDense(layers.Layer):\n",
    "    def __init__(self, units=32, activation=None, **kwargs):\n",
    "        super(MulBiasDense, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create trainable weights for the layer\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        # The new multiplicative bias\n",
    "        self.b_mul = self.add_weight(shape=(self.units,),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        super(MulBiasDense, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Define the forward pass computation\n",
    "        out = (tf.matmul(inputs, self.w) + self.b) * self.b_mul\n",
    "        if self.activation:\n",
    "            return layers.Activation(self.activation)(out)\n",
    "        return out\n",
    "\n",
    "# 2. Build the model using the custom layer (we can use the Functional API for this)\n",
    "K.clear_session()\n",
    "\n",
    "inp = Input(shape=(4,))\n",
    "out = MulBiasDense(units=32, activation='relu')(inp)\n",
    "out = MulBiasDense(units=16, activation='relu')(out)\n",
    "out = Dense(3, activation='softmax')(out)\n",
    "\n",
    "model_C = Model(inputs=inp, outputs=out)\n",
    "\n",
    "# 3. Compile and summarize\n",
    "model_C.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model_C.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Retrieving data for TensorFlow/Keras models\n",
    "\n",
    "This section covers the different ways to create input pipelines to feed data to models. We'll use a new dataset of flower images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 `tf.data` API\n",
    "\n",
    "The `tf.data` API is TensorFlow's recommended way to build high-performance, complex data pipelines. It allows you to build a graph of transformations (like `.map()`, `.shuffle()`, `.batch()`) that process data efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from the book to build a tf.data pipeline for flower images\n",
    "# This assumes 'flower_labels.csv' and an 'flower_images' directory exist\n",
    "# We will simulate this with placeholder data for demonstration\n",
    "\n",
    "print(\"Building a tf.data pipeline...\")\n",
    "\n",
    "# --- Setup for demonstration (Simulating files) ---\n",
    "import os\n",
    "from PIL import Image\n",
    "data_dir = os.path.join('data', 'flower_images')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "csv_path = os.path.join(data_dir, 'flower_labels.csv')\n",
    "\n",
    "# Create a dummy CSV file\n",
    "with open(csv_path, 'w') as f:\n",
    "    f.write('file,label\\n')\n",
    "    for i in range(1, 21):\n",
    "        f.write(f'flower_{i:03d}.png,{i % 10}\\n')\n",
    "\n",
    "# Create dummy image files\n",
    "for i in range(1, 21):\n",
    "    img_array = np.random.rand(64, 64, 3) * 255\n",
    "    img = Image.fromarray(img_array.astype('uint8')).convert('RGB')\n",
    "    img.save(os.path.join(data_dir, f'flower_{i:03d}.png'))\n",
    "\n",
    "print(\"Dummy files created.\")\n",
    "# --- End of setup ---\n",
    "\n",
    "\n",
    "# 1. Read CSV file as a tf.data.Dataset\n",
    "csv_ds = tf.data.experimental.CsvDataset(\n",
    "    csv_path,\n",
    "    record_defaults=[\"\", -1], \n",
    "    header=True\n",
    ")\n",
    "\n",
    "# 2. Separate filenames and labels using .map()\n",
    "fname_ds = csv_ds.map(lambda a, b: a)\n",
    "label_ds = csv_ds.map(lambda a, b: b)\n",
    "\n",
    "# 3. Create a function to load and process images\n",
    "def get_image(file_path):\n",
    "    img = tf.io.read_file(data_dir + os.path.sep + file_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return tf.image.resize(img, [64, 64])\n",
    "\n",
    "# 4. Map the image loading function to the filenames dataset\n",
    "image_ds = fname_ds.map(get_image)\n",
    "\n",
    "# 5. One-hot encode the labels\n",
    "label_ds = label_ds.map(lambda x: tf.one_hot(x, depth=10))\n",
    "\n",
    "# 6. Zip the image and label datasets together\n",
    "data_ds = tf.data.Dataset.zip((image_ds, label_ds))\n",
    "\n",
    "# 7. Shuffle, batch, and repeat\n",
    "data_ds = data_ds.shuffle(buffer_size=20)\n",
    "data_ds = data_ds.batch(5)\n",
    "\n",
    "# 8. Inspect a batch\n",
    "for images, labels in data_ds.take(1):\n",
    "    print(f\"\\nBatch of images shape: {images.shape}\")\n",
    "    print(f\"Batch of labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Keras DataGenerators\n",
    "\n",
    "For simpler use cases, especially with images, Keras provides `ImageDataGenerator`. This can read images directly from directories or from a `pandas.DataFrame` without the manual setup of `tf.data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 1. Load the CSV file into a pandas DataFrame\n",
    "labels_df = pd.read_csv(csv_path, header=0)\n",
    "\n",
    "# 2. Initialize the ImageDataGenerator\n",
    "img_gen = ImageDataGenerator()\n",
    "\n",
    "# 3. Use .flow_from_dataframe() to create the generator\n",
    "gen_iter = img_gen.flow_from_dataframe(\n",
    "    dataframe=labels_df,\n",
    "    directory=data_dir,\n",
    "    x_col='file',      # Column with filenames\n",
    "    y_col='label',     # Column with labels\n",
    "    class_mode='raw',  # Labels are provided as raw integers\n",
    "    batch_size=5,\n",
    "    target_size=(64, 64)\n",
    ")\n",
    "\n",
    "# 4. Inspect a batch\n",
    "images, labels = next(gen_iter)\n",
    "print(f\"Batch of images shape: {images.shape}\")\n",
    "print(f\"Batch of labels: {labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 `tensorflow-datasets` package\n",
    "\n",
    "The `tensorflow-datasets` (tfds) package is the easiest way to access hundreds of common, pre-processed datasets (like MNIST, CIFAR-10, etc.) with a single line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# 1. Load the 'cifar10' dataset\n",
    "# This will download it if not already present\n",
    "data, info = tfds.load(\"cifar10\", with_info=True)\n",
    "\n",
    "# 2. Inspect the dataset info\n",
    "print(info)\n",
    "\n",
    "# 3. The 'data' object is a dictionary of tf.data.Dataset objects\n",
    "print(data.keys())\n",
    "\n",
    "# 4. Prepare the training dataset for a model\n",
    "def format_data(x):\n",
    "    # Normalize image and one-hot encode label\n",
    "    return (tf.cast(x[\"image\"], 'float32') / 255.0, tf.one_hot(x[\"label\"], depth=10))\n",
    "\n",
    "train_ds = data[\"train\"].map(format_data).batch(16)\n",
    "\n",
    "# 5. Inspect a batch\n",
    "for images, labels in train_ds.take(1):\n",
    "    print(f\"\\nBatch of images shape: {images.shape}\")\n",
    "    print(f\"Batch of labels shape: {labels.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
