{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Telling things apart: Image segmentation\n",
    "\n",
    "This notebook reproduces the code and summarizes the theoretical concepts from Chapter 8 of *'TensorFlow in Action'* by Thushan Ganegedara.\n",
    "\n",
    "This chapter moves from image *classification* (one label per image) to **image segmentation** (one label per pixel). This is a dense prediction task that identifies *where* objects are in an image.\n",
    "\n",
    "We will cover:\n",
    "1.  **Understanding Segmentation Data**: Loading the PASCAL VOC 2012 dataset, including its special palettized PNG format.\n",
    "2.  **Building a `tf.data` Pipeline**: Creating an efficient pipeline for loading, preprocessing, and augmenting segmentation data.\n",
    "3.  **Implementing DeepLabv3**: Building a state-of-the-art segmentation model using a pretrained ResNet-50 backbone, atrous convolution, and an Atrous Spatial Pyramid Pooling (ASPP) module.\n",
    "4.  **Custom Loss & Metrics**: Implementing segmentation-specific losses (Dice Loss, Weighted Cross-Entropy) and metrics (Mean IoU) to handle class imbalance.\n",
    "5.  **Training & Evaluation**: Training the model and evaluating its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Understanding the Data (PASCAL VOC 2012)\n",
    "\n",
    "We will use the **PASCAL VOC 2012** dataset. In segmentation, the data consists of pairs:\n",
    "1.  **Input Image**: A standard RGB image (e.g., `[Height, Width, 3]`).\n",
    "2.  **Target Mask**: A special \"palettized\" image. It looks like a colored-in version of the input, but it's a 2D array (`[Height, Width]`) where each pixel's value is an **integer class index** (e.g., 0=Background, 1=Aeroplane, ..., 12=Dog)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow.keras.backend as K\n",
    "from functools import partial\n",
    "\n",
    "# 1. Download the data\n",
    "data_url = \"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\"\n",
    "tar_path = os.path.join('data', 'VOCtrainval_11-May-2012.tar')\n",
    "extract_path = os.path.join('data', 'VOCtrainval_11-May-2012')\n",
    "img_dir = os.path.join(extract_path, 'VOCdevkit', 'VOC2012', 'JPEGImages')\n",
    "seg_dir = os.path.join(extract_path, 'VOCdevkit', 'VOC2012', 'SegmentationClass')\n",
    "subset_dir = os.path.join(extract_path, 'VOCdevkit', 'VOC2012', 'ImageSets', 'Segmentation')\n",
    "\n",
    "if not os.path.exists(extract_path):\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    if not os.path.exists(tar_path):\n",
    "        print(\"Downloading PASCAL VOC 2012 dataset (approx. 2GB)...\")\n",
    "        r = requests.get(data_url)\n",
    "        with open(tar_path, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        print(\"Download complete.\")\n",
    "    \n",
    "    print(\"Extracting data...\")\n",
    "    with tarfile.open(tar_path, 'r') as tar:\n",
    "        tar.extractall('data')\n",
    "    print(\"Extraction complete.\")\n",
    "else:\n",
    "    print(\"Data already downloaded and extracted.\")\n",
    "\n",
    "# 2. Function to read palettized target images\n",
    "# These PNGs store class indices, not RGB colors.\n",
    "# We need this function to load them correctly as 2D NumPy arrays.\n",
    "def load_image_func(image_path):\n",
    "    \"\"\"Load a palettized image from a file path.\"\"\"\n",
    "    img = np.array(Image.open(image_path))\n",
    "    return img\n",
    "\n",
    "print(\"\\nSetup complete. Ready to build data pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Defining a TensorFlow data pipeline\n",
    "\n",
    "We need a robust `tf.data` pipeline to handle loading the image pairs, resizing/cropping, augmentation, and batching. This is the most complex data pipeline we've built so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global parameters\n",
    "random_seed = 42\n",
    "batch_size = 16 # Smaller batch size for segmentation models\n",
    "input_size = (384, 384)\n",
    "output_size = None # Our model will output at a different resolution\n",
    "epochs = 25\n",
    "num_classes = 22 # 21 classes + 1 background\n",
    "\n",
    "# 1. Get filenames for a given subset (train/val/test)\n",
    "def get_subset_filenames(orig_dir, seg_dir, subset_dir, subset):\n",
    "    if subset.startswith('train'):\n",
    "        file_list = pd.read_csv(os.path.join(subset_dir, \"train.txt\"), \n",
    "                                index_col=None, header=None, squeeze=True).tolist()\n",
    "    elif subset.startswith('val') or subset.startswith('test'):\n",
    "        file_list = pd.read_csv(os.path.join(subset_dir, \"val.txt\"), \n",
    "                                index_col=None, header=None, squeeze=True).tolist()\n",
    "        \n",
    "        # Split the 'val.txt' list into validation and test sets\n",
    "        random.seed(random_seed)\n",
    "        random.shuffle(file_list)\n",
    "        split_idx = len(file_list) // 2\n",
    "        if subset.startswith('val'):\n",
    "            file_list = file_list[:split_idx]\n",
    "        else:\n",
    "            file_list = file_list[split_idx:]\n",
    "    else:\n",
    "        raise NotImplementedError(\"Subset={} is not recognized\".format(subset))\n",
    "    \n",
    "    orig_filenames = [os.path.join(orig_dir, f + '.jpg') for f in file_list]\n",
    "    seg_filenames = [os.path.join(seg_dir, f + '.png') for f in file_list]\n",
    "    \n",
    "    for o, s in zip(orig_filenames, seg_filenames):\n",
    "        yield o, s\n",
    "\n",
    "# 2. Helper functions for resizing/cropping and augmentations\n",
    "def randomly_crop_or_resize(x, y, resize_to_before_crop, input_size, augmentation):\n",
    "    def rand_crop(x, y):\n",
    "        x = tf.image.resize(x, resize_to_before_crop, method='bilinear')\n",
    "        y = tf.image.resize(y, resize_to_before_crop, method='nearest') # Must use 'nearest' for masks\n",
    "        \n",
    "        # Get a random crop\n",
    "        crop_shape = (input_size[0], input_size[1], x.shape[-1])\n",
    "        x_crop = tf.image.random_crop(x, crop_shape)\n",
    "        \n",
    "        # We need to apply the *same* crop to the mask (y)\n",
    "        # This part is simplified from the book for clarity; tf.image.random_crop doesn't \n",
    "        # guarantee the same crop. A better way is to get parameters from tf.image.sample_distorted_bounding_box\n",
    "        # For this notebook, we'll just resize to keep it simple.\n",
    "        return resize(x, y)\n",
    "\n",
    "    def resize(x, y):\n",
    "        x = tf.image.resize(x, input_size, method='bilinear')\n",
    "        # y is [H, W], we add a channel dim, resize, and remove it\n",
    "        y = tf.image.resize(y[..., tf.newaxis], input_size, method='nearest')\n",
    "        y = tf.squeeze(y, axis=-1)\n",
    "        return x, y\n",
    "\n",
    "    if augmentation:\n",
    "        # In a real pipeline, you'd use tf.cond to randomly pick one\n",
    "        x, y = resize(x, y) # Simplified for this notebook\n",
    "    else:\n",
    "        x, y = resize(x, y)\n",
    "    return x, y\n",
    "\n",
    "def fix_shape(x, y, size):\n",
    "    x.set_shape([size[0], size[1], 3])\n",
    "    y.set_shape([size[0], size[1]])\n",
    "    return x, y\n",
    "\n",
    "def randomly_flip_horizontal(x, y):\n",
    "    rand = tf.random.uniform([], 0.0, 1.0)\n",
    "    def flip(x, y):\n",
    "        return tf.image.flip_left_right(x), tf.image.flip_left_right(y)\n",
    "    return tf.cond(rand < 0.5, lambda: flip(x, y), lambda: (x, y))\n",
    "\n",
    "# 3. The main pipeline builder function (based on Listing 8.6)\n",
    "def get_subset_tf_dataset(\n",
    "    subset_filename_gen_func, batch_size, epochs, \n",
    "    input_size=(256, 256), output_size=None, resize_to_before_crop=None, \n",
    "    augmentation=False, shuffle=False\n",
    "):\n",
    "    \n",
    "    # Create dataset of filenames\n",
    "    filename_ds = tf.data.Dataset.from_generator(\n",
    "        subset_filename_gen_func, output_types=(tf.string, tf.string)\n",
    "    )\n",
    "    \n",
    "    # Load images from files. Use tf.numpy_function to run the PIL-based loader\n",
    "    image_ds = filename_ds.map(lambda x, y: (\n",
    "        tf.image.decode_jpeg(tf.io.read_file(x), channels=3),\n",
    "        tf.numpy_function(load_image_func, [y], [tf.uint8])\n",
    "    )).cache() # Use .cache() for performance\n",
    "    \n",
    "    # Normalize input image, cast target mask\n",
    "    image_ds = image_ds.map(lambda x, y: (tf.cast(x, 'float32') / 255.0, tf.cast(y, 'float32')))\n",
    "    \n",
    "    # Resize / crop\n",
    "    image_ds = image_ds.map(lambda x, y: randomly_crop_or_resize(\n",
    "        x, y, resize_to_before_crop, input_size, augmentation\n",
    "    ))\n",
    "    \n",
    "    # Set static shape information\n",
    "    image_ds = image_ds.map(lambda x, y: fix_shape(x, y, size=input_size))\n",
    "    \n",
    "    # Apply augmentations (only if augmentation=True)\n",
    "    if augmentation:\n",
    "        image_ds = image_ds.map(randomly_flip_horizontal)\n",
    "        image_ds = image_ds.map(lambda x, y: (tf.image.random_hue(x, 0.1), y))\n",
    "        image_ds = image_ds.map(lambda x, y: (tf.image.random_brightness(x, 0.1), y))\n",
    "        image_ds = image_ds.map(lambda x, y: (tf.image.random_contrast(x, 0.8, 1.2), y))\n",
    "    \n",
    "    # Resize output if needed\n",
    "    if output_size:\n",
    "        image_ds = image_ds.map(lambda x, y: (x, tf.image.resize(y[..., tf.newaxis], output_size, method='nearest')))\n",
    "        \n",
    "    if shuffle:\n",
    "        image_ds = image_ds.shuffle(buffer_size=batch_size * 5)\n",
    "        \n",
    "    # Batch and repeat\n",
    "    image_ds = image_ds.batch(batch_size).repeat(epochs)\n",
    "    \n",
    "    # Prefetch for performance\n",
    "    image_ds = image_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    # Squeeze the target mask's channel dimension (from [B, H, W, 1] to [B, H, W])\n",
    "    image_ds = image_ds.map(lambda x, y: (x, tf.squeeze(y)))\n",
    "    \n",
    "    return image_ds\n",
    "\n",
    "# 4. Instantiate the pipelines (based on Listing 8.7)\n",
    "partial_subset_fn = partial(\n",
    "    get_subset_filenames, orig_dir=img_dir, seg_dir=seg_dir, subset_dir=subset_dir\n",
    ")\n",
    "\n",
    "train_subset_fn = partial(partial_subset_fn, subset='train')\n",
    "val_subset_fn = partial(partial_subset_fn, subset='val')\n",
    "test_subset_fn = partial(partial_subset_fn, subset='test')\n",
    "\n",
    "tr_image_ds = get_subset_tf_dataset(\n",
    "    train_subset_fn, batch_size, epochs,\n",
    "    input_size=input_size, resize_to_before_crop=(444, 444),\n",
    "    augmentation=True, shuffle=True\n",
    ")\n",
    "\n",
    "val_image_ds = get_subset_tf_dataset(\n",
    "    val_subset_fn, batch_size, epochs, \n",
    "    input_size=input_size, shuffle=False\n",
    ")\n",
    "\n",
    "test_image_ds = get_subset_tf_dataset(\n",
    "    test_subset_fn, batch_size, 1, \n",
    "    input_size=input_size, shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Training dataset element spec: {tr_image_ds.element_spec}\")\n",
    "print(f\"Validation dataset element spec: {val_image_ds.element_spec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 DeepLabv3: Using pretrained networks to segment images\n",
    "\n",
    "We will implement the **DeepLabv3** model. This architecture uses a powerful pretrained CNN (like ResNet-50) as a feature extractor (the \"backbone\").\n",
    "\n",
    "Its key innovations are:\n",
    "1.  **Atrous (Dilated) Convolution**: This is a convolution with \"holes.\" It allows the filter to cover a larger area (a larger \"receptive field\") *without* increasing the number of parameters or computation. This is key for capturing multi-scale context.\n",
    "2.  **Atrous Spatial Pyramid Pooling (ASPP)**: This module runs several parallel atrous convolutions with *different* dilation rates (e.g., 6, 12, 18) on the backbone's output. This captures information from multiple scales simultaneously. These outputs are concatenated, along with a global average pooled feature, to create a rich, multi-scale feature representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the ResNet-50 backbone (up to conv4 block)\n",
    "inp = layers.Input(shape=input_size + (3,))\n",
    "resnet50 = tf.keras.applications.ResNet50(\n",
    "    include_top=False, input_tensor=inp, pooling=None\n",
    ")\n",
    "\n",
    "# Find the output of the 'conv4' block\n",
    "out = resnet50.get_layer(\"conv4_block6_out\").output\n",
    "resnet50_upto_conv4 = models.Model(resnet50.input, out)\n",
    "\n",
    "# 2. Re-implement the 'conv5' block using atrous convolution (dilation rate = 2)\n",
    "# This involves helper functions for the ResNet blocks\n",
    "def block_level3(inp, filters, kernel_size, rate, block_id, convlayer_id, activation=True):\n",
    "    conv_name = f'conv5_block{block_id}_{convlayer_id}_conv'\n",
    "    bn_name = f'conv5_block{block_id}_{convlayer_id}_bn'\n",
    "    act_name = f'conv5_block{block_id}_{convlayer_id}_relu'\n",
    "    \n",
    "    conv_out = layers.Conv2D(filters, kernel_size, dilation_rate=rate, padding='same', name=conv_name)(inp)\n",
    "    bn_out = layers.BatchNormalization(name=bn_name)(conv_out)\n",
    "    if activation:\n",
    "        return layers.Activation('relu', name=act_name)(bn_out)\n",
    "    return bn_out\n",
    "\n",
    "def block_level2(inp, rate, block_id):\n",
    "    block_1_out = block_level3(inp, 512, (1,1), rate, block_id, 1)\n",
    "    block_2_out = block_level3(block_1_out, 512, (3,3), rate, block_id, 2)\n",
    "    block_3_out = block_level3(block_2_out, 2048, (1,1), rate, block_id, 3, activation=False)\n",
    "    return block_3_out\n",
    "\n",
    "def resnet_block(inp, rate):\n",
    "    block0_out = block_level3(inp, 2048, (1,1), 1, block_id=1, convlayer_id=0, activation=False)\n",
    "    block1_out = block_level2(inp, rate, block_id=1)\n",
    "    block1_add = layers.Add(name='conv5_block1_add')([block0_out, block1_out])\n",
    "    block1_relu = layers.Activation('relu', name='conv5_block1_relu')(block1_add)\n",
    "    \n",
    "    # ... (Blocks 2 and 3)\n",
    "    block2_out = block_level2(block1_relu, rate, block_id=2)\n",
    "    block2_add = layers.Add(name='conv5_block2_add')([block1_relu, block2_out])\n",
    "    block2_relu = layers.Activation('relu', name='conv5_block2_relu')(block2_add)\n",
    "    \n",
    "    block3_out = block_level2(block2_relu, rate, block_id=3)\n",
    "    block3_add = layers.Add(name='conv5_block3_add')([block2_relu, block3_out])\n",
    "    block3_relu = layers.Activation('relu', name='conv5_block3_relu')(block3_add)\n",
    "    return block3_relu\n",
    "\n",
    "print(\"Building atrous conv5 block...\")\n",
    "resnet_block4_out = resnet_block(resnet50_upto_conv4.output, rate=2)\n",
    "\n",
    "# 3. Implement the ASPP Module\n",
    "def atrous_spatial_pyramid_pooling(inp):\n",
    "    dims = K.shape(inp)\n",
    "    out_shape = (dims[1], dims[2])\n",
    "\n",
    "    # Branch 1: 1x1 convolution\n",
    "    outa_1_conv = block_level3(inp, 256, (1,1), 1, '_aspp_a', 1, activation='relu')\n",
    "    # Branch 2: 3x3 atrous conv, rate=6\n",
    "    outa_2_conv = block_level3(inp, 256, (3,3), 6, '_aspp_a', 2, activation='relu')\n",
    "    # Branch 3: 3x3 atrous conv, rate=12\n",
    "    outa_3_conv = block_level3(inp, 256, (3,3), 12, '_aspp_a', 3, activation='relu')\n",
    "    # Branch 4: 3x3 atrous conv, rate=18\n",
    "    outa_4_conv = block_level3(inp, 256, (3,3), 18, '_aspp_a', 4, activation='relu')\n",
    "\n",
    "    # Branch 5: Global Average Pooling\n",
    "    outb_1_avg = layers.GlobalAveragePooling2D()(inp)\n",
    "    outb_1_avg = layers.Reshape((1, 1, K.int_shape(outb_1_avg)[-1]))(outb_1_avg)\n",
    "    outb_1_conv = block_level3(outb_1_avg, 256, (1,1), 1, '_aspp_b', 1, activation='relu')\n",
    "    # Upsample back to feature map size\n",
    "    outb_1_up = tf.image.resize(outb_1_conv, out_shape, method='bilinear')\n",
    "\n",
    "    # Concatenate all branches\n",
    "    out_aspp = layers.Concatenate(axis=-1)([\n",
    "        outa_1_conv, outa_2_conv, outa_3_conv, outa_4_conv, outb_1_up\n",
    "    ])\n",
    "    return out_aspp\n",
    "\n",
    "print(\"Building ASPP module...\")\n",
    "out_aspp = atrous_spatial_pyramid_pooling(resnet_block4_out)\n",
    "\n",
    "# 4. Final Layers (Classifier Head)\n",
    "# 1x1 convolution to get the right number of class channels (logits)\n",
    "out = layers.Conv2D(num_classes, (1,1), padding='same')(out_aspp)\n",
    "\n",
    "# Upsample the final prediction to match the input image size\n",
    "final_out = tf.image.resize(out, input_size, method='bilinear')\n",
    "\n",
    "# 5. Create the DeepLabv3 Model\n",
    "deeplabv3 = models.Model(resnet50_upto_conv4.input, final_out)\n",
    "\n",
    "print(\"DeepLabv3 model built successfully.\")\n",
    "deeplabv3.summary(line_length=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Compiling the model: Loss functions and evaluation metrics\n",
    "\n",
    "Standard accuracy or cross-entropy isn't ideal for segmentation due to **class imbalance** (e.g., the 'background' class often dominates 90% of the pixels). We need specialized losses and metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.1 Loss Functions\n",
    "\n",
    "1.  **Weighted Sparse Categorical Cross-Entropy**: We use standard cross-entropy but apply *weights* to each pixel's loss. Pixels from rare classes (like 'cat') get a higher weight, and pixels from common classes (like 'background') get a lower weight. This forces the model to pay attention to minority classes.\n",
    "2.  **Dice Loss**: A popular segmentation loss based on the Dice Coefficient (similar to F1-score). It directly maximizes the overlap (intersection) between the predicted mask and the true mask.\n",
    "    $DiceLoss = 1 - \\frac{2 \\times |Intersection(A, B)|}{|A| + |B|}$\n",
    "\n",
    "We will combine these two losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Function to get pixel weights (based on Listing 8.13)\n",
    "def get_label_weights(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, 'int32')\n",
    "    weights = tf.reduce_sum(tf.one_hot(y_true, num_classes), axis=[1, 2])\n",
    "    tot = tf.reduce_sum(weights, axis=-1, keepdims=True)\n",
    "    weights = (tot - weights) / tot # [batch, num_classes]\n",
    "    y_true_flat = tf.reshape(y_true, [-1])\n",
    "    y_weights = tf.gather(params=tf.reshape(weights, [-1]), indices=y_true_flat)\n",
    "    y_weights = tf.reshape(y_weights, K.shape(y_true))\n",
    "    return y_weights\n",
    "\n",
    "# 2. Weighted Cross-Entropy Loss (based on Listing 8.14)\n",
    "def ce_weighted_from_logits(num_classes):\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, 'int32')\n",
    "        # Mask out-of-bounds labels (e.g., boundary pixel value 255)\n",
    "        valid_mask = (y_true < num_classes)\n",
    "        y_true_masked = tf.boolean_mask(y_true, valid_mask)\n",
    "        y_pred_masked = tf.boolean_mask(y_pred, valid_mask)\n",
    "        y_weights = get_label_weights(y_true_masked, y_pred_masked)\n",
    "        \n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=y_true_masked, \n",
    "            logits=y_pred_masked\n",
    "        )\n",
    "        return tf.reduce_mean(loss * y_weights)\n",
    "    return loss_fn\n",
    "\n",
    "# 3. Dice Loss (based on Listing 8.15)\n",
    "def dice_loss_from_logits(num_classes):\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        smooth = 1.0\n",
    "        y_true = tf.cast(y_true, 'int32')\n",
    "        y_pred = tf.nn.softmax(y_pred)\n",
    "        y_true_one_hot = tf.one_hot(y_true, num_classes, dtype=tf.float32)\n",
    "        \n",
    "        # Flatten\n",
    "        y_true_flat = tf.reshape(y_true_one_hot, [-1, num_classes])\n",
    "        y_pred_flat = tf.reshape(y_pred, [-1, num_classes])\n",
    "        \n",
    "        intersection = tf.reduce_sum(y_true_flat * y_pred_flat, axis=0)\n",
    "        union = tf.reduce_sum(y_true_flat, axis=0) + tf.reduce_sum(y_pred_flat, axis=0)\n",
    "        \n",
    "        score = (2. * intersection + smooth) / (union + smooth)\n",
    "        loss = 1.0 - tf.reduce_mean(score)\n",
    "        return loss\n",
    "    return loss_fn\n",
    "\n",
    "# 4. Combined Loss (based on Listing 8.16)\n",
    "def ce_dice_loss_from_logits(num_classes):\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        ce_loss = ce_weighted_from_logits(num_classes)(y_true, y_pred)\n",
    "        dice_loss = dice_loss_from_logits(num_classes)(y_true, y_pred)\n",
    "        return ce_loss + dice_loss\n",
    "    return loss_fn\n",
    "\n",
    "print(\"Custom loss functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.2 Evaluation Metrics\n",
    "\n",
    "We need custom metrics that understand segmentation:\n",
    "1.  **`PixelAccuracyMetric`**: Simplest metric. What percentage of pixels were classified correctly? (Can be misleading if 'background' is 99% of the image).\n",
    "2.  **`MeanAccuracyMetric`**: Calculates the accuracy *for each class* individually, then computes the mean of those accuracies. This is much better for imbalanced datasets.\n",
    "3.  **`MeanIoUMetric` (Mean Intersection over Union)**: The gold standard for segmentation. For each class, it computes $IoU = \\frac{True \\, Positives}{True \\, Positives + False \\, Positives + False \\, Negatives}$. It then averages this IoU score across all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use Keras's built-in MeanIoU metric for simplicity, \n",
    "# as the custom implementations in the book (Listings 8.17-8.19) \n",
    "# are primarily for demonstrating how to build stateful metrics.\n",
    "\n",
    "# Keras's MeanIoU handles the masking of out-of-bounds labels.\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "deeplabv3.compile(\n",
    "    loss=ce_dice_loss_from_logits(num_classes),\n",
    "    optimizer=optimizer,\n",
    "    metrics=[\n",
    "        tf.keras.metrics.MeanIoU(num_classes=num_classes, name='mean_iou')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Copy weights from the original ResNet-50 conv5 block\n",
    "w_dict = {}\n",
    "for l in [\"conv5_block1_0_conv\", \"conv5_block1_0_bn\", \n",
    "          \"conv5_block1_1_conv\", \"conv5_block1_1_bn\", \n",
    "          \"conv5_block1_2_conv\", \"conv5_block1_2_bn\", \n",
    "          \"conv5_block1_3_conv\", \"conv5_block1_3_bn\"]:\n",
    "    if l in [layer.name for layer in resnet50.layers]:\n",
    "        w_dict[l] = resnet50.get_layer(l).get_weights()\n",
    "\n",
    "for k, w in w_dict.items():\n",
    "    if k in [layer.name for layer in deeplabv3.layers]:\n",
    "        deeplabv3.get_layer(k).set_weights(w)\n",
    "\n",
    "print(\"Model compiled with custom loss and MeanIoU metric.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 & 8.6: Training and Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of steps per epoch\n",
    "n_train_files = len(pd.read_csv(os.path.join(subset_dir, \"train.txt\"), \n",
    "                                index_col=None, header=None, squeeze=True))\n",
    "n_val_files = len(pd.read_csv(os.path.join(subset_dir, \"val.txt\"), \n",
    "                              index_col=None, header=None, squeeze=True)) // 2\n",
    "\n",
    "n_train_steps = n_train_files // batch_size\n",
    "n_valid_steps = n_val_files // batch_size\n",
    "\n",
    "print(f\"Training steps: {n_train_steps}\")\n",
    "print(f\"Validation steps: {n_valid_steps}\")\n",
    "\n",
    "# Define callbacks\n",
    "csv_logger = CSVLogger(os.path.join('eval', '1_pretrained_deeplabv3.log'))\n",
    "lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, mode='min')\n",
    "es_callback = EarlyStopping(monitor='val_loss', patience=6, mode='min')\n",
    "\n",
    "# Train the model (running for only 1 epoch for demonstration)\n",
    "print(\"Starting model training...\")\n",
    "history = deeplabv3.fit(\n",
    "    x=tr_image_ds,\n",
    "    steps_per_epoch=n_train_steps,\n",
    "    validation_data=val_image_ds,\n",
    "    validation_steps=n_valid_steps,\n",
    "    epochs=1, # Book runs for 25\n",
    "    callbacks=[lr_callback, csv_logger, es_callback]\n",
    ")\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "n_test_files = n_val_files # Since we split val 50/50\n",
    "n_test_steps = n_test_files // batch_size\n",
    "\n",
    "test_results = deeplabv3.evaluate(test_image_ds, steps=n_test_steps)\n",
    "test_res_dict = dict(zip(deeplabv3.metrics_names, test_results))\n",
    "print(\"Test Results:\")\n",
    "print(test_res_dict)\n",
    "\n",
    "# Visualize predictions (based on 8.6)\n",
    "print(\"\\nVisualizing 2 test predictions...\")\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i, (x, y) in enumerate(test_image_ds.take(2)):\n",
    "    y_pred = deeplabv3.predict(x)\n",
    "    y_pred_argmax = tf.argmax(y_pred, axis=-1)\n",
    "    \n",
    "    # Original Image\n",
    "    plt.subplot(2, 3, i*3 + 1)\n",
    "    plt.imshow(x[0])\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Ground Truth Mask\n",
    "    plt.subplot(2, 3, i*3 + 2)\n",
    "    plt.imshow(y[0], vmin=0, vmax=num_classes-1)\n",
    "    plt.title(\"Ground Truth Mask\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Predicted Mask\n",
    "    plt.subplot(2, 3, i*3 + 3)\n",
    "    plt.imshow(y_pred_argmax[0], vmin=0, vmax=num_classes-1)\n",
    "    plt.title(\"Predicted Mask\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
