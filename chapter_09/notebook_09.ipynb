{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut3QjWd7wdny"
      },
      "source": [
        "# Chapter 9: Natural language processing with TensorFlow: Sentiment analysis\n",
        "\n",
        "This notebook reproduces the code and summarizes the theoretical concepts from Chapter 9 of *'TensorFlow in Action'* by Thushan Ganegedara.\n",
        "\n",
        "This chapter builds a sentiment analyzer for video game reviews. The key topics covered are:\n",
        "1.  **Text Preprocessing**: Cleaning and preparing raw text data for a model using NLTK.\n",
        "2.  **Data Analysis**: Analyzing vocabulary size and sequence length to inform model design.\n",
        "3.  **Data Pipelines**: Using Keras `Tokenizer` and the `tf.data` API to create an efficient pipeline for text.\n",
        "4.  **Sequential Modeling**: Implementing a **Long Short-Term Memory (LSTM)** network to classify sentiment.\n",
        "5.  **Word Embeddings**: Improving the model by replacing one-hot encoding with a trainable `Embedding` layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2ZPpYlywdoG"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63bsjMF3wdoH"
      },
      "source": [
        "## 9.1 What the text? Exploring and processing text\n",
        "\n",
        "The first step is to load our data and perform **Exploratory Data Analysis (EDA)**. We will be using the \"Video Games 5-core\" dataset from Amazon reviews. This dataset is in JSON format.\n",
        "\n",
        "We will perform the following preprocessing steps:\n",
        "1.  Load the data using `pandas`.\n",
        "2.  Filter for **verified reviews** only, to ensure data quality.\n",
        "3.  Map the 1-5 star ratings (`overall`) to a binary sentiment label: `1` (positive) for 4-5 stars, and `0` (negative) for 1-3 stars.\n",
        "4.  Observe the **class imbalance** (many more positive reviews than negative ones)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSJip96WwdoH",
        "outputId": "dc0b6e4a-70b9-4030-a1f2-c57a08d1313a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data already downloaded and extracted.\n",
            "\n",
            "Class distribution:\n",
            "label\n",
            "1    277213\n",
            "0     55291\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import gzip\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "random_seed = 42\n",
        "np.random.seed(random_seed)\n",
        "tf.random.set_seed(random_seed)\n",
        "\n",
        "# --- 1. Download and Extract Data (Listing 9.1) ---\n",
        "data_dir = 'data'\n",
        "json_gz_path = os.path.join(data_dir, 'Video_Games_5.json.gz')\n",
        "json_path = os.path.join(data_dir, 'Video_Games_5.json')\n",
        "\n",
        "if not os.path.exists(json_path):\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "    if not os.path.exists(json_gz_path):\n",
        "        print(\"Downloading Video_Games_5.json.gz (50MB)...\")\n",
        "        url = \"https://gitlab.eecs.wsu.edu/2018080000/475datascience/-/raw/master/Video_Games_5.json.gz\"\n",
        "        r = requests.get(url)\n",
        "        with open(json_gz_path, 'wb') as f:\n",
        "            f.write(r.content)\n",
        "        print(\"Download complete.\")\n",
        "\n",
        "    print(\"Extracting data...\")\n",
        "    with gzip.open(json_gz_path, 'rb') as f_in:\n",
        "        with open(json_path, 'wb') as f_out:\n",
        "            shutil.copyfileobj(f_in, f_out)\n",
        "    print(\"Extraction complete.\")\n",
        "else:\n",
        "    print(\"Data already downloaded and extracted.\")\n",
        "\n",
        "# --- 2. Load and Pre-filter Data ---\n",
        "review_df = pd.read_json(json_path, lines=True, orient='records')\n",
        "review_df = review_df[[\"overall\", \"verified\", \"reviewTime\", \"reviewText\"]]\n",
        "\n",
        "# Remove records with empty reviewText\n",
        "review_df = review_df[~review_df[\"reviewText\"].isna()]\n",
        "review_df = review_df[review_df[\"reviewText\"].str.strip().str.len() > 0]\n",
        "\n",
        "# Filter for verified reviews only\n",
        "verified_df = review_df.loc[review_df[\"verified\"], :].copy()\n",
        "\n",
        "# --- 3. Map to Binary Labels ---\n",
        "verified_df[\"label\"] = verified_df[\"overall\"].map({5: 1, 4: 1, 3: 0, 2: 0, 1: 0})\n",
        "\n",
        "# --- 4. Observe Class Imbalance ---\n",
        "print(\"\\nClass distribution:\")\n",
        "print(verified_df[\"label\"].value_counts())\n",
        "\n",
        "# Shuffle and separate inputs and labels\n",
        "verified_df = verified_df.sample(frac=1.0, random_state=random_seed)\n",
        "inputs, labels = verified_df[\"reviewText\"], verified_df[\"label\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxnYDHnMwdoU"
      },
      "source": [
        "### Text Cleaning with NLTK\n",
        "\n",
        "Raw text is very noisy. We will use the **Natural Language Toolkit (NLTK)** library to clean it. Our `clean_text` function will perform several key operations:\n",
        "\n",
        "1.  **Lowercase**: Converts all text to lowercase.\n",
        "2.  **Contraction Expansion**: Expands `n't` to ` not ` (crucial for sentiment).\n",
        "3.  **Tokenization**: Splits the text into a list of individual words (tokens).\n",
        "4.  **Stop Word Removal**: Removes common, uninformative words like \"the\", \"a\", \"is\". We explicitly *keep* \"not\" and \"no\" as they are vital for sentiment.\n",
        "5.  **Punctuation/Number Removal**: Removes all punctuation and digits.\n",
        "6.  **Lemmatization**: Reduces words to their base or dictionary form (e.g., \"walking\" -> \"walk\", \"was\" -> \"be\"). This requires Part-of-Speech (PoS) tagging to know if a word is a noun, verb, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJOlnyuHwdoV",
        "outputId": "fafc5a80-facf-41e0-8d4c-2d0bc63398b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: It’s an okay game. I am always dying, which depresses me. I can't play this.\n",
            "Cleaned: ['’', 'okay', 'game', 'always', 'die', 'depresses', 'ca', 'not', 'play']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to nltk...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     nltk...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to nltk...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to nltk...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to nltk...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to nltk...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to nltk...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# Create NLTK directory if it doesn't exist\n",
        "nltk_dir = os.path.abspath('nltk')\n",
        "os.makedirs(nltk_dir, exist_ok=True) # Ensure directory exists\n",
        "\n",
        "# Append to NLTK data path so NLTK knows where to look for downloaded data\n",
        "nltk.data.path.append(nltk_dir)\n",
        "\n",
        "# Download NLTK resources needed for tokenizing, PoS tagging, and lemmatizing\n",
        "# NLTK's download function intelligently skips already downloaded resources.\n",
        "nltk.download('averaged_perceptron_tagger', download_dir='nltk')\n",
        "nltk.download('averaged_perceptron_tagger_eng', download_dir='nltk') # Added specific tagger\n",
        "nltk.download('wordnet', download_dir='nltk')\n",
        "nltk.download('omw-1.4', download_dir='nltk')\n",
        "nltk.download('stopwords', download_dir='nltk')\n",
        "nltk.download('punkt', download_dir='nltk')\n",
        "nltk.download('punkt_tab', download_dir='nltk')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Keep 'not' and 'no' as they are important for sentiment\n",
        "EN_STOPWORDS = set(stopwords.words('english')) - {'not', 'no'}\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define the text cleaning function (based on Listing 9.2)\n",
        "def clean_text(doc):\n",
        "    \"\"\"Cleans a given text document.\"\"\"\n",
        "\n",
        "    doc = doc.lower()\n",
        "    doc = doc.replace(\"n't \", ' not ')\n",
        "    doc = re.sub(r\"(?:'ll |'re |'d |'ve )\", \" \", doc)\n",
        "    doc = re.sub(r\"\\d+\", \"\", doc)\n",
        "\n",
        "    # Tokenize and remove stopwords/punctuation\n",
        "    tokens = [w for w in word_tokenize(doc)\n",
        "              if w not in EN_STOPWORDS and w not in string.punctuation]\n",
        "\n",
        "    # Get Part-of-Speech tags\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "    # Lemmatize based on PoS tag (only for Nouns and Verbs)\n",
        "    clean_text = [\n",
        "        lemmatizer.lemmatize(w, pos=p[0].lower()) if p[0] in ['N', 'V'] else w\n",
        "        for (w, p) in pos_tags\n",
        "    ]\n",
        "    return clean_text\n",
        "\n",
        "# Test the function\n",
        "sample_doc = \"It\\u2019s an okay game. I am always dying, which depresses me. I can't play this.\"\n",
        "print(f\"Original: {sample_doc}\")\n",
        "print(f\"Cleaned: {clean_text(sample_doc)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ie_IGl0AwdoY",
        "outputId": "1f458098-2035-4d12-a18c-1da60a7eea6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pre-cleaned data...\n",
            "\n",
            "First 5 cleaned reviews:\n",
            "128841    [seem, bad, luck, play, charge, cable, work, s...\n",
            "422950    [great, stick, box, everything, fine, no, dama...\n",
            "403141                                    [perfect, thanks]\n",
            "268147    [three, different, grip, ps, vita, favorite, h...\n",
            "183205    [son, want, christmas, not, search, around, to...\n",
            "Name: reviewText, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# This step can take a long time (up to an hour)\n",
        "cleaned_inputs_path = os.path.join(data_dir, 'sentiment_inputs.pkl')\n",
        "labels_path = os.path.join(data_dir, 'sentiment_labels.pkl')\n",
        "\n",
        "if not os.path.exists(cleaned_inputs_path):\n",
        "    print(\"Cleaning all review texts... (This may take a long time)\")\n",
        "    inputs = inputs.apply(lambda x: clean_text(x))\n",
        "    # Save the cleaned data to avoid re-running this step\n",
        "    inputs.to_pickle(cleaned_inputs_path)\n",
        "    labels.to_pickle(labels_path)\n",
        "    print(\"Cleaning and saving complete.\")\n",
        "else:\n",
        "    print(\"Loading pre-cleaned data...\")\n",
        "    inputs = pd.read_pickle(cleaned_inputs_path)\n",
        "    labels = pd.read_pickle(labels_path)\n",
        "\n",
        "print(\"\\nFirst 5 cleaned reviews:\")\n",
        "print(inputs.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UZyh6CFwdoc"
      },
      "source": [
        "## 9.2 Getting text ready for the model\n",
        "\n",
        "Now that our text is clean, we need to convert it into numbers. Before we do, we must:\n",
        "1.  **Split Data**: Create training, validation, and test sets. **Crucially**, we will create *balanced* validation and test sets to get a reliable accuracy score. The remaining data (which will be imbalanced) will be our training set.\n",
        "2.  **Analyze Training Data**: We will analyze the *training data only* to find our `n_vocab` (vocabulary size) and decide on sequence lengths. Analyzing only the training data prevents **data leakage**.\n",
        "3.  **Tokenize**: Use the Keras `Tokenizer` to build a vocabulary (word -> ID mapping) from the training data and then convert all three datasets into sequences of integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwJPYn0bwdod",
        "outputId": "46847580-b1a7-4995-d10b-be7d2e069f6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data: 310388 (Imbalanced)\n",
            "label\n",
            "1    266155\n",
            "0     44233\n",
            "Name: count, dtype: int64\n",
            "Validation data: 11058 (Balanced)\n",
            "label\n",
            "1    5529\n",
            "0    5529\n",
            "Name: count, dtype: int64\n",
            "Test data: 11058 (Balanced)\n",
            "label\n",
            "1    5529\n",
            "0    5529\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Vocabulary size (words appearing >= 25 times): 11388\n",
            "\n",
            "Sequence Length Statistics (90% of data):\n",
            "count    278577.000000\n",
            "mean         15.258334\n",
            "std          16.047219\n",
            "min           1.000000\n",
            "33%           5.000000\n",
            "50%          10.000000\n",
            "66%          16.000000\n",
            "max          73.000000\n",
            "Name: reviewText, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# 1. Split data (based on Listing 9.3)\n",
        "def train_valid_test_split(inputs, labels, train_fraction=0.8):\n",
        "    neg_indices = pd.Series(labels.loc[(labels == 0)].index)\n",
        "    pos_indices = pd.Series(labels.loc[(labels == 1)].index)\n",
        "\n",
        "    # Create balanced validation and test sets\n",
        "    n_valid = int(min([len(neg_indices), len(pos_indices)]) * ((1 - train_fraction) / 2.0))\n",
        "    n_test = n_valid\n",
        "\n",
        "    neg_test_inds = neg_indices.sample(n=n_test, random_state=random_seed)\n",
        "    neg_valid_inds = neg_indices.loc[~neg_indices.isin(neg_test_inds)].sample(n=n_test, random_state=random_seed)\n",
        "    neg_train_inds = neg_indices.loc[~neg_indices.isin(neg_test_inds.tolist() + neg_valid_inds.tolist())]\n",
        "\n",
        "    pos_test_inds = pos_indices.sample(n=n_test, random_state=random_seed)\n",
        "    pos_valid_inds = pos_indices.loc[~pos_indices.isin(pos_test_inds)].sample(n=n_test, random_state=random_seed)\n",
        "    pos_train_inds = pos_indices.loc[~pos_indices.isin(pos_test_inds.tolist() + pos_valid_inds.tolist())]\n",
        "\n",
        "    tr_x = inputs.loc[neg_train_inds.tolist() + pos_train_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
        "    tr_y = labels.loc[neg_train_inds.tolist() + pos_train_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
        "    v_x = inputs.loc[neg_valid_inds.tolist() + pos_valid_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
        "    v_y = labels.loc[neg_valid_inds.tolist() + pos_valid_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
        "    ts_x = inputs.loc[neg_test_inds.tolist() + pos_test_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
        "    ts_y = labels.loc[neg_test_inds.tolist() + pos_test_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
        "\n",
        "    print(f\"Training data: {len(tr_x)} (Imbalanced)\")\n",
        "    print(tr_y.value_counts())\n",
        "    print(f\"Validation data: {len(v_x)} (Balanced)\")\n",
        "    print(v_y.value_counts())\n",
        "    print(f\"Test data: {len(ts_x)} (Balanced)\")\n",
        "    print(ts_y.value_counts())\n",
        "\n",
        "    return (tr_x, tr_y), (v_x, v_y), (ts_x, ts_y)\n",
        "\n",
        "(tr_x, tr_y), (v_x, v_y), (ts_x, ts_y) = train_valid_test_split(inputs, labels, train_fraction=0.8)\n",
        "\n",
        "# 2. Analyze Vocabulary\n",
        "data_list = [w for doc in tr_x for w in doc]\n",
        "cnt = Counter(data_list)\n",
        "freq_df = pd.Series(list(cnt.values()), index=list(cnt.keys())).sort_values(ascending=False)\n",
        "\n",
        "# We'll set our vocabulary to words that appear at least 25 times\n",
        "n_vocab = (freq_df >= 25).sum()\n",
        "print(f\"\\nVocabulary size (words appearing >= 25 times): {n_vocab}\")\n",
        "\n",
        "# 3. Analyze Sequence Length\n",
        "seq_length_ser = tr_x.str.len()\n",
        "print(\"\\nSequence Length Statistics (90% of data):\")\n",
        "p_10 = seq_length_ser.quantile(0.1)\n",
        "p_90 = seq_length_ser.quantile(0.9)\n",
        "print(seq_length_ser[(seq_length_ser >= p_10) & (seq_length_ser < p_90)].describe(percentiles=[0.33, 0.66]))\n",
        "# We will use [5, 15] as bucket boundaries based on the 33% and 66% percentiles.\n",
        "bucket_boundaries = [5, 15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-6aQOVqwdof",
        "outputId": "f7c10710-75b6-491b-fbd3-2a6f55ed3378"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example of original text:\n",
            "['fun', 'fun', 'fun']\n",
            "\n",
            "Example of tokenized sequence:\n",
            "[15, 15, 15]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# 4. Tokenize the text\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=n_vocab, # Use the vocab size we calculated\n",
        "    oov_token='unk'    # Token for out-of-vocabulary words\n",
        ")\n",
        "\n",
        "# Fit the tokenizer on the training data ONLY\n",
        "tokenizer.fit_on_texts(tr_x.tolist())\n",
        "\n",
        "# Convert all three datasets to integer sequences\n",
        "tr_x_seq = tokenizer.texts_to_sequences(tr_x.tolist())\n",
        "v_x_seq = tokenizer.texts_to_sequences(v_x.tolist())\n",
        "ts_x_seq = tokenizer.texts_to_sequences(ts_x.tolist())\n",
        "\n",
        "print(\"\\nExample of original text:\")\n",
        "print(tr_x.iloc[0])\n",
        "print(\"\\nExample of tokenized sequence:\")\n",
        "print(tr_x_seq[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV_XJvclwdog"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuS_Di5awdoh"
      },
      "source": [
        "## 9.3 Defining an end-to-end NLP pipeline with TensorFlow\n",
        "\n",
        "We now have lists of variable-length integer sequences. To feed this to a model efficiently, we use `tf.data`.\n",
        "\n",
        "Our pipeline will:\n",
        "1.  Use `tf.ragged.constant` to handle the variable-length sequences.\n",
        "2.  Use `tf.data.Dataset.from_tensor_slices` to create a dataset.\n",
        "3.  Use `tf.data.experimental.bucket_by_sequence_length` to group sequences of similar lengths into batches. This is **much more efficient** than padding all sentences to the same maximum length, as it minimizes the amount of padding per batch.\n",
        "4.  Split the data into `(inputs, labels)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YioY6F7Dwdoh",
        "outputId": "b5b80986-d65a-421d-c77a-a6cc2b1ba142"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X batch shape: (128, 3)\n",
            "Y batch shape: (128,)\n",
            "Example X: [63  0  0]...\n",
            "Example Y: 1\n"
          ]
        }
      ],
      "source": [
        "# Define the pipeline function (based on Listing 9.4)\n",
        "def get_tf_pipeline(text_seq, labels, batch_size=64, bucket_boundaries=[5, 15], max_length=50, shuffle=False):\n",
        "\n",
        "    # 1. Combine labels and inputs (labels are prepended)\n",
        "    data_seq = [[b] + a for a, b in zip(text_seq, labels.values)]\n",
        "\n",
        "    # 2. Create a RaggedTensor and truncate to max_length\n",
        "    tf_data = tf.ragged.constant(data_seq, dtype=tf.int32)[:, :max_length]\n",
        "\n",
        "    # 3. Create the dataset\n",
        "    text_ds = tf.data.Dataset.from_tensor_slices(tf_data)\n",
        "\n",
        "    # 4. Filter empty sequences\n",
        "    text_ds = text_ds.filter(lambda x: tf.size(x) > 1)\n",
        "\n",
        "    # 5. Define the bucketing function\n",
        "    bucket_fn = tf.data.experimental.bucket_by_sequence_length(\n",
        "        element_length_func=lambda x: tf.cast(tf.shape(x)[0], 'int32'),\n",
        "        bucket_boundaries=bucket_boundaries,\n",
        "        bucket_batch_sizes=[batch_size] * (len(bucket_boundaries) + 1),\n",
        "        padding_values=0,\n",
        "        pad_to_bucket_boundary=False\n",
        "    )\n",
        "\n",
        "    # 6. Apply bucketing and shuffling\n",
        "    text_ds = text_ds.map(lambda x: x).apply(bucket_fn)\n",
        "    if shuffle:\n",
        "        text_ds = text_ds.shuffle(buffer_size=10 * batch_size)\n",
        "\n",
        "    # 7. Split back into (inputs, labels)\n",
        "    # x[:, 0] is the label, x[:, 1:] is the input sequence\n",
        "    text_ds = text_ds.map(lambda x: (x[:, 1:], x[:, 0]))\n",
        "\n",
        "    return text_ds\n",
        "\n",
        "# Create the train and validation datasets\n",
        "batch_size = 128\n",
        "train_ds = get_tf_pipeline(tr_x_seq, tr_y, batch_size=batch_size, shuffle=True)\n",
        "valid_ds = get_tf_pipeline(v_x_seq, v_y, batch_size=batch_size)\n",
        "test_ds = get_tf_pipeline(ts_x_seq, ts_y, batch_size=batch_size)\n",
        "\n",
        "# Inspect a batch\n",
        "for x_batch, y_batch in train_ds.take(1):\n",
        "    print(f\"X batch shape: {x_batch.shape}\")\n",
        "    print(f\"Y batch shape: {y_batch.shape}\")\n",
        "    print(f\"Example X: {x_batch[0, :10]}...\")\n",
        "    print(f\"Example Y: {y_batch[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EScdFDKwdoj"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZKiUvfqwdoj"
      },
      "source": [
        "## 9.4 & 9.5: Sentiment Analysis Model (LSTM) & Training\n",
        "\n",
        "We will now build our sentiment analysis model using an **LSTM (Long Short-Term Memory)** network.\n",
        "\n",
        "**Why LSTM?** LSTMs are a type of RNN designed to overcome the vanishing gradient problem. They can learn long-range dependencies (e.g., remember the word \"not\" from the beginning of a long review) by using a series of \"gates\" (input, forget, output) to control a persistent cell state (its memory).\n",
        "\n",
        "Our first model (Listing 9.5) will use:\n",
        "1.  A `Masking` layer to ignore the `0` padding values.\n",
        "2.  A custom `OnehotEncoder` layer to convert integer IDs to one-hot vectors.\n",
        "3.  An `LSTM` layer.\n",
        "4.  `Dense` layers for classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "id": "ciXVl8Mcwdok",
        "outputId": "9875f854-17b3-4ee5-f784-ff820e22727b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/masking.py:48: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ masking (\u001b[38;5;33mMasking\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ onehot_encoder (\u001b[38;5;33mOnehotEncoder\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11389\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m5,897,216\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │        \u001b[38;5;34m66,048\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m513\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ masking (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ onehot_encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">OnehotEncoder</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11389</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,897,216</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,963,777\u001b[0m (22.75 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,963,777</span> (22.75 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,963,777\u001b[0m (22.75 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,963,777</span> (22.75 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Negative class weight: 6.02\n",
            "\n",
            "Training model with One-Hot Encoder...\n",
            "Epoch 1/2\n",
            "   2423/Unknown \u001b[1m4794s\u001b[0m 2s/step - accuracy: 0.7844 - loss: 0.8036"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2423/2423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4896s\u001b[0m 2s/step - accuracy: 0.7845 - loss: 0.8036 - val_accuracy: 0.8171 - val_loss: 0.3986 - learning_rate: 0.0010\n",
            "Epoch 2/2\n",
            "\u001b[1m2423/2423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4897s\u001b[0m 2s/step - accuracy: 0.8473 - loss: 0.5974 - val_accuracy: 0.8342 - val_loss: 0.3779 - learning_rate: 0.0010\n",
            "\n",
            "Evaluating One-Hot model on test set...\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 1s/step - accuracy: 0.8273 - loss: 0.3859\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.37712302803993225, 0.8323673009872437]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
        "from tensorflow.keras import backend as K_\n",
        "\n",
        "# Custom OnehotEncoder layer (Listing 9.5)\n",
        "class OnehotEncoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, depth, **kwargs):\n",
        "        super(OnehotEncoder, self).__init__(**kwargs)\n",
        "        self.depth = depth\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = tf.cast(inputs, 'int32')\n",
        "        if len(inputs.shape) == 3:\n",
        "            inputs = inputs[:, :, 0]\n",
        "        return tf.one_hot(inputs, depth=self.depth)\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return mask\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({'depth': self.depth})\n",
        "        return config\n",
        "\n",
        "K_.clear_session()\n",
        "\n",
        "# 1. Define the model (Listing 9.5)\n",
        "model_onehot = tf.keras.models.Sequential([\n",
        "    # Note: The input_shape is (None, 1) - (sequence_length, features)\n",
        "    # We use (None,) for variable sequence length, but the pipeline adds the feature dim.\n",
        "    # For simplicity, we'll let the model infer from the first batch,\n",
        "    # or specify input_shape=(None,) for the Embedding model later.\n",
        "    # Here, we specify the input_shape required by the Masking layer\n",
        "    tf.keras.layers.Masking(mask_value=0.0, input_shape=(None, 1)),\n",
        "    OnehotEncoder(depth=n_vocab + 1), # +1 for the 'unk' token\n",
        "    tf.keras.layers.LSTM(128, return_state=False, return_sequences=False),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid') # Binary classification\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model_onehot.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_onehot.summary()\n",
        "\n",
        "# 3. Calculate class weight to handle imbalance\n",
        "neg_weight = (tr_y == 1).sum() / (tr_y == 0).sum()\n",
        "print(f\"\\nNegative class weight: {neg_weight:.2f}\")\n",
        "\n",
        "# 4. Define callbacks\n",
        "os.makedirs('eval', exist_ok=True)\n",
        "csv_logger = CSVLogger(os.path.join('eval', '1_sentiment_analysis_onehot.log'))\n",
        "es_callback = EarlyStopping(monitor='val_loss', patience=6, mode='min')\n",
        "lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, mode='min')\n",
        "\n",
        "# 5. Train the model (Listing 9.6)\n",
        "print(\"\\nTraining model with One-Hot Encoder...\")\n",
        "history_onehot = model_onehot.fit(\n",
        "    train_ds,\n",
        "    validation_data=valid_ds,\n",
        "    epochs=2, # Book runs for 10, we'll use 2 for speed\n",
        "    class_weight={0: neg_weight, 1: 1.0},\n",
        "    callbacks=[es_callback, lr_callback, csv_logger]\n",
        ")\n",
        "\n",
        "# 6. Evaluate\n",
        "print(\"\\nEvaluating One-Hot model on test set...\")\n",
        "model_onehot.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hriMUQQpwdom"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWmsQFZ8wdon"
      },
      "source": [
        "## 9.6 Injecting Semantics with Word Embeddings\n",
        "\n",
        "One-hot encoding is simple but has two major flaws:\n",
        "1.  **Inefficient**: The vectors are huge (size `n_vocab`) and sparse (mostly zeros).\n",
        "2.  **Lacks Semantics**: The vectors for \"good\" and \"great\" are just as different as the vectors for \"good\" and \"terrible\". The model has to learn all semantic relationships from scratch.\n",
        "\n",
        "**Word Embeddings** solve this. An `Embedding` layer is a lookup table that maps each integer ID to a dense, low-dimensional vector (e.g., 128 dimensions). These vectors are *trainable*, so the model learns to place words with similar meanings close together in the vector space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Nx9pjUBBwdou",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        },
        "outputId": "16eae40b-df1d-460f-e97c-8fa0a97130e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:100: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │     \u001b[38;5;34m1,457,792\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │        \u001b[38;5;34m66,048\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m513\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,457,792</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,655,937\u001b[0m (6.32 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,655,937</span> (6.32 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,655,937\u001b[0m (6.32 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,655,937</span> (6.32 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model with Embedding Layer...\n",
            "Epoch 1/2\n",
            "   2422/Unknown \u001b[1m326s\u001b[0m 129ms/step - accuracy: 0.7650 - loss: 0.7957"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2423/2423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 132ms/step - accuracy: 0.7650 - loss: 0.7956 - val_accuracy: 0.8318 - val_loss: 0.3820 - learning_rate: 0.0010\n",
            "Epoch 2/2\n",
            "\u001b[1m2423/2423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 130ms/step - accuracy: 0.8480 - loss: 0.5933 - val_accuracy: 0.8396 - val_loss: 0.3718 - learning_rate: 0.0010\n",
            "\n",
            "Evaluating Embedding model on test set...\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 64ms/step - accuracy: 0.8330 - loss: 0.3796\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "K_.clear_session()\n",
        "\n",
        "# 1. Define the model with an Embedding layer (Listing 9.7)\n",
        "model_embed = tf.keras.models.Sequential([\n",
        "    # The Embedding layer takes integer IDs.\n",
        "    # input_dim = vocab size + 1 (for 0 padding)\n",
        "    # output_dim = size of the dense vector (e.g., 128)\n",
        "    # mask_zero=True automatically handles the padding (0) values.\n",
        "    tf.keras.layers.Embedding(input_dim=n_vocab + 1,\n",
        "                              output_dim=128,\n",
        "                              mask_zero=True,\n",
        "                              input_shape=(None,)), # (batch_size, sequence_length)\n",
        "\n",
        "    tf.keras.layers.LSTM(128, return_state=False, return_sequences=False),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model_embed.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_embed.summary()\n",
        "\n",
        "# 3. Define callbacks\n",
        "csv_logger_embed = CSVLogger(os.path.join('eval', '2_sentiment_analysis_embed.log'))\n",
        "es_callback_embed = EarlyStopping(monitor='val_loss', patience=6, mode='min')\n",
        "lr_callback_embed = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, mode='min')\n",
        "\n",
        "# 4. Train the model\n",
        "print(\"\\nTraining model with Embedding Layer...\")\n",
        "history_embed = model_embed.fit(\n",
        "    train_ds,\n",
        "    validation_data=valid_ds,\n",
        "    epochs=2, # Book runs for 10\n",
        "    class_weight={0: neg_weight, 1: 1.0},\n",
        "    callbacks=[es_callback_embed, lr_callback_embed, csv_logger_embed]\n",
        ")\n",
        "\n",
        "# 5. Evaluate\n",
        "print(\"\\nEvaluating Embedding model on test set...\")\n",
        "model_embed.evaluate(test_ds)\n",
        "\n",
        "# 6. Save the final model\n",
        "os.makedirs('models', exist_ok=True)\n",
        "model_embed.save(os.path.join('models', '1_sentiment_analysis_embed.h5'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "k9U61RS6wdov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd3dfda7-29de-4f06-af2e-c0a4198cbf1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Inspecting Model Predictions ---\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 49ms/step\n",
            "\n",
            "Most Negative Reviews (as predicted by model):\n",
            "==================================================\n",
            "Predicted: 0.000 | Actual: 0\n",
            "product not us region natively asian version pch- though normally play no region lock us account issue device deliver not us ver asian ver \n",
            "\n",
            "Predicted: 0.000 | Actual: 0\n",
            "game `` look '' amaze hardware gun play seriously `` meh '' game play `` meh '' \n",
            "\n",
            "Predicted: 0.000 | Actual: 0\n",
            "virus virus virus please careful completely shut pc buy new one \n",
            "\n",
            "Predicted: 0.000 | Actual: 0\n",
            "boring \n",
            "\n",
            "Predicted: 0.001 | Actual: 0\n",
            "'s good \n",
            "\n",
            "\n",
            "Most Positive Reviews (as predicted by model):\n",
            "==================================================\n",
            "Predicted: 1.000 | Actual: 0\n",
            "not like expect something like pacman come playstation disappoint not make pacman/sonic game like use ... \n",
            "\n",
            "Predicted: 1.000 | Actual: 1\n",
            "game new describe accept get platinum hit picture show original kinda suck still good deal \n",
            "\n",
            "Predicted: 1.000 | Actual: 1\n",
            "work good playing could not seem keep attention not seem fun expect \n",
            "\n",
            "Predicted: 1.000 | Actual: 0\n",
            "armband complete joke think design people anorexia anyone amount flesh bone trouble get forearm forget upper arm -- 's impossible thing design super tiny arm people simply not also no quick convenient way get wii-mote struggle get wii-mote elastic strap defeat whole purpose armband first place 's elastic strap go remote interfere remote 's sending ability make hit `` next '' `` start '' button confirmation difficult not make mistake thing not worth money guess keep set remote picking -- might actually burn calorie process \n",
            "\n",
            "Predicted: 1.000 | Actual: 0\n",
            "take extremly long download still probelms decent game \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 7. Inspect predictions\n",
        "print(\"\\n--- Inspecting Model Predictions ---\")\n",
        "test_x_list = ts_x.tolist()\n",
        "test_y_list = ts_y.tolist()\n",
        "\n",
        "# Get all predictions from the test set\n",
        "test_ds_unbatched = get_tf_pipeline(ts_x_seq, ts_y, batch_size=128)\n",
        "test_pred = model_embed.predict(test_ds_unbatched)\n",
        "\n",
        "# Get indices of the sorted predictions\n",
        "sorted_pred_idx = np.argsort(test_pred.flatten())\n",
        "\n",
        "# Get top 5 most negative reviews\n",
        "min_pred_idx = sorted_pred_idx[:5]\n",
        "print(\"\\nMost Negative Reviews (as predicted by model):\")\n",
        "print(\"=\"*50)\n",
        "for i in min_pred_idx:\n",
        "    print(f\"Predicted: {test_pred[i][0]:.3f} | Actual: {test_y_list[i]}\")\n",
        "    print(\" \".join(test_x_list[i]), \"\\n\")\n",
        "\n",
        "# Get top 5 most positive reviews\n",
        "max_pred_idx = sorted_pred_idx[-5:]\n",
        "print(\"\\nMost Positive Reviews (as predicted by model):\")\n",
        "print(\"=\"*50)\n",
        "for i in max_pred_idx:\n",
        "    print(f\"Predicted: {test_pred[i][0]:.3f} | Actual: {test_y_list[i]}\")\n",
        "    print(\" \".join(test_x_list[i]), \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e38ccf3"
      },
      "source": [
        "# Task\n",
        "The `punkt_tab` NLTK resource needs to be downloaded to resolve the `LookupError`. The current code only attempts to download it if the NLTK data directory does not exist, which can lead to issues if the directory exists but the resource is missing.\n",
        "\n",
        "To fix this, I will modify the code cell `xJOlnyuHwdoV` to ensure that `nltk.download('punkt_tab', download_dir='nltk')` (along with other necessary NLTK downloads) is always attempted, relying on NLTK's internal checks to skip re-downloading already present resources. I will also ensure the NLTK data path is appended before the downloads.\n",
        "\n",
        "```python\n",
        "# Create NLTK directory if it doesn't exist\n",
        "nltk_dir = os.path.abspath('nltk')\n",
        "os.makedirs(nltk_dir, exist_ok=True) # Ensure directory exists\n",
        "\n",
        "# Append to NLTK data path so NLTK knows where to look for downloaded data\n",
        "nltk.data.path.append(nltk_dir)\n",
        "\n",
        "# Download NLTK resources needed for tokenizing, PoS tagging, and lemmatizing\n",
        "# NLTK's download function intelligently skips already downloaded resources.\n",
        "nltk.download('averaged_perceptron_tagger', download_dir='nltk')\n",
        "nltk.download('wordnet', download_dir='nltk')\n",
        "nltk.download('omw-1.4', download_dir='nltk')\n",
        "nltk.download('stopwords', download_dir='nltk')\n",
        "nltk.download('punkt', download_dir='nltk')\n",
        "nltk.download('punkt_tab', download_dir='nltk')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "612f37bd"
      },
      "source": [
        "## modify_cells\n",
        "\n",
        "### Subtask:\n",
        "Modify the code cell to include the download of the missing 'punkt_tab' NLTK resource.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e861046f"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   A `LookupError` for the `punkt_tab` NLTK resource was identified as the core problem. This error occurred because the original code only attempted to download NLTK resources if the NLTK data directory did not exist, leading to issues if the directory was present but the specific resource was missing.\n",
        "*   The solution involved modifying the code to unconditionally attempt the download of `punkt_tab` and other necessary NLTK resources (`averaged_perceptron_tagger`, `wordnet`, `omw-1.4`, `stopwords`, `punkt`).\n",
        "*   The `nltk.data.path` was appended with the specified NLTK download directory (`nltk`) to ensure NLTK correctly locates the downloaded resources.\n",
        "*   The `os.makedirs(nltk_dir, exist_ok=True)` function was used to create the NLTK directory if it didn't already exist, ensuring a valid path for downloads.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   By ensuring NLTK downloads are always attempted and the data path is correctly configured, the system becomes more robust against missing NLTK resources, regardless of the initial directory state.\n",
        "*   The solution leverages NLTK's built-in intelligence to skip re-downloading already present resources, making the process efficient despite the unconditional download attempts.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}