{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9: Natural language processing with TensorFlow: Sentiment analysis\n",
    "\n",
    "This notebook reproduces the code and summarizes the theoretical concepts from Chapter 9 of *'TensorFlow in Action'* by Thushan Ganegedara.\n",
    "\n",
    "This chapter builds a sentiment analyzer for video game reviews. The key topics covered are:\n",
    "1.  **Text Preprocessing**: Cleaning and preparing raw text data for a model using NLTK.\n",
    "2.  **Data Analysis**: Analyzing vocabulary size and sequence length to inform model design.\n",
    "3.  **Data Pipelines**: Using Keras `Tokenizer` and the `tf.data` API to create an efficient pipeline for text.\n",
    "4.  **Sequential Modeling**: Implementing a **Long Short-Term Memory (LSTM)** network to classify sentiment.\n",
    "5.  **Word Embeddings**: Improving the model by replacing one-hot encoding with a trainable `Embedding` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 What the text? Exploring and processing text\n",
    "\n",
    "The first step is to load our data and perform **Exploratory Data Analysis (EDA)**. We will be using the \"Video Games 5-core\" dataset from Amazon reviews. This dataset is in JSON format.\n",
    "\n",
    "We will perform the following preprocessing steps:\n",
    "1.  Load the data using `pandas`.\n",
    "2.  Filter for **verified reviews** only, to ensure data quality.\n",
    "3.  Map the 1-5 star ratings (`overall`) to a binary sentiment label: `1` (positive) for 4-5 stars, and `0` (negative) for 1-3 stars.\n",
    "4.  Observe the **class imbalance** (many more positive reviews than negative ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# --- 1. Download and Extract Data (Listing 9.1) ---\n",
    "data_dir = 'data'\n",
    "json_gz_path = os.path.join(data_dir, 'Video_Games_5.json.gz')\n",
    "json_path = os.path.join(data_dir, 'Video_Games_5.json')\n",
    "\n",
    "if not os.path.exists(json_path):\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    if not os.path.exists(json_gz_path):\n",
    "        print(\"Downloading Video_Games_5.json.gz (50MB)...\")\n",
    "        url = \"http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Video_Games_5.json.gz\"\n",
    "        r = requests.get(url)\n",
    "        with open(json_gz_path, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        print(\"Download complete.\")\n",
    "    \n",
    "    print(\"Extracting data...\")\n",
    "    with gzip.open(json_gz_path, 'rb') as f_in:\n",
    "        with open(json_path, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    print(\"Extraction complete.\")\n",
    "else:\n",
    "    print(\"Data already downloaded and extracted.\")\n",
    "\n",
    "# --- 2. Load and Pre-filter Data ---\n",
    "review_df = pd.read_json(json_path, lines=True, orient='records')\n",
    "review_df = review_df[[\"overall\", \"verified\", \"reviewTime\", \"reviewText\"]]\n",
    "\n",
    "# Remove records with empty reviewText\n",
    "review_df = review_df[~review_df[\"reviewText\"].isna()]\n",
    "review_df = review_df[review_df[\"reviewText\"].str.strip().str.len() > 0]\n",
    "\n",
    "# Filter for verified reviews only\n",
    "verified_df = review_df.loc[review_df[\"verified\"], :].copy()\n",
    "\n",
    "# --- 3. Map to Binary Labels ---\n",
    "verified_df[\"label\"] = verified_df[\"overall\"].map({5: 1, 4: 1, 3: 0, 2: 0, 1: 0})\n",
    "\n",
    "# --- 4. Observe Class Imbalance ---\n",
    "print(\"\\nClass distribution:\")\n",
    "print(verified_df[\"label\"].value_counts())\n",
    "\n",
    "# Shuffle and separate inputs and labels\n",
    "verified_df = verified_df.sample(frac=1.0, random_state=random_seed)\n",
    "inputs, labels = verified_df[\"reviewText\"], verified_df[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning with NLTK\n",
    "\n",
    "Raw text is very noisy. We will use the **Natural Language Toolkit (NLTK)** library to clean it. Our `clean_text` function will perform several key operations:\n",
    "\n",
    "1.  **Lowercase**: Converts all text to lowercase.\n",
    "2.  **Contraction Expansion**: Expands `n't` to ` not ` (crucial for sentiment).\n",
    "3.  **Tokenization**: Splits the text into a list of individual words (tokens).\n",
    "4.  **Stop Word Removal**: Removes common, uninformative words like \"the\", \"a\", \"is\". We explicitly *keep* \"not\" and \"no\" as they are vital for sentiment.\n",
    "5.  **Punctuation/Number Removal**: Removes all punctuation and digits.\n",
    "6.  **Lemmatization**: Reduces words to their base or dictionary form (e.g., \"walking\" -> \"walk\", \"was\" -> \"be\"). This requires Part-of-Speech (PoS) tagging to know if a word is a noun, verb, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download NLTK resources needed for tokenizing, PoS tagging, and lemmatizing\n",
    "nltk_dir = os.path.abspath('nltk')\n",
    "if not os.path.exists(nltk_dir):\n",
    "    os.makedirs(nltk_dir)\n",
    "    nltk.download('averaged_perceptron_tagger', download_dir='nltk')\n",
    "    nltk.download('wordnet', download_dir='nltk')\n",
    "    nltk.download('omw-1.4', download_dir='nltk')\n",
    "    nltk.download('stopwords', download_dir='nltk')\n",
    "    nltk.download('punkt', download_dir='nltk')\n",
    "\n",
    "nltk.data.path.append(nltk_dir)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Keep 'not' and 'no' as they are important for sentiment\n",
    "EN_STOPWORDS = set(stopwords.words('english')) - {'not', 'no'}\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define the text cleaning function (based on Listing 9.2)\n",
    "def clean_text(doc):\n",
    "    \"\"\"Cleans a given text document.\"\"\"\n",
    "    \n",
    "    doc = doc.lower()\n",
    "    doc = doc.replace(\"n't \", ' not ')\n",
    "    doc = re.sub(r\"(?:'ll |'re |'d |'ve )\", \" \", doc)\n",
    "    doc = re.sub(r\"\\d+\", \"\", doc)\n",
    "    \n",
    "    # Tokenize and remove stopwords/punctuation\n",
    "    tokens = [w for w in word_tokenize(doc) \n",
    "              if w not in EN_STOPWORDS and w not in string.punctuation]\n",
    "    \n",
    "    # Get Part-of-Speech tags\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # Lemmatize based on PoS tag (only for Nouns and Verbs)\n",
    "    clean_text = [\n",
    "        lemmatizer.lemmatize(w, pos=p[0].lower()) if p[0] in ['N', 'V'] else w\n",
    "        for (w, p) in pos_tags\n",
    "    ]\n",
    "    return clean_text\n",
    "\n",
    "# Test the function\n",
    "sample_doc = \"Itâ€™s an okay game. I am always dying, which depresses me. I can't play this.\"\n",
    "print(f\"Original: {sample_doc}\")\n",
    "print(f\"Cleaned: {clean_text(sample_doc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step can take a long time (up to an hour)\n",
    "cleaned_inputs_path = os.path.join(data_dir, 'sentiment_inputs.pkl')\n",
    "labels_path = os.path.join(data_dir, 'sentiment_labels.pkl')\n",
    "\n",
    "if not os.path.exists(cleaned_inputs_path):\n",
    "    print(\"Cleaning all review texts... (This may take a long time)\")\n",
    "    inputs = inputs.apply(lambda x: clean_text(x))\n",
    "    # Save the cleaned data to avoid re-running this step\n",
    "    inputs.to_pickle(cleaned_inputs_path)\n",
    "    labels.to_pickle(labels_path)\n",
    "    print(\"Cleaning and saving complete.\")\n",
    "else:\n",
    "    print(\"Loading pre-cleaned data...\")\n",
    "    inputs = pd.read_pickle(cleaned_inputs_path)\n",
    "    labels = pd.read_pickle(labels_path)\n",
    "\n",
    "print(\"\\nFirst 5 cleaned reviews:\")\n",
    "print(inputs.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Getting text ready for the model\n",
    "\n",
    "Now that our text is clean, we need to convert it into numbers. Before we do, we must:\n",
    "1.  **Split Data**: Create training, validation, and test sets. **Crucially**, we will create *balanced* validation and test sets to get a reliable accuracy score. The remaining data (which will be imbalanced) will be our training set.\n",
    "2.  **Analyze Training Data**: We will analyze the *training data only* to find our `n_vocab` (vocabulary size) and decide on sequence lengths. Analyzing only the training data prevents **data leakage**.\n",
    "3.  **Tokenize**: Use the Keras `Tokenizer` to build a vocabulary (word -> ID mapping) from the training data and then convert all three datasets into sequences of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split data (based on Listing 9.3)\n",
    "def train_valid_test_split(inputs, labels, train_fraction=0.8):\n",
    "    neg_indices = pd.Series(labels.loc[(labels == 0)].index)\n",
    "    pos_indices = pd.Series(labels.loc[(labels == 1)].index)\n",
    "    \n",
    "    # Create balanced validation and test sets\n",
    "    n_valid = int(min([len(neg_indices), len(pos_indices)]) * ((1 - train_fraction) / 2.0))\n",
    "    n_test = n_valid\n",
    "    \n",
    "    neg_test_inds = neg_indices.sample(n=n_test, random_state=random_seed)\n",
    "    neg_valid_inds = neg_indices.loc[~neg_indices.isin(neg_test_inds)].sample(n=n_test, random_state=random_seed)\n",
    "    neg_train_inds = neg_indices.loc[~neg_indices.isin(neg_test_inds.tolist() + neg_valid_inds.tolist())]\n",
    "    \n",
    "    pos_test_inds = pos_indices.sample(n=n_test, random_state=random_seed)\n",
    "    pos_valid_inds = pos_indices.loc[~pos_indices.isin(pos_test_inds)].sample(n=n_test, random_state=random_seed)\n",
    "    pos_train_inds = pos_indices.loc[~pos_indices.isin(pos_test_inds.tolist() + pos_valid_inds.tolist())]\n",
    "    \n",
    "    tr_x = inputs.loc[neg_train_inds.tolist() + pos_train_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    tr_y = labels.loc[neg_train_inds.tolist() + pos_train_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    v_x = inputs.loc[neg_valid_inds.tolist() + pos_valid_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    v_y = labels.loc[neg_valid_inds.tolist() + pos_valid_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    ts_x = inputs.loc[neg_test_inds.tolist() + pos_test_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    ts_y = labels.loc[neg_test_inds.tolist() + pos_test_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    \n",
    "    print(f\"Training data: {len(tr_x)} (Imbalanced)\")\n",
    "    print(tr_y.value_counts())\n",
    "    print(f\"Validation data: {len(v_x)} (Balanced)\")\n",
    "    print(v_y.value_counts())\n",
    "    print(f\"Test data: {len(ts_x)} (Balanced)\")\n",
    "    print(ts_y.value_counts())\n",
    "    \n",
    "    return (tr_x, tr_y), (v_x, v_y), (ts_x, ts_y)\n",
    "\n",
    "(tr_x, tr_y), (v_x, v_y), (ts_x, ts_y) = train_valid_test_split(inputs, labels, train_fraction=0.8)\n",
    "\n",
    "# 2. Analyze Vocabulary\n",
    "data_list = [w for doc in tr_x for w in doc]\n",
    "cnt = Counter(data_list)\n",
    "freq_df = pd.Series(list(cnt.values()), index=list(cnt.keys())).sort_values(ascending=False)\n",
    "\n",
    "# We'll set our vocabulary to words that appear at least 25 times\n",
    "n_vocab = (freq_df >= 25).sum()\n",
    "print(f\"\\nVocabulary size (words appearing >= 25 times): {n_vocab}\")\n",
    "\n",
    "# 3. Analyze Sequence Length\n",
    "seq_length_ser = tr_x.str.len()\n",
    "print(\"\\nSequence Length Statistics (90% of data):\")\n",
    "p_10 = seq_length_ser.quantile(0.1)\n",
    "p_90 = seq_length_ser.quantile(0.9)\n",
    "print(seq_length_ser[(seq_length_ser >= p_10) & (seq_length_ser < p_90)].describe(percentiles=[0.33, 0.66]))\n",
    "# We will use [5, 15] as bucket boundaries based on the 33% and 66% percentiles.\n",
    "bucket_boundaries = [5, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# 4. Tokenize the text\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=n_vocab, # Use the vocab size we calculated\n",
    "    oov_token='unk'    # Token for out-of-vocabulary words\n",
    ")\n",
    "\n",
    "# Fit the tokenizer on the training data ONLY\n",
    "tokenizer.fit_on_texts(tr_x.tolist())\n",
    "\n",
    "# Convert all three datasets to integer sequences\n",
    "tr_x_seq = tokenizer.texts_to_sequences(tr_x.tolist())\n",
    "v_x_seq = tokenizer.texts_to_sequences(v_x.tolist())\n",
    "ts_x_seq = tokenizer.texts_to_sequences(ts_x.tolist())\n",
    "\n",
    "print(\"\\nExample of original text:\")\n",
    "print(tr_x.iloc[0])\n",
    "print(\"\\nExample of tokenized sequence:\")\n",
    "print(tr_x_seq[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Defining an end-to-end NLP pipeline with TensorFlow\n",
    "\n",
    "We now have lists of variable-length integer sequences. To feed this to a model efficiently, we use `tf.data`.\n",
    "\n",
    "Our pipeline will:\n",
    "1.  Use `tf.ragged.constant` to handle the variable-length sequences.\n",
    "2.  Use `tf.data.Dataset.from_tensor_slices` to create a dataset.\n",
    "3.  Use `tf.data.experimental.bucket_by_sequence_length` to group sequences of similar lengths into batches. This is **much more efficient** than padding all sentences to the same maximum length, as it minimizes the amount of padding per batch.\n",
    "4.  Split the data into `(inputs, labels)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline function (based on Listing 9.4)\n",
    "def get_tf_pipeline(text_seq, labels, batch_size=64, bucket_boundaries=[5, 15], max_length=50, shuffle=False):\n",
    "    \n",
    "    # 1. Combine labels and inputs (labels are prepended)\n",
    "    data_seq = [[b] + a for a, b in zip(text_seq, labels.values)]\n",
    "    \n",
    "    # 2. Create a RaggedTensor and truncate to max_length\n",
    "    tf_data = tf.ragged.constant(data_seq, dtype=tf.int32)[:, :max_length]\n",
    "    \n",
    "    # 3. Create the dataset\n",
    "    text_ds = tf.data.Dataset.from_tensor_slices(tf_data)\n",
    "    \n",
    "    # 4. Filter empty sequences\n",
    "    text_ds = text_ds.filter(lambda x: tf.size(x) > 1)\n",
    "    \n",
    "    # 5. Define the bucketing function\n",
    "    bucket_fn = tf.data.experimental.bucket_by_sequence_length(\n",
    "        element_length_func=lambda x: tf.cast(tf.shape(x)[0], 'int32'),\n",
    "        bucket_boundaries=bucket_boundaries,\n",
    "        bucket_batch_sizes=[batch_size] * (len(bucket_boundaries) + 1),\n",
    "        padding_values=0,\n",
    "        pad_to_bucket_boundary=False\n",
    "    )\n",
    "    \n",
    "    # 6. Apply bucketing and shuffling\n",
    "    text_ds = text_ds.map(lambda x: x).apply(bucket_fn)\n",
    "    if shuffle:\n",
    "        text_ds = text_ds.shuffle(buffer_size=10 * batch_size)\n",
    "    \n",
    "    # 7. Split back into (inputs, labels)\n",
    "    # x[:, 0] is the label, x[:, 1:] is the input sequence\n",
    "    text_ds = text_ds.map(lambda x: (x[:, 1:], x[:, 0]))\n",
    "    \n",
    "    return text_ds\n",
    "\n",
    "# Create the train and validation datasets\n",
    "batch_size = 128\n",
    "train_ds = get_tf_pipeline(tr_x_seq, tr_y, batch_size=batch_size, shuffle=True)\n",
    "valid_ds = get_tf_pipeline(v_x_seq, v_y, batch_size=batch_size)\n",
    "test_ds = get_tf_pipeline(ts_x_seq, ts_y, batch_size=batch_size)\n",
    "\n",
    "# Inspect a batch\n",
    "for x_batch, y_batch in train_ds.take(1):\n",
    "    print(f\"X batch shape: {x_batch.shape}\")\n",
    "    print(f\"Y batch shape: {y_batch.shape}\")\n",
    "    print(f\"Example X: {x_batch[0, :10]}...\")\n",
    "    print(f\"Example Y: {y_batch[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 & 9.5: Sentiment Analysis Model (LSTM) & Training\n",
    "\n",
    "We will now build our sentiment analysis model using an **LSTM (Long Short-Term Memory)** network.\n",
    "\n",
    "**Why LSTM?** LSTMs are a type of RNN designed to overcome the vanishing gradient problem. They can learn long-range dependencies (e.g., remember the word \"not\" from the beginning of a long review) by using a series of \"gates\" (input, forget, output) to control a persistent cell state (its memory).\n",
    "\n",
    "Our first model (Listing 9.5) will use:\n",
    "1.  A `Masking` layer to ignore the `0` padding values.\n",
    "2.  A custom `OnehotEncoder` layer to convert integer IDs to one-hot vectors.\n",
    "3.  An `LSTM` layer.\n",
    "4.  `Dense` layers for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "\n",
    "# Custom OnehotEncoder layer (Listing 9.5)\n",
    "class OnehotEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, depth, **kwargs):\n",
    "        super(OnehotEncoder, self).__init__(**kwargs)\n",
    "        self.depth = depth\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        inputs = tf.cast(inputs, 'int32')\n",
    "        if len(inputs.shape) == 3:\n",
    "            inputs = inputs[:, :, 0]\n",
    "        return tf.one_hot(inputs, depth=self.depth)\n",
    "    \n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({'depth': self.depth})\n",
    "        return config\n",
    "\n",
    "K_.clear_session()\n",
    "\n",
    "# 1. Define the model (Listing 9.5)\n",
    "model_onehot = tf.keras.models.Sequential([\n",
    "    # Note: The input_shape is (None, 1) - (sequence_length, features)\n",
    "    # We use (None,) for variable sequence length, but the pipeline adds the feature dim.\n",
    "    # For simplicity, we'll let the model infer from the first batch,\n",
    "    # or specify input_shape=(None,) for the Embedding model later.\n",
    "    # Here, we specify the input_shape required by the Masking layer\n",
    "    tf.keras.layers.Masking(mask_value=0.0, input_shape=(None, 1)),\n",
    "    OnehotEncoder(depth=n_vocab + 1), # +1 for the 'unk' token\n",
    "    tf.keras.layers.LSTM(128, return_state=False, return_sequences=False),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') # Binary classification\n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "model_onehot.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_onehot.summary()\n",
    "\n",
    "# 3. Calculate class weight to handle imbalance\n",
    "neg_weight = (tr_y == 1).sum() / (tr_y == 0).sum()\n",
    "print(f\"\\nNegative class weight: {neg_weight:.2f}\")\n",
    "\n",
    "# 4. Define callbacks\n",
    "os.makedirs('eval', exist_ok=True)\n",
    "csv_logger = CSVLogger(os.path.join('eval', '1_sentiment_analysis_onehot.log'))\n",
    "es_callback = EarlyStopping(monitor='val_loss', patience=6, mode='min')\n",
    "lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, mode='min')\n",
    "\n",
    "# 5. Train the model (Listing 9.6)\n",
    "print(\"\\nTraining model with One-Hot Encoder...\")\n",
    "history_onehot = model_onehot.fit(\n",
    "    train_ds,\n",
    "    validation_data=valid_ds,\n",
    "    epochs=2, # Book runs for 10, we'll use 2 for speed\n",
    "    class_weight={0: neg_weight, 1: 1.0},\n",
    "    callbacks=[es_callback, lr_callback, csv_logger]\n",
    ")\n",
    "\n",
    "# 6. Evaluate\n",
    "print(\"\\nEvaluating One-Hot model on test set...\")\n",
    "model_onehot.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.6 Injecting Semantics with Word Embeddings\n",
    "\n",
    "One-hot encoding is simple but has two major flaws:\n",
    "1.  **Inefficient**: The vectors are huge (size `n_vocab`) and sparse (mostly zeros).\n",
    "2.  **Lacks Semantics**: The vectors for \"good\" and \"great\" are just as different as the vectors for \"good\" and \"terrible\". The model has to learn all semantic relationships from scratch.\n",
    "\n",
    "**Word Embeddings** solve this. An `Embedding` layer is a lookup table that maps each integer ID to a dense, low-dimensional vector (e.g., 128 dimensions). These vectors are *trainable*, so the model learns to place words with similar meanings close together in the vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_.clear_session()\n",
    "\n",
    "# 1. Define the model with an Embedding layer (Listing 9.7)\n",
    "model_embed = tf.keras.models.Sequential([\n",
    "    # The Embedding layer takes integer IDs. \n",
    "    # input_dim = vocab size + 1 (for 0 padding)\n",
    "    # output_dim = size of the dense vector (e.g., 128)\n",
    "    # mask_zero=True automatically handles the padding (0) values.\n",
    "    tf.keras.layers.Embedding(input_dim=n_vocab + 1, \n",
    "                              output_dim=128, \n",
    "                              mask_zero=True,\n",
    "                              input_shape=(None,)), # (batch_size, sequence_length)\n",
    "    \n",
    "    tf.keras.layers.LSTM(128, return_state=False, return_sequences=False),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "model_embed.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_embed.summary()\n",
    "\n",
    "# 3. Define callbacks\n",
    "csv_logger_embed = CSVLogger(os.path.join('eval', '2_sentiment_analysis_embed.log'))\n",
    "es_callback_embed = EarlyStopping(monitor='val_loss', patience=6, mode='min')\n",
    "lr_callback_embed = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, mode='min')\n",
    "\n",
    "# 4. Train the model\n",
    "print(\"\\nTraining model with Embedding Layer...\")\n",
    "history_embed = model_embed.fit(\n",
    "    train_ds,\n",
    "    validation_data=valid_ds,\n",
    "    epochs=2, # Book runs for 10\n",
    "    class_weight={0: neg_weight, 1: 1.0},\n",
    "    callbacks=[es_callback_embed, lr_callback_embed, csv_logger_embed]\n",
    ")\n",
    "\n",
    "# 5. Evaluate\n",
    "print(\"\\nEvaluating Embedding model on test set...\")\n",
    "model_embed.evaluate(test_ds)\n",
    "\n",
    "# 6. Save the final model\n",
    "os.makedirs('models', exist_ok=True)\n",
    "model_embed.save(os.path.join('models', '1_sentiment_analysis_embed.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Inspect predictions\n",
    "print(\"\\n--- Inspecting Model Predictions ---\")\n",
    "test_x_list = ts_x.tolist()\n",
    "test_y_list = ts_y.tolist()\n",
    "\n",
    "# Get all predictions from the test set\n",
    "test_ds_unbatched = get_tf_pipeline(ts_x_seq, ts_y, batch_size=128)\n",
    "test_pred = model_embed.predict(test_ds_unbatched)\n",
    "\n",
    "# Get indices of the sorted predictions\n",
    "sorted_pred_idx = np.argsort(test_pred.flatten())\n",
    "\n",
    "# Get top 5 most negative reviews\n",
    "min_pred_idx = sorted_pred_idx[:5]\n",
    "print(\"\\nMost Negative Reviews (as predicted by model):\")\n",
    "print(\"=\"*50)\n",
    "for i in min_pred_idx:\n",
    "    print(f\"Predicted: {test_pred[i][0]:.3f} | Actual: {test_y_list[i]}\")\n",
    "    print(\" \".join(test_x_list[i]), \"\\n\")\n",
    "\n",
    "# Get top 5 most positive reviews\n",
    "max_pred_idx = sorted_pred_idx[-5:]\n",
    "print(\"\\nMost Positive Reviews (as predicted by model):\")\n",
    "print(\"=\"*50)\n",
    "for i in max_pred_idx:\n",
    "    print(f\"Predicted: {test_pred[i][0]:.3f} | Actual: {test_y_list[i]}\")\n",
    "    print(\" \".join(test_x_list[i]), \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
